{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Sherlock","text":""},{"location":"#what-is-sherlock","title":"What is Sherlock?","text":"<p>Sherlock is a shared computing cluster available for use by all Stanford Faculty members and their research teams, for sponsored or departmental faculty research.  All research teams on Sherlock have access to a base set of managed computing resources, GPU-based servers, and a multi-petabyte, high-performance parallel file system for short-term storage.</p> <p>Faculty can supplement these shared nodes by purchasing additional servers, and become Sherlock owners. By investing in the cluster, PI groups not only receive exclusive access to the nodes they purchase, but also get access to all of the other owner compute nodes when they're not in use, thus giving them access to the whole breadth of Sherlock resources.</p>"},{"location":"#why-should-i-use-sherlock","title":"Why should I use Sherlock?","text":"<p>Using Sherlock for your work provides many advantages over individual solutions: hosted in an on-premises, state-of-the-art datacenter dedicated to research computing systems, the Sherlock cluster is powered and cooled by installations that are optimized for scientific computing.</p> <p>On Sherlock, simulations and workloads benefit from performance levels that only large scale HPC systems can offer: high-performance I/O infrastructure, petabytes of storage, large variety of hardware configurations, GPU accelerators, centralized system administration and management provided by Stanford Research Computing.</p> <p>Such features are not easily accessible at the departmental level, and often require both significant initial investments and recurring costs. Joining Sherlock allows researchers and Faculty members to avoid those costs and benefit from economies of scale, as well as to access larger, professionally managed computing resources that what would not be available on an individual or even departmental basis.</p>"},{"location":"#how-much-does-it-cost","title":"How much does it cost?","text":"<p>Sherlock is free to use for anyone doing departmental or sponsored research at Stanford.</p> <p>Any Faculty member can request access for research purposes, and get an account with a base storage allocation and unlimited compute time on the global, shared pool of resources.</p> <p>Stanford Research Computing provides faculty with the opportunity to purchase from a catalog a recommended compute node configurations, for the use of their research teams. Using a traditional compute cluster condominium model, participating faculty and their teams get priority access to the resources they purchase. When those resources are idle, other \"owners\" can use them, until the purchasing owner wants to use them. When this happens, those other owners jobs are re-queued to free up resources. Participating owner PIs also have shared access to the original base Sherlock nodes, along with everyone else.</p>"},{"location":"#how-big-is-it","title":"How big is it?","text":"<p>Quite big! It's actually difficult to give a definitive answer, as Sherlock is constantly evolving and expanding with new hardware additions.</p> <p>As of October 2025, Sherlock features over 7,900 CPU cores available to all researchers, and more than 66,100 additional CPU cores available to Sherlock owners, faculty who have augmented the cluster with their own purchases. With a computing power over 13.1 Petaflops, Sherlock would have its place in the Top500 list of the 500 most powerful computer systems in the world.</p> <p>For more details about Sherlock size and technical specifications, please refer to the tech specs section of the documentation. And for even more numbers and figures, see the Sherlock facts page.</p>"},{"location":"#ok-im-sold-how-do-i-start","title":"OK, I'm sold, how do I start?","text":"<p>You can request an account right now, take a look at the documentation, and drop us an email if you have any questions.</p>"},{"location":"#i-want-my-own-nodes","title":"I want my own nodes!","text":"<p>If you're interested in becoming an owner on Sherlock, and benefit from all the advantages associated, please take a look at the catalog of configurations, feel free to use the ordering form to submit your request, and we'll get back to you.</p>"},{"location":"docs/","title":"Sherlock documentation","text":""},{"location":"docs/#welcome-to-sherlock","title":"Welcome to Sherlock!","text":"<p>Sherlock is a High-Performance Computing (HPC) cluster, operated by the Stanford Research Computing Center to provide computing resources to the Stanford community at large. You'll find all the documentation, tips, FAQs and information about Sherlock among these pages.</p>"},{"location":"docs/#why-use-sherlock","title":"Why use Sherlock?","text":"<p>Using Sherlock for your work provides many advantages over individual solutions: hosted in an on-premises, state-of-the-art datacenter, the Sherlock cluster is powered and cooled by installations that are optimized for scientific computing.</p> <p>On Sherlock, simulations and workloads benefit from performance levels that only large scale HPC systems can offer: high-performance I/O infrastructure, petabytes of storage, large variety of hardware configurations, GPU accelerators, centralized system administration and management provided by Stanford Research Computing.</p> <p>Such features are not easily accessible at the departmental level, and often require both significant initial investments and recurring costs. Joining Sherlock allows researchers and faculty members to avoid those costs and benefit from economies of scale, as well as to access larger, professionally managed computing resources that what would not be available on an individual or even departmental basis.</p>"},{"location":"docs/#how-much-does-it-cost","title":"How much does it cost?","text":"<p>Sherlock is free to use for anyone doing departmental or sponsored research at Stanford.  Any faculty member can request access for research purposes, and get an account with a base storage allocation and unlimited compute time on the global, shared pool of resources.</p> <p>No CPU.hour charge</p> <p>Unlike all Cloud Service Providers and many HPC systems, there is no usage charge on Sherlock.</p> <p>When you submit your work on Sherlock, you don't need to keep an eye on the clock and worry about how much that run will cost you. There is no limit on the total amount of computing you can run on the cluster, as long as resources are available, and there's no charge to use them, no matter how large or small your computations are.</p> <p>In case those free resources are not sufficient, Stanford Research Computing offers Faculty members the opportunity to invest into the cluster, and get access to additional computing resources for their research teams. Using a traditional compute cluster condominium model, participating faculty and their teams get priority access to the resources they purchase. When they're idle, those resources are available to use by other owners on the cluster, giving them access to virtually unlimited resources.</p>"},{"location":"docs/#information-sources","title":"Information sources","text":"<p>Searching the docs</p> <p>If you're looking for information on a specific topic, the Search feature of this site will allow you to quickly find the page you're looking for. Just press S, F or / to open the Search bar and start typing.</p> <p>To help users take their first steps on Sherlock, we provide documentation and information through various channels:</p> Channel URL Purpose Documentation You are here www.sherlock.stanford.edu/docs information to help new users start on Sherlock, and more in-depth documentation for users already familiar with the environment. Changelog news.sherlock.stanford.edu announces, news and updates about Sherlock. Dashboard status.sherlock.stanford.edu status of Sherlock's main components and services, outages, planned maintenance. <p>To get started, you can take a look at the concepts and glossary pages to get familiar with the terminology used throughout the documentation pages. Then, we recommend going through the following sections:</p> <ul> <li>Prerequisites</li> <li>Connecting to the cluster</li> <li>Submitting jobs</li> </ul>"},{"location":"docs/#acknowledgment-citation","title":"Acknowledgment / citation","text":"<p>It is important and expected that publications resulting from computations performed on Sherlock acknowledge this. The following wording is suggested:</p> <p>Acknowledgment</p> <p>Some of the computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.</p>"},{"location":"docs/#support","title":"Support","text":""},{"location":"docs/#email-recommended","title":"Email (recommended)","text":"<p>Research Computing support can be reached by sending an email to srcc-support@stanford.edu and mentioning Sherlock.</p> <p>How to submit effective support requests</p> <p>To ensure a timely and relevant response, please make sure to include some additional details, such as job ids, commands executed and error messages received, so we can help you better. For more details, see the Troubleshooting page.</p> <p>As a member of the Sherlock community, you're also automatically subscribed to the sherlock-announce mailing-list, which is only used by the Stanford Research Computing team to send important announcements about Sherlock.</p>"},{"location":"docs/#onboarding-sessions","title":"Onboarding sessions","text":"<p>We offer regular onboarding sessions for new Sherlock users.</p> <p>On-boarding session times</p> <p>On-boarding sessions are offered every first Wednesday of the month, 1PM-2PM PST, via Zoom</p> <p>These one-hour sessions are a brief introduction to Sherlock's layout, its scheduler, the different file systems available on the cluster, as well as some job submission and software installation best practices for new users. They are a good intro course if you are new to Sherlock or HPC in general.</p> <p>If you can't attend live on-boarding sessions, you can still take a look at the  on-boarding slides as well as to this  session recording.</p>"},{"location":"docs/#office-hours","title":"Office hours","text":"<p>Sending a question to srcc-support@stanford.edu is always the best first option for questions. That way you can include detailed descriptions of the problem or question, valuable output and error messages and any steps you took when you encountered your error. Also, everyone on our team will see your ticket, enabling the most appropriate group member to respond.</p> <p>Office hours are a good place for more generalized questions about Sherlock, Slurm, Linux usage, data storage, queue structures/scheduling, job optimization and general capabilities of Sherlock. It's also useful for more technically nuanced questions that may not be easily answered with our ticketing system. In office hours some problems can indeed be solved quickly or progress can be made so that you can then work self-sufficiently towards a solution on your own.</p> <p>Office hours are online</p> <p>Office hours are held virtually, online via Zoom</p> <p>Office hours times</p> <p>Click here to join the Sherlock Office Hours Zoom</p> <ul> <li>Tuesday 10-11am</li> <li>Thursday 3-4pm</li> </ul> <p>You'll need a full-service SUNet ID (basically, a @stanford.edu email address) in order to authenticate and join Office Hours via Zoom.  If you do not have a full service account, please contact us at srcc-support@stanford.edu.</p> <p>If you can't make any of the Office Hours sessions, you can also make an appointment with Sherlock's support team.</p>"},{"location":"docs/#what-to-expect","title":"What to expect","text":"<ul> <li> <p>We cannot accommodate walk-ins: we're unfortunately not staffed to welcome   unscheduled visits, so please make sure that you're planning to stop by   during office hours. We will not be able to help you otherwise.</p> </li> <li> <p>We can rarely help with application-specific or algorithm problems.</p> </li> <li> <p>You should plan your projects sufficiently in advance and not come to office   hours at the last minute before a deadline. Sherlock is a busy resource with   several thousand users and you should not expect your jobs to complete before   a given date.</p> </li> <li> <p>Not all questions and problems can be answered or solved during office hours,   especially ones involving hardware, filesystem or network issues. Sherlock   features several thousand computing, networking and storage components, that   are constantly being monitored by our team. You can be sure that when   Sherlock has an issue, we are aware of it and working on it.</p> </li> </ul>"},{"location":"docs/#user-community","title":"User community","text":"<p>Sherlock is present on the Stanford Slack Grid, and you're more than welcome to join the following channels:</p> <ul> <li><code>#sherlock-announce</code>, for announcements related to     Sherlock and its surrounding services,</li> <li><code>#sherlock-users</code>, as a place for Sherlock users to     connect directly with each other. If you have general questions about     Sherlock, want to reach out to other Sherlock users to share tips, good     practices, tutorials or other info, please feel free to do so there.</li> </ul> <p>For more details about the SRCC Slack Workspace, and instructions on how to join this workspace and its channels, please see the Stanford Research Computing support page.</p> <p>Slack is not an official support channel</p> <p>Please note that while Stanford Research Computing staff will monitor these channels, the official way to get support is still to email us at srcc-support@stanford.edu.</p>"},{"location":"docs/#quick-start","title":"Quick Start","text":"<p>If you're in a rush<sup>1</sup>, here's a 3-step ultra-quick start:</p> <ol> <li>connect to Sherlock</li> </ol> <pre><code>$ ssh login.sherlock.stanford.edu\n</code></pre> <ol> <li>get an interactive session on a compute node</li> </ol> <pre><code>[kilian@sh-ln01 login! ~]$ sh_dev\n</code></pre> <ol> <li>run a command</li> </ol> <pre><code>[kilian@sh02-01n58 ~]$ module load python\n[kilian@sh02-01n58 ~]$ python -c \"print('Hello Sherlock')\"\nHello Sherlock\n</code></pre> <p>Congrats! You ran your first job on Sherlock!</p>"},{"location":"docs/#replay","title":"Replay","text":"<p>Here's what it looks like in motion:</p> <ol> <li> <p>even in a rush, you'll still need an account on the cluster. See   the Prerequisites page for details.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/concepts/","title":"Concepts","text":""},{"location":"docs/concepts/#sherlock-a-shared-resource","title":"Sherlock, a shared resource","text":"<p>Sherlock is a shared compute cluster available for use by all Stanford faculty members and their research teams to support departmental or sponsored research.</p> <p>Sherlock is a resource for research</p> <p>Sherlock is not suitable for course work, class assignments or general-use training sessions.</p> <p>Users interested in using computing resources in such contexts are encouraged to investigate FarmShare, Stanford\u2019s community computing environment, which is primarily intended for supporting coursework.</p> <p>It is open to the Stanford community as a computing resource to support departmental or sponsored research, thus a faculty member's sponsorship is required for all user accounts.</p> <p>Usage policy</p> <p>Please note that your use of this system falls under the \"Computer and Network Usage Policy\", as described in the Stanford Administrative Guide. In particular, sharing authentication credentials is strictly prohibited.  Violation of this policy will result in termination of access to Sherlock.</p> <p>Sherlock is designed, deployed, maintained and operated by Stanford Research Computing staff. Stanford Research Computing is a joint effort of the Dean of Research and IT Services to build and support a comprehensive program to advance computational research at Stanford.</p> <p>Sherlock has been initially purchased and supported with seed funding from Stanford's Provost. It comprises a set of freely available compute nodes, a few specific resources such as large-memory machines and GPU servers, as well as the associated networking equipment and storage.  These resources can be used to run computational codes and programs, and are managed through a job scheduler using a fair-share algorithm.</p>"},{"location":"docs/concepts/#data-risk-classification","title":"Data risk classification","text":"<p>Low and Moderate Risk data</p> <p>Sherlock is approved for computing with Low and Moderate Risk data only.</p> <p>High Risk data</p> <p>Sherlock is NOT approved to store or process HIPAA, PHI, PII nor any kind of High Risk data.  The system is approved for computing with Low and Moderate Risk data only, and is not suitable to process High Risk data.</p> <p> Users are responsible for ensuring the compliance of their own data.</p> <p>For more information about data risk classifications, see the Information Security Risk Classification page.</p>"},{"location":"docs/concepts/#investing-in-sherlock","title":"Investing in Sherlock","text":"<p>For users who need more than casual access to a shared computing environment, Sherlock also offers Faculty members the possibility to invest in additional, dedicated computing resources.</p> <p>Unlike traditional clusters, Sherlock is a collaborative system where the majority of nodes are purchased and shared by the cluster users. When a user (typically a PI) purchases one or more nodes, they become an owner. Owners choose from a standard set of server configurations supported by Stanford Research Computing (known as the Sherlock catalog) to add to the cluster.</p> <p>When they're not in use, PI-purchased compute nodes can be used by other owners. This model also allows Sherlock owners to benefit from the scale of the cluster by giving them access to more compute nodes than their individual purchase, which gives them much greater flexibility than owning a standalone cluster.</p> <p>The majority of Sherlock nodes are owners nodes</p> <p>The vast majority of Sherlock's compute nodes have been purchased by individual PIs and groups, and PI purchases are the main driver behind the rapid expansion of the cluster, which went from 120 nodes to more than 1,000 nodes in less than 3 years.</p> <p>The resource scheduler configuration works like this:</p> <ul> <li>owners and their research teams get immediate and exclusive access to the   resources they purchased,</li> <li>when those nodes are idle, other owners can use them,</li> <li>when the purchasing owners want to use their resources, jobs from other   owners that may be running on them are preempted (ie. killed and   re-queued).</li> </ul> <p>This provides a way to get more resources to run less important jobs in the background, while making sure that an owner always gets immediate access to his/her own nodes.</p> <p>Participating owners also have shared access to the public, shared Sherlock nodes, along with everyone else.</p>"},{"location":"docs/concepts/#benefits","title":"Benefits","text":"<p>Benefits to owners include:</p> <p> no wait time in queue: immediate and exclusive access to the  purchased nodes</p> <p> access to more resources: possibility to submit jobs to the other owners' nodes when they're not in use</p> <p>Compared to hosting and managing computing resources on your own, purchasing nodes on Sherlock provides:</p> <ul> <li>data center hosting, including backup power and cooling</li> <li>system configuration, maintenance and administration</li> <li>hardware diagnostics and repairs</li> </ul> <p>Those benefits come in addition to the other Sherlock advantages:</p> <ul> <li>access to high-performance, large parallel scratch storage space</li> <li>access to snapshot'ed, replicated, enterprise-class storage space</li> <li>optimized software stack, especially tailored for a range of research needs</li> <li>tools to build and install additional software applications as needed</li> <li>user support</li> </ul>"},{"location":"docs/concepts/#limitations","title":"Limitations","text":"<p>Purchasing nodes on Sherlock is different from traditional server hosting.</p> <p>In particular, purchasing your own compute nodes on Sherlock will NOT allow:</p> <p> root access: owner nodes on Sherlock are still managed by Stanford Research Computing staff in accordance with Stanford's Minimum Security Standards. Although users are welcome to install (or request) any software they may need, purchasing compute nodes on Sherlock does not allow <code>root</code> access to the nodes.</p> <p> running permanent services: permanent processes such as web servers or databases can only run on owner nodes through the scheduler, using recurring or persistent jobs. Purchasing compute nodes on Sherlock does not provide a way to run anything that couldn't run on freely-available nodes.</p> <p> direct network connectivity: owners' nodes are connected to the Sherlock's internal network and are not directly accessible from the outside, which means that they can't host public services like web or application servers.</p> <p> bypassing the scheduler: jobs running on owners' nodes still need to be submitted to the scheduler.  Direct shell access to the nodes is not possible outside of scheduled interactive sessions.</p> <p> hardware changes: the hardware components of purchased nodes cannot be modified, removed, swapped or upgraded during the nodes' service lifetime.</p> <p> configuration: the configuration of purchased nodes is tuned to provide optimal performance over a majority of use cases and applications, is identical on all nodes across the cluster, and cannot be changed, modified or altered in any way.</p> <p> persistent local storage: local storage space provided on the compute nodes is only usable for the duration of a job and cannot be used to store long-term data.</p> <p> additional storage space: purchasing compute nodes on Sherlock does not provide additional storage space. Please note that Stanford Research Computing does offer the possibility for PIs to purchase their own storage space on Oak, for their long-term research data needs.</p>"},{"location":"docs/concepts/#purchasing-nodes","title":"Purchasing nodes","text":"<p>If you are interested in becoming an owner, you can find the latest information about ordering Sherlock nodes on the ordering page. Feel free to contact us is you have any additional question.</p>"},{"location":"docs/concepts/#cluster-generations","title":"Cluster generations","text":"<p>The research computing landscape evolves very quickly, and to both accommodate growth and technological advances, it's necessary to adapt the Sherlock environment to these evolutions.</p> <p>Every year or so, a new generation of processors is released, which is why, over a span of several years, multiple generations of CPUs and GPUs make their way into Sherlock. This provides users with access to the latest features and performance enhancements, but it also adds some heterogeneity to the cluster, which is important to keep in mind when compiling software and requesting resources to run them.</p> <p>Another key component of Sherlock is the interconnect network that links all of Sherlock's compute nodes together and act as a backbone for the whole cluster. This network fabric is of finite capacity, and based on the individual networking switches characteristics and the typical research computing workflows, it can accommodate up to about 850 compute nodes.</p> <p>As nodes get added to Sherlock, the number of available ports decreases, and at some point, the fabric gets full and no more nodes can be added. Sherlock reached that stage for the first time in late 2016, which prompted the installation of a whole new fabric, to allow for further system expansion.</p> <p>This kind of evolution is the perfect opportunity to upgrade other components too: management software, ancillary services architecture and user applications. In January 2017, those components were completely overhauled and a new, completely separate cluster was kick-started, using using a different set of hardware and software, while conserving the same storage infrastructure, to ease the transition process.</p> <p>After a transition period, the older Sherlock hardware, compute and login nodes, have been be merged in the new cluster, and from a logical perspective (connection, job scheduling and computing resources), nodes attached to each of the fabrics have been reunited to form a single cluster again.</p> <p>As Sherlock continues to evolve and grow, the new fabric will also approach capacity again, and the same process will happen again to start the next generation of Sherlock.</p>"},{"location":"docs/concepts/#maintenances-and-upgrades","title":"Maintenances and upgrades","text":"<p>Stanford Research Computing institutes a monthly scheduled maintenance window on Sherlock, to ensure optimal operation, avoid potential issues and prepare for future expansions. This window will be used to make hardware repairs, software and firmware updates, and perform general manufacturer recommended maintenance on our environment.</p> <p>As often as possible, maintenance tasks are performed in a rolling, non-disruptive fashion, but downtimes are sometimes an unfortunate necessity to allow disruptive operations that can't be conducted while users are working on the system.</p> <p>Maintenance schedule</p> <p>As often as possible, maintenances will take place on the first Tuesday of every month, from 08:00 to 12:00 Pacific time (noon), and will be announced 2 weeks in advance, through the usual communication channels.</p> <p>In case an exceptional amount of work is required, the maintenance window could be extended to 10 hours (from 08:00 to 18:00).</p> <p>During these times, access to Sherlock will be unavailable, login will be disabled and jobs won't run. A reservation will be placed in the scheduler so running jobs can finish before the maintenance, and jobs that wouldn't finish by the maintenance window would be pushed after it.</p>"},{"location":"docs/concepts/#common-questions","title":"Common questions","text":"<p>Q: Why doing maintenances at all?</p> <p>A: Due to the scale of our computing environment and the increasing complexity of the systems we deploy, it is prudent to arrange for a regular time when we can comfortably and without pressure fix problems or update facilities with minimal impact to our customers. Most, if not all, major HPC centers have regular maintenance schedules.  We also need to enforce the Minimum Security rules instituted by the Stanford Information Security Office, which mandate deployment of security patches in a timely manner.</p> <p>Q: Why Tuesdays 08:00-12:00? Why not do this late at night?</p> <p>A: We have observed that the least busy time for our services is at the beginning of the week in the morning hours. Using this time period should not interrupt most of our users. If the remote possibility of a problem that extends past the scheduled downtime occurs, we would have our full staff fresh and available to assist in repairs and quickly restore service.</p> <p>Q: I have jobs running, what will happen to them?</p> <p>A: For long-running jobs, we strongly recommend checkpointing your results on a periodic basis. Besides, we will place a reservation in the scheduler for each maintenance that would prevent jobs to run past it. This means that the scheduler will only allow jobs to run if they can finish by the time the maintenance starts. If you submit a long job soon before the maintenance, it will be delayed until after the maintenance. That will ensure that no work is lost when the maintenance starts.</p>"},{"location":"docs/credits/","title":"About us","text":""},{"location":"docs/credits/#stanford-research-computing","title":"Stanford Research Computing","text":"<p>Stanford Research Computing) is a joint effort of the Dean of Research and IT Services to build and support a comprehensive program to advance computational research at Stanford.  That includes offering and supporting traditional high performance computing (HPC) systems, as well as systems for high throughput and data-intensive computing.</p> <p>The Stanford Research Computing team also helps researchers transition their analyses and models from the desktop to more capable and plentiful resources, providing the opportunity to explore their data and answer research questions at a scale typically not possible on desktops or departmental servers. Partnering with national initiatives and program as well as vendors, Stanford Research Computing offers training and learning opportunities around HPC tools and technologies.</p> <p>For more information, please see the Stanford Research Computing website</p>"},{"location":"docs/credits/#credits","title":"Credits","text":"<p>We would like to thank the following companies for their generous sponsorship, and for providing services and resources that help us manage Sherlock every day:</p> <ul> <li> GitHub</li> <li> Hund</li> <li> Noticeable</li> <li> UptimeRobot</li> </ul> <p>The Sherlock website and documentation also rely on the following projects:</p> <ul> <li> MkDocs</li> <li> Material for MkDocs</li> </ul>"},{"location":"docs/credits/#why-the-sherlock-name","title":"Why the Sherlock name?","text":"<p>If you're curious about where the Sherlock name came from, we always considered that computing resources in general and HPC clusters in particular should be the catalyst of innovation, be ahead of their time, and spur new discoveries.</p> <p>And what better account of what's happening on a high-performance computing cluster than Benedict Cumberbatch describing his role as Sherlock Holmes in the BBC's modern adaptation of Arthur Conan Doyle's classic?</p> <p>Benedict Cumberbatch, about Sherlock</p> <p>There's a great charge you get from playing him, because of the volume of words in your head and the speed of thought \u2013 you really have to make your connections incredibly fast. He is one step ahead of the audience, and of anyone around him with normal intellect. They can't quite fathom where his leaps are taking him.</p> <p>Yes, exactly. That's Sherlock.</p>"},{"location":"docs/credits/#sherlock-of-hbo-fame","title":"Sherlock, of HBO fame","text":"<p>And finally, we couldn't resist to the pleasure of citing the most prestigious accomplishment of Sherlock to date: a mention in HBO's Silicon Valley Season 4 finale!</p> <p> </p> <p>Yep, you got that right, Richard Hendricks wanted to use our very own Sherlock!</p> <p>{ align=left style=\"height:100px; margin-top:0\" } Kudos to the show's crew and a big thank you to HBO Data compression stars, Professor Tsachy Weissman and Dmitri Pavlichin, for this incredible Sherlock shout-out. This has been an everlasting source of pride and amazement for the whole SRCC team! </p>"},{"location":"docs/glossary/","title":"Glossary","text":""},{"location":"docs/glossary/#whats-a-cluster","title":"What's a cluster?","text":"<p>A computing cluster is a federation of multiple compute nodes (independent computers), most commonly linked together through a high-performance interconnect network.</p> <p>What makes it a \"super-computer\" is the ability for a program to address resources (such as memory, CPU cores) located in different compute nodes, through the high-performance interconnect network.</p> <p></p> <p>On a computing cluster, users typically connect to login nodes, using a secure remote login protocol such as SSH. Unlike in traditional interactive environments, users then need to prepare compute jobs to submit to a resource scheduler. Based on a set of rules and limits, the scheduler will then try to match the jobs' resource requirements with available resources such as CPUs, memory or computing accelerators such as GPUs. It will then execute the user defined tasks on the selected resources, and generate output files in one of the different storage locations available on the cluster, for the user to review and analyze.</p>"},{"location":"docs/glossary/#cluster-components","title":"Cluster components","text":"<p>The terms that are typically used to describe cluster components could be confusing, so in an effort to clarify things, here's a schema of the most important ones, and their definition. </p>"},{"location":"docs/glossary/#cpu","title":"CPU","text":"A Central Processing Unit (CPU), or core, or CPU core, is the smallest unit in a microprocessor that can carry out computational tasks, that is, run programs. Modern processors typically have multiple cores."},{"location":"docs/glossary/#socket","title":"Socket","text":"A socket is the connector that houses the microprocessor. By extension, it represents the physical package of a processor, that typically contains multiple cores."},{"location":"docs/glossary/#node","title":"Node","text":"A node is a physical, stand-alone computer, that can handle computing tasks and run jobs. It's connected to other compute nodes via a fast network interconnect, and contains CPUs, memory and devices managed by an operating system."},{"location":"docs/glossary/#cluster","title":"Cluster","text":"A cluster is the complete collection of nodes with networking and file storage facilities. It's usually a group of independent computers connected via a fast network interconnect, managed by a resource manager, which acts as a large parallel computer."},{"location":"docs/glossary/#other-commonly-used-terms","title":"Other commonly used terms","text":"<p>To make this documentation more accessible, we try to explain key terms in a non-technical way. When reading these pages, please keep in mind the following definitions, presented in alphabetical order:</p>"},{"location":"docs/glossary/#application","title":"Application","text":"An application is a computer program designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. In the context of scientific computing, an application typically performs computations related to a scientific goal (molecular dynamics simulations, genome assembly, compuational fluid dynamics simulations, etc)."},{"location":"docs/glossary/#backfill","title":"Backfill","text":"Backfill scheduling is a method that a scheduler can use in order to maximize utilization. It allows smaller (both in terms of size and time requirements), lower priority jobs to start before larger, higher priority ones, as long as doing so doesn't push back the higher-priority jobs expected start time."},{"location":"docs/glossary/#executable","title":"Executable","text":"A binary (or executable) program refers to the machine-code compiled version of an application. This is  which is a binary file that a computer can execute directly. As opposed to the application source code, which is the human-readable version of the application internal instructions, and which needs to be compiled by a compiler to produce the executable binary."},{"location":"docs/glossary/#fairshare","title":"Fairshare","text":"A resource scheduler ranks jobs by priority for execution. Each job's priority in queue is determined by multiple factors, among which one being the user's fairshare score.  A user's fairshare score is computed based on a target (the given portion of the resources that this user should be able to use) and the user's effetive usage, ie the amount of resources (s)he effectively used in the past.  As a result, the more resources past jobs have used, the lower the priority of the next jobs will be.  Past usage is computed based on a sliding window and progressively forgotten over time.  This enables all users on a shared resource to get a fair portion of it for their own use, by giving higher priority to users who have been underserved in the past."},{"location":"docs/glossary/#flops","title":"FLOPS","text":"Floating-point Operations Per Second (FLOPS) are a measure of computing performance, and represent the number of floating-point operations that a CPU can perform each second. Modern CPUs and GPUs are capable of doing TeraFLOPS (10^12 floating-point operations per second), depending on the precision of those operations (half-precision: 16 bits, single-precision: 32 bits, double-precision: 64 bits)."},{"location":"docs/glossary/#gpu","title":"GPU","text":"A Graphical Processing Unit (GPU) is a specialized device initially designed to generate graphical output.  On modern computing architecture, they are used to accelerate certain types of computation, which they are much faster than CPUs at. GPUs have their own memory, and are attached to CPUs, within a node. Each compute node can host one or more GPUs."},{"location":"docs/glossary/#hpc","title":"HPC","text":"High Performance Computing (HPC) refers to the practice of aggregating computing power to achieve higher performance that would be possible by using a typical computer."},{"location":"docs/glossary/#infiniband","title":"Infiniband","text":"Infiniband is a networking standard that features high bandwidth and low latency. The current Infiniband devices are capable of transferring data at up to 200 Gbits/sec with less than a microsecond latency. As of this writing, the popular Infiniband versions are HDR (High Data Rate) with 200 Gbits/sec and EDR (Enhanced Data Rate) with 100 Gbits/sec."},{"location":"docs/glossary/#iops","title":"IOPS","text":"Input/output operations per second (IOPS, pronounced eye-ops) is an input/output performance measurement used to characterize computer storage system performance."},{"location":"docs/glossary/#job","title":"Job","text":"A job, or batch job, is the scheduler\u2019s base unit of computing by which resources are allocated to a user for a specified amount of time. Users create job submission scripts to ask the scheduler for resources such as cores, memory, runtime, etc. The scheduler puts the requests in a queue and allocates requested resources based on jobs\u2019 priority."},{"location":"docs/glossary/#job-step","title":"Job step","text":"Job steps are sets of (possibly parallel) tasks within a job"},{"location":"docs/glossary/#login-nodes","title":"Login nodes","text":"<p>Login nodes are points of access to a compute cluster. Users usually connect to login nodes via SSH to compile and debug their code, review their results, do some simple tests, and submit their batch jobs to the parallel computer.</p> <p>Login nodes are not for computing</p> <p>Login nodes are usually shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>"},{"location":"docs/glossary/#modules","title":"Modules","text":"Environment modules, or software modules, are a type of software management tool used on in most HPC environments. Using modules enable users to selectively pick the software that they want to use and add them to their environment. This allows to switch between different versions or flavors of the same software, pick compilers, libraries and software components and avoid conflicts between them."},{"location":"docs/glossary/#mpi","title":"MPI","text":"Message Passing Interface (MPI) is a standardized and portable message-passing system designed to exchange information between processes running on different nodes. There are several implementations of the MPI standard, which is the most common way used to scale parallel applications beyond a single compute node."},{"location":"docs/glossary/#openmp","title":"OpenMP","text":"Open Multi Processing (OpenMP) is a parallel programming model designed for shared memory architecture. It's based on pragmas that can be added in applications to let the compiler generate a code that can run on multiple cores, within the same node."},{"location":"docs/glossary/#partition","title":"Partition","text":"<p>A partition is a set of compute nodes within a cluster with a common feature. For example, compute nodes with GPU, or compute nodes belonging to same owner, could form a partition.</p> <p>On Sherlock, you can see detailed partition information with the <code>sh_part</code> or <code>sinfo</code> commands.</p>"},{"location":"docs/glossary/#qos","title":"QOS","text":"A Quality Of Service (QOS) is the set of rules and limitations that apply to a categories of job. The combination of a partition (set of machines where a job can run) and QOS (set of rules that applies to that job) makes what is often referred to as a scheduler queue."},{"location":"docs/glossary/#run-time","title":"Run time","text":"The run time, or walltime, of a job is the time required to finish its execution."},{"location":"docs/glossary/#scheduler","title":"Scheduler","text":"The goal of a job scheduler is to find the appropriate resources to run a set of computational tasks in the most efficient manner. Based on resource requirements and job descriptions, it will prioritize those jobs, allocate resources (nodes, CPUs, memory) and schedule their execution."},{"location":"docs/glossary/#slurm","title":"Slurm","text":"Simple Linux Utility for Resource Management (SLURM) is a software that manages computing resources and schedule tasks on them. Slurm coordinates running of many programs on a shared facility and makes sure that resources are used in an optimal manner."},{"location":"docs/glossary/#ssh","title":"SSH","text":"Secure Shell (SSH) is a protocol to securely access remote computers. Based on the client-server model, multiple users with an SSH client can access a remote computer. Some operating systems such as Linux and Mac OS have a built-in SSH client and others can use one of many publicly available clients."},{"location":"docs/glossary/#thread","title":"Thread","text":"A process, in the simplest terms, is an executing program. One or more threads run in the context of the process. A thread is the basic unit to which the operating system allocates processor time. A thread can execute any part of the process code, including parts currently being executed by another thread. Threads are co-located on the same node."},{"location":"docs/glossary/#task","title":"Task","text":"In the Slurm context, a task is to be understood as a process. A multi-process program is made of several tasks. A task is typically used to schedule a MPI process, that in turn can use several CPUs.  By contrast, a multi-threaded program is composed of only one task, which uses several CPUs."},{"location":"docs/orders/","title":"Ordering nodes on Sherlock","text":"<p>For research groups needing access to additional, dedicated computing resources on Sherlock, we offer the possibility for PIs to purchase their own compute nodes to add to the cluster.</p> <p>Operating costs for managing and housing PI-purchased compute nodes are waived in exchange for letting other users make use of any idle compute cycles on the PI-owned nodes. Owners have priority access to the computing resources they purchase, but can access more nodes for their research if they need to. This provides the PI with much greater flexibility than owning a standalone cluster.</p>"},{"location":"docs/orders/#conditions","title":"Conditions","text":""},{"location":"docs/orders/#service-term","title":"Service term","text":"<p>Compute nodes are purchased for a duration of 4 years</p> <p>Compute nodes are purchased and maintained based on a 4-year lifecycle, which is the duration of the equipment warranty and vendor support.</p> <p>Owners will be notified during the 4<sup>th</sup> year that their nodes' lifetime is about to reach its term, at which point they'll be welcome to either:</p> <ul> <li>renew their investment by purchasing new nodes,</li> <li>continue to use the public portion of Sherlock's resources.</li> </ul> <p>At the end of their service term, compute nodes are physically retired from the cluster, to make room for new equipment. Compute nodes may be kept running for an additional year at most after the end of their service term, while PIs plan for equipment refresh. Nodes failing during this period may not be repaired, and failed hardware will be disabled or removed from the system.</p> <p>Please note that outside of exceptional circumstances, nodes purchased in Sherlock cannot be removed from cluster before the end of their service term.</p>"},{"location":"docs/orders/#shared-ownership","title":"Shared ownership","text":"<p>Minimum order of one node per PI</p> <p>The number of nodes in a shared order must be greater or equal to the number of purchasing PI groups.</p> <p>For operational, administrative as well as usability reasons, we do not support shared ownership of equipment. Meaning that multiple PI groups cannot purchase and share a single compute node. Shared orders have a minimum of one node per purchasing PI group.</p>"},{"location":"docs/orders/#compute-nodes-catalog","title":"Compute nodes catalog","text":"<p>Stanford Research Computing offers a select number of compute node configurations that have been tested and validated on Sherlock and that aim to cover most computing needs.</p> <p>Sherlock catalog</p> <p>Complete details are available in the Sherlock compute nodes catalog <sup>1</sup></p>"},{"location":"docs/orders/#configurations","title":"Configurations","text":"<p>We try to provide hardware configurations that can cover the needs and requirements of a wide range of computing applications, in various scientific fields, and to propose a spectrum of pricing tiers, as shown in the table below:</p> Type Description Recommended usage Price range <code>CBASE</code> Base configuration Best per-core performance for serial applications, multi-threaded (OpenMP) and distributed (MPI) applications. Most flexible and cost-effective configuration $ <code>CPERF</code> High-performance cores CPU-bound applications requiring high per-core performance and large L3 caches $$ <code>CSCALE</code> Many-core configuration Multi-threaded single-node applications requiring higher numbers of CPU cores (lower per-core performance) $$ <code>CBIGMEM</code> Large-memory configuration Serial or multi-threaded applications requiring terabytes of memory (genome assembly, etc...) $$$ <code>G4FP32</code> Base GPU configuration Single-precision (FP32) GPU-accelerated applications (CryoEM, MD...) with low GPU memory requirements $$ <code>G4FT64</code> HPC GPU configuration AI, ML/DL and GPU-accelerated HPC codes requiring double-precision (FP64) and larger amounts of GPU memory $$$ <code>G8TF64</code> Best-in-class GPU configuration AI, ML/DL and GPU-accelerated HPC codes requiring double-precision (FP64), large amounts of GPU memory, and heavy multi-GPU scaling $$$$ Choosing the best node configuration for your needs <p>Although some configurations may appear cheaper when looking at the dollar/core ratio, this is not the only point to consider when determining the best configuration for your workload.</p> Performance per core <p>There are other factors to take into account, notably the memory and I/O bandwidth per core, which could be lower on higher core-count configurations like <code>CSCALE</code>. With multiple times more cores than <code>CBASE</code>, they still provide the same total amount of bandwidth to remote and local storage, as well as, to a lesser extend, to memory.  Higher core-count CPUs also often offer lower core frequencies, which combined with less bandwidth per core, may result in lower performance for serial jobs.</p> <p><code>CSCALE</code> nodes are an excellent fit for multi-threaded applications that don't span multiple nodes. But for more diverse workloads, they don't offer the same level of flexibility than the <code>CBASE</code> nodes, which can run a mix of serial, multi-threaded and MPI applications equally well.</p> Resources availability <p>Another important factor to take into account is that less nodes for a given number of cores offers less resilience against potential hardware failures: if a 128-core node becomes unavailable for some reason, that's 128 cores that nobody can use while the node is being repaired. But with 128 cores in 4x 32-core nodes, if a node fails, there are still 96 cores that can be used.</p> <p>We'll be happy to help you determine the best configuration for your computing needs, feel free to reach out to schedule a consultation.</p> <p>Configuration details for the different compute node types are listed in the Sherlock compute nodes catalog <sup>1</sup></p>"},{"location":"docs/orders/#prices","title":"Prices","text":"<p>Prices for the different compute node types are listed in the Sherlock compute nodes catalog <sup>1</sup>. They include tax and shipping fees, and are subject to change when quoted: they tend to follow the market-wide variations induced by global political and economical events, which are way outside of our control. Prices are provided there as a guideline for expectations.</p> <p>There are two components in the cost of a compute node purchase:</p> <ol> <li> <p>the cost of the hardware itself (capital purchase),</p> </li> <li> <p>a one-time, per-node infrastructure fee<sup>3</sup> that      will be charged to cover the costs of connecting the nodes to the cluster      infrastructure (racks, PDUs, networking switches, cables...)</p> </li> </ol> <p>No recurring fees</p> <p>There is currently no recurring fee associated with purchasing compute nodes on Sherlock. In particular, there is no CPU.hour charge, purchased nodes are available to their owners 100% of the time, at no additional cost.</p> <p>Currently, there are no user, administrative or management fees associated with ongoing system administration of the Sherlock environment. However, PIs should anticipate the eventuality of modest system administration and support fees being  levied within the 4 year lifetime of their compute nodes.</p>"},{"location":"docs/orders/#purchasing-process","title":"Purchasing process","text":"<p>Minimum purchase</p> <p>Please note that the minimum purchase is one physical server per PI group. We cannot accommodate multiple PIs pooling funds for a single node.</p> <p>Single-node orders may incur additional delays</p> <p>Some node configurations need to be ordered from the vendor in fully-populated chassis of 4 nodes (see the Sherlock catalog for details). So orders for quantities non-multiples of 4 need will to be grouped with other PI's orders, which may incur additional delays.</p> <p>Purchasing nodes on Sherlock is usually a 5-step process:</p> <ol> <li>the PI use the order form to submit an order,</li> <li>Stanford Research Computing requests a formal vendor quote to finalize      pricing and communicate it back to the PI for approval,</li> <li>Stanford Research Computing submits a Stanford PO to the vendor,</li> <li>Stanford Research Computing takes delivery of the hardware and proceeds to      its installation,</li> <li>Stanford Research Computing notifies the PI that their nodes are ready to      be used.</li> </ol> <p>The typical delay between a PO submission to the vendor and the availability of the compute nodes to the PIs is usually between 4 and 8 weeks.</p>"},{"location":"docs/orders/#required-information","title":"Required information","text":"<p>To place an order, we'll need the following information:</p> <ul> <li>The SUNet ID of the PI making the purchase request</li> <li>A PTA<sup>2</sup> number to charge the hardware (capital) portion of the purchase</li> <li>A PTA<sup>2</sup> number to charge the per-node infrastructure fees (non-capital)     It could be the same PTA used for the capital portion of the     purchase, or a different one</li> </ul> <p>Hardware costs could be spread over multiple PTAs (with a maximum of 2 PTAs per order). But please note that the infrastructure fees have to be charged to a single PTA.</p>"},{"location":"docs/orders/#placing-an-order","title":"Placing an order","text":"<p>To start ordering compute nodes for Sherlock:</p> <p> check the Sherlock catalog <sup>1</sup> to review prices and select your configurations</p> <p>Choose </p> <p> fill in the order form <sup>1</sup> to submit your request and provide the required information</p> <p>Order </p> <p>And we'll be in touch shortly!</p> <ol> <li> <p>SUNet ID required, document restricted to <code>@stanford.edu</code> accounts.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>PTA is an acronym used for a Project-Task-Award combination   representing an account in the Stanford Financial system.\u00a0\u21a9\u21a9</p> </li> <li> <p>infrastructure fees are considered non-capital for cost   accounting purposes and may incur indirect cost burdens on cost-reimbursable   contracts and grants.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/tags/","title":"Tags","text":"<p>Here is a list of documentation tags:</p>"},{"location":"docs/tags/#tag:advanced","title":"advanced","text":"<ul> <li>            Node features          </li> </ul>"},{"location":"docs/tags/#tag:connection","title":"connection","text":"<ul> <li>            Connecting          </li> <li>            Connection options          </li> <li>            Data transfer          </li> </ul>"},{"location":"docs/tags/#tag:slurm","title":"slurm","text":"<ul> <li>            Job management          </li> <li>            Node features          </li> <li>            Running jobs          </li> <li>            Submitting jobs          </li> </ul>"},{"location":"docs/tags/#tag:tech","title":"tech","text":"<ul> <li>            Facts          </li> <li>            Technical specifications          </li> </ul>"},{"location":"docs/advanced-topics/connection/","title":"Advanced connection options","text":"","tags":["connection"]},{"location":"docs/advanced-topics/connection/#login-nodes","title":"Login nodes","text":"<p>Sherlock login nodes are regrouped behind a single DNS alias: <code>login.sherlock.stanford.edu</code>.</p> <p>This alias provides a load-balanced login environment, and the assurance that you will be connected to the least loaded login node when you connect to Sherlock.</p> <p>If for any reason, you want to directly connect to a specific login node and bypass the automatic load-balanced dispatching of new connections (which we don't recommend), you can use that login node's hostname explicitly. For instance:</p> <pre><code>$ ssh &lt;sunetid&gt;@ln21.sherlock.stanford.edu\n</code></pre> <p>This can be useful if you run long-standing processes on the login nodes, such as screen or tmux sessions. To find them back when you reconnect to Sherlock, you will indeed need to login to the same login node you started them on.</p> <p>The drawback is that by connecting to a specific login node, you will forfeit the load-balancing benefits, which could result in a crowded environment, or even in login errors in case that specific login node is unavailable.</p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#authentication-methods","title":"Authentication methods","text":"<p>Public-key authentication</p> <p>SSH public-key authentication is not supported on Sherlock.</p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#password-recommended","title":"Password (recommended)","text":"<p>The recommended way to authenticate to Sherlock is to simply use your SUNet ID and password, as described in the Connecting page.</p> <p>Passwords are not stored on Sherlock. Sherlock login nodes will delegate password authentication to the University central Kerberos service.</p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#gssapi","title":"GSSAPI","text":"<p>For compatibility with previous generations of Sherlock, GSSAPI<sup>1</sup> authentication is still allowed, and could be considered a more convenient option, as this mechanism doesn't require entering your password for each connection.</p> <p>GSSAPI authentication relies on a token system, where users obtain Kerberos ticket-granting tickets, transmit them via SSH to the server they want to connect to, which will, in turn, verify their validity. That way, passwords are never stored locally, and never transit over the network. That's why Kerberos is usually considered the most secure method to authenticate.</p> <p>To connect using GSSAPI on Sherlock, you'll need to go through a few steps<sup>2</sup>:</p> <ol> <li> <p>make sure the Kerberos user tools are installed on your local machine.    You'll need the <code>kinit</code> (and optionally <code>klist</code> and <code>kdestroy</code>) utilities.    Please refer to your OS documentation to install them if required.</p> </li> <li> <p>download and install the Stanford <code>krb5.conf</code> file, which contains    information about the Stanford Kerberos environment:</p> <pre><code>$ sudo curl -o /etc/krb5.conf https://web.stanford.edu/dept/its/support/kerberos/dist/krb5.conf\n</code></pre> </li> <li> <p>configure your SSH client, by modifying (or creating if it doesn't    exist already) the <code>.ssh/config</code> file in your home directory on your local    machine. Using a text editor, you can add the following lines to your    <code>~/.ssh/config</code> file (indentation is important):</p> <pre><code>Host login.sherlock.stanford.edu\n    GSSAPIDelegateCredentials yes\n    GSSAPIAuthentication yes\n</code></pre> </li> </ol> <p>Once everything is in place (you only need to do this once), you'll be able to test that your Kerberos installation works by running <code>kinit &lt;sunetid&gt;@stanford.edu</code>. You should get a password prompt, and upon success, you'll be able to list your Kerberos credentials with the <code>klist</code> command:</p> <pre><code>$ kinit kilian@stanford.edu\nPassword for kilian@stanford.edu:\n$ klist\nTicket cache: FILE:/tmp/krb5cc_215845_n4S4I6KgyM\nDefault principal: kilian@stanford.edu\n\nValid starting     Expires            Service principal\n07/28/17 17:33:54  07/29/17 18:33:32  krbtgt/stanford.edu@stanford.edu\n        renew until 08/04/17 17:33:32\n</code></pre> <p>Kerberos ticket expiration</p> <p>Kerberos tickets have a 25-hour lifetime. So you'll need to run the <code>kinit</code> command pretty much once a day to continue being able to authenticate to Sherlock.</p> <p>Please note that when your Kerberos ticket expire, existing Sherlock connections will not be interrupted. So you'll be able to keep connections open to Sherlock for several days without any issue.</p> <p>You're now ready to connect to Sherlock using GSSAPI. Simply SSH as usual:</p> <pre><code>$ ssh &lt;sunetid&gt;@login.sherlock.stanford.edu\n</code></pre> <p>and if everything goes well, you should directly see the two-factor (Duo) prompt, without having to enter your password.</p> <p>If you want to destroy your Kerberos ticket before its expiration, you can use the <code>kdestroy</code> command.</p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#ssh-options","title":"SSH options","text":"<p>OpenSSH offers a variety of configuration options that you can use in <code>~/.ssh/config</code> on your local computer. The following section describe some of the options you can use with Sherlock that may make connecting and transferring files more convenient.</p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#avoiding-multiple-duo-prompts","title":"Avoiding multiple Duo prompts","text":"<p>In order to avoid getting a second-factor (Duo) prompt every time you want to open a new connection to Sherlock, you can take advantage of the multiplexing features provided by OpenSSH.</p> <p>Simply add the following lines to your <code>~/.ssh/config</code> file on your local machine to activate the <code>ControlMaster</code> option. If you already have a <code>Host login.sherlock.stanford.edu</code> block in your configuration file, simply add the <code>Control*</code> option lines in the same block.</p> <pre><code>Host login.sherlock.stanford.edu\n    ControlMaster auto\n    ControlPath ~/.ssh/%l%r@%h:%p\n</code></pre> <p>It will allow SSH to re-use an existing connection to Sherlock each time you open a new session (create a new SSH connection), thus avoiding subsequent 2FA prompts once the initial connection is established.</p> <p>The slight disadvantage of this approach is that once you have a connection open to one of Sherlock's login nodes, all your subsequent connections will be using the same login node. This will somewhat defeat the purpose of the load-balancing mechanism used by the login nodes.</p> <p>Connection failure with <code>unix_listener</code> error</p> <p>If your connection fails with the following error message: <pre><code>unix_listener: \"...\" too long for Unix domain socket\n</code></pre> you're being hit by a macOS limitation, and you should replace the <code>ControlPath</code> line above by: <pre><code>ControlPath ~/.ssh/%C\n</code></pre></p>","tags":["connection"]},{"location":"docs/advanced-topics/connection/#connecting-from-abroad","title":"Connecting from abroad","text":"<p>VPN</p> <p>As a good security practice, we always recommend to use the Stanford VPN when connecting from untrusted networks.</p> <p>Access to Sherlock is not restricted to campus, meaning that you can connect to Sherlock from pretty much anywhere, including when traveling abroad.  We don't restrict inbound SSH connections to any specific IP address range or geographical location, so you shouldn't have any issue to reach the login nodes from anywhere.</p> <p>Regarding two-step authentication, University IT provides alternate authentication options when phone service or Duo Mobile push notifications are not available.</p> <ol> <li> <p>The Generic Security Service Application Program Interface (GSSAPI,   also GSS-API) is an application programming interface for programs to access   security services. It allows program to interact with security services such   as Kerberos for user authentication.\u00a0\u21a9</p> </li> <li> <p>Those instructions should work on Linux    and MacOs  computers.   For Windows , we recommend using the WSL, as   described in the Prerequisites page.\u00a0\u21a9</p> </li> </ol>","tags":["connection"]},{"location":"docs/advanced-topics/job-management/","title":"Job management","text":"","tags":["slurm"]},{"location":"docs/advanced-topics/job-management/#job-submission-limits","title":"Job submission limits","text":"<p>You may have encountered situations where your jobs get rejected at submission with errors like this:</p> <pre><code>sbatch: error: MaxSubmitJobsPerAccount\nsbatch: error: MaxSubmitJobsPerUser\n</code></pre> <p>There are a number of limits on Sherlock, that are put in place to guarantee that all of the users can have a fair access to resources and a smooth experience while using them. One of those limits is about the total number of jobs a single user (and a single group) can have in queue at any given time. This helps ensuring that the scheduler is able to continue operating in an optimal fashion, without being overloaded by a single user or group.</p> <p>To see the job submission limits on Sherlock run the <code>sh_part</code> command.</p> <p>To run longer than 2 days on the normal partition you will need to add the \"long\" QOS to your submission scripts. For example to run for exactly 3 days add the following two lines to your sbatch script:</p> <pre><code>#SBATCH --time=3-00:00:00\n#SBATCH --qos=long\n</code></pre> <p>If you have access to an owners partition you will not need to add this QOS since the MaxWall on owners is 7 days.</p>","tags":["slurm"]},{"location":"docs/advanced-topics/job-management/#minimizing-the-number-of-jobs-in-queue","title":"Minimizing the number of jobs in queue","text":"<p>It's generally a good practice to try reducing the number of jobs submitted to the scheduler, and depending on your workflow, there are various approaches for this. One solution may be to pack more work within a single job, which could help in reducing the overall number of jobs you'll have to submit.</p> <p>Imagine you have a 100-task array job, where you run 1 <code>app</code> task per array item, which looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-100\n#SBATCH -n 1\n\n./app ${SLURM_ARRAY_TASK_ID}\n</code></pre> <p>This script would create 100 jobs in queue (even though they would all be regrouped under the same job array), each using 1 CPU to run 1 task.</p> <p>Instead of that 100-task array job, you can try something like this:</p> <pre><code>#!/bin/bash\n#SBATCH --array=0-99:10\n#SBATCH -n 10\n\nfor i in {0..9}; do\n\u00a0 \u00a0 srun -n 1 ./app $((SLURM_ARRAY_TASK_ID+i)) &amp;\ndone\n\nwait # important to make sure the job doesn't exit before the background tasks are done\n</code></pre> <ul> <li><code>--array=0-99:10</code> will use job array indexes 0, 10, 20 ... 90</li> <li><code>-n 10</code> will make sure each job can be subdivided in 10 1-CPU steps</li> <li>the <code>for</code> loop will launch 10 tasks, with indexes from <code>SLURM_ARRAY_TASK_ID</code>   to <code>SLURM_ARRAY_TASK_ID + 9</code>.</li> </ul> <p>This would submit a 10-task array job, each of them running 10 steps simultaneously, on the 10 CPUs that each of the job array item will be allocated.</p> <p>In the end, you'll have run the same number of <code>app</code> instances, but you'll have divided the number of jobs submitted by 10, and allow you to submit the same amount of work to the scheduler, while staying under the submission limits.</p>","tags":["slurm"]},{"location":"docs/advanced-topics/node-features/","title":"Node features","text":"<p>In heterogeneous environments, computing resources are often grouped together into single pools of resources, to make things easier and more accessible. Most applications can run on any type of hardware, so having all resources regrouped in the same partitions maximizes utilization and make job submission much easier, as users don't have dozens of options to choose from.</p> <p>But for more specific use cases, it may be necessary to specifically select the hardware jobs will run on, either for performance or reproducibility purposes.</p> <p>To that end, all the compute nodes on Sherlock have feature tags assigned to them. Multiple characteristics are available for each node, such as their class, CPU manufacturer, generation, part number and frequency, as well as Infiniband and GPU characteristics.</p> <p>Requiring specific node features is generally not necessary</p> <p>Using node features is an advanced topic which is generally not necessary to run simple jobs on Sherlock. If you're just starting, you most likely don't need to worry about those, they're only useful in very specific cases.</p>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#available-features","title":"Available features","text":"<p>The table below lists the possible features defined for each node.</p> Feature name Description Examples <code>CLASS:xxx</code> Node type, as defined in the Sherlock catalog <code>CLASS:SH3_CBASE</code>, <code>CLASS:SH3_G4TF64</code> <code>CPU_MNF:xxx</code> CPU manufacturer <code>CPU_MNF:INTEL</code>, <code>CPU_MNF:AMD</code> <code>CPU_GEN:xxx</code> CPU generation <code>CPU_GEN:RME</code> for AMD Rome<code>CPU_GEN:SKX</code> for Intel Skylake <code>CPU_SKU:xxx</code> CPU name <code>CPU_SKU:5118</code>, <code>CPU_SKU:7502P</code> <code>CPU_FRQ:xxx</code> CPU core base frequency <code>CPU_FRQ:2.50GHz</code>, <code>CPU_FRQ:2.75GHz</code> <code>GPU_BRD:xxx</code> GPU brand <code>GPU_BRD:GEFORCE</code>, <code>GPU_BRD:TESLA</code> <code>GPU_GEN:xxx</code> GPU generation <code>GPU_GEN:VLT</code> for Volta<code>GPU_GEN:AMP</code> for Ampere <code>GPU_SKU:xxx</code> GPU name <code>GPU_SKU:A100_SXM4</code>, <code>GPU_SKU:RTX_3090</code> <code>GPU_MEM:xxx</code> GPU memory <code>GPU_MEM:32GB</code>, <code>GPU_MEM:80GB</code> <code>GPU_CC:xxx</code> GPU Compute Capability <code>GPU_CC:6.1</code>, <code>GPU_CC:8.0</code> <code>IB:xxx</code> Infiniband generation/speed <code>IB:EDR</code>, <code>IB:HDR</code> <code>NO_GPU</code> special tag set on CPU-only nodes","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#listing-the-features-available-in-a-partition","title":"Listing the features available in a partition","text":"<p>All the node features available in a partition can be listed with <code>sh_node_feat</code> command.</p> <p>For instance, to list all the GPU types in the <code>gpu</code> partition:</p> <pre><code>$ sh_node_feat -p gpu | grep GPU_SKU\nGPU_SKU:P100_PCIE\nGPU_SKU:P40\nGPU_SKU:RTX_2080Ti\nGPU_SKU:V100_PCIE\nGPU_SKU:V100S_PCIE\nGPU_SKU:V100_SXM2\n</code></pre> <p>To list all the CPU generations available in the <code>normal</code> partition:</p> <pre><code>$ sh_node_feat -p normal | grep CPU_GEN\nCPU_GEN:BDW\nCPU_GEN:MLN\nCPU_GEN:RME\nCPU_GEN:SKX\n</code></pre>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#requesting-specific-node-features","title":"Requesting specific node features","text":"<p>Those node features can be used in job submission options, as additional constraints for the job, so that the scheduler will only select nodes that match the requested features.</p> <p>Adding job constraints often increases job pending times</p> <p>It's important to keep in mind that requesting specific node features usually increases job pending times in queue. The more constraints the scheduler has to satisfy, the smaller the pool of compute nodes jobs can run on. hence the longer it may take for the scheduler to find eligible resources to run those jobs.</p> <p>To specify a node feature as a job constraint, the <code>-C</code>/<code>--constraint</code> option can be used.</p> <p>For instance, to submit a job that should only run on an AMD Rome CPU, you can add the following to your job submission options:</p> <pre><code>#SBATCH -C CPU_GEN:RME\n</code></pre> <p>Or to make sure that your training job will run on a GPU with 80GB of GPU memory:</p> <pre><code>#SBATCH -G 1\n#SBATCH -C GPU_MEM:80GB\n</code></pre>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#multiple-constraints","title":"Multiple constraints","text":"<p>For more complex cases, multiple constraints could be composed in different ways, using logical operators.</p> <p>Many node feature combinations are impossible to satisfy</p> <p>Many combinations will result in impossible conditions, and will make jobs impossible to run on any node. The scheduler is usualyl able to detect this and reject the job at submission time.</p> <p>For instance, submitting a job requesting an Intel CPU on the HDR IB fabric:</p> <pre><code>#SBATCH -C 'CPU_MNF:INTEL&amp;IB:HDR'\n</code></pre> <p>will result in the following error: <pre><code>error: Job submit/allocate failed: Requested node configuration is not available\n</code></pre></p> <p>as all the compute nodes on the IB fabric use AMD CPUs. Constraints must be used carefully and sparsingly to avoid unexpected suprises.</p> <p>Some of the possible logical operations between constraints are listed below:</p>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#and","title":"<code>AND</code>","text":"<p>Only nodes with all the requested features are eligible to run the job. The ampersand sign (<code>&amp;</code>) is used as the <code>AND</code> operator. For example:</p> <pre><code>#SBATCH -C 'GPU_MEM:32GB&amp;IB:HDR'\n</code></pre> <p>will request a GPU with 32GB of memory on the HDR Infiniband fabric to run the job.</p>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#or","title":"<code>OR</code>","text":"<p>Only nodes with at least one of specified features will be eligible to run the job. The pipe sign (<code>|</code>) is used as the <code>OR</code> operator.</p> <p>In multi-node jobs, it means that nodes allocated to the job may end up having different features.  For example, the following options:</p> <pre><code>#SBATCH -N 1\n#SBATCH -C \"CPU_GEN:RME|CPU_GEN:MLN\"\n</code></pre> <p>may result in a two-node job where one node as an AMD Rome CPU, and the other node has a AMD Milan CPU.</p>","tags":["slurm","advanced"]},{"location":"docs/advanced-topics/node-features/#matching-or","title":"Matching <code>OR</code>","text":"<p>When you need all nodes in a multi-node job to have the same set of features, a matching <code>OR</code> condition can be defined by enclosing the options within square brackets (<code>[</code>,<code>]</code>).</p> <p>For instance, the following options may be used to request a job to run on nodes with the same frequency, either 2.5 GHz or 2.75GHz:</p> <pre><code>#SBATCH -C \"[CPU_FRQ:2.50GHz|CPU_FRQ:2.75GHz]\"\n</code></pre> <p>Node features are text tags</p> <p>Node features are text tags, they have no associated numerical value, meaning that they can't be compared.</p> <p>For instance, it's possible to add a constraint for GPU Compute Capability greater than 8.0. The workaround is to add a job constraint that satisfies all the possible values of that tag, like:</p> <pre><code>#SBATCH -C \"GPU_CC:8.0|GPU_CC:8.6\"\n</code></pre> <p>For more information, complete details about the <code>--constraints</code>/<code>-C</code> job submission option and its syntax can be found in the official Slurm documentation.</p>","tags":["slurm","advanced"]},{"location":"docs/getting-started/","title":"Getting started","text":""},{"location":"docs/getting-started/#prerequisites","title":"Prerequisites","text":"<p>To start using Sherlock, you will need:</p> <ul> <li> <p>an active SUNet ID,</p> <p>What is a SUNet ID?</p> <p>A SUNet ID is a unique 3-8 character account name that identifies you as a member of the Stanford community, with access to the Stanford University Network of computing resources and services. Not to be confused with University ID (a 8-digit number that appears on your Stanford ID Card), your SUNet ID is a permanent and visible part of your Stanford identity and often appears in your Stanford email address (eg. sunetid@stanford.edu).</p> <p>SUNet IDs are not managed by Research Computing. For more information, see https://accounts.stanford.edu/</p> <p>SUNet ID service levels and external collaborators</p> <p>Base-level service is sufficient for Sherlock accounts. External collaborators, or users without a SUNet ID, can be sponsored by a PI a get a sponsored SUNet ID at no cost. Please see the sponsorship page for more information.</p> </li> <li> <p>a Sherlock account,</p> </li> <li>a SSH client,</li> <li>good understanding of the concepts and terms   used throughout that documentation,</li> <li>some familiarity with Unix/Linux command-line environments, and   notions of shell scripting.</li> </ul>"},{"location":"docs/getting-started/#how-to-request-an-account","title":"How to request an account","text":"<p>To request an account, the sponsoring Stanford faculty member should email srcc-support@stanford.edu, specifying the names and SUNet IDs of his/her research team members needing an account.</p> <p>Sherlock is open to the Stanford community as a computing resource to support departmental or sponsored research, thus a faculty member's explicit consent is required for account requests.</p> <p>Sherlock is a resource for research</p> <p>Sherlock is a resource to help and support research, and is not suitable for course work, class assignments or general-use training sessions.</p> <p>There is no fee associated with using Sherlock, and no limit in the amount of accounts each faculty member can request. We will periodically ensure that all accounts associated with each PI are still active, and reserve the right to close any Sherlock account whose SUNet ID is expired.</p>"},{"location":"docs/getting-started/#ssh-clients","title":"SSH clients","text":""},{"location":"docs/getting-started/#linux","title":"Linux","text":"<p>Linux distributions usually come with a version of the OpenSSH client already installed. So no additional software installation is required. If not, please refer to your distribution's documentation to install it.</p>"},{"location":"docs/getting-started/#macos","title":"macOS","text":"<p>macOS systems usually come with a version of the OpenSSH client already installed. So no additional software installation is required</p>"},{"location":"docs/getting-started/#windows","title":"Windows","text":"<p>Microsoft Windows includes a SSH client by default, that can be used to connect to Sherlock from a Windows terminal.</p> <p>Windows also has a feature called the \"Windows Subsystem for Linux\" (WSL), which provides a Linux-like experience and make switching across systems more seamless. Please refer to the official documentation or this HOWTO for installation instructions.</p> <p>The two options above will ensure the best compatibility with the Sherlock environment. If you'd like to explore other avenues, many other SSH client implementations are available, but have not necessarily been tested with Sherlock, so your mileage may vary.</p>"},{"location":"docs/getting-started/#unixlinux-resources","title":"Unix/Linux resources","text":"<p>A full tutorial on using Unix/Linux is beyond the scope of this documentation. However, there are many tutorials for beginning to use Unix/Linux on the web.</p> <p>A few tutorials we recommend are:</p> <ul> <li>Introduction to Unix (Imperial College, London)</li> <li>The Unix Shell (Software Carpentry)</li> </ul> <p>More specifically about HPC and Research Computing:</p> <ul> <li>HPC in a day (Software Carpentry}</li> <li>Intro to HPC (HPC Carpentry)</li> <li>Research Computing Q&amp;A (Ask.Cyberinfrastructure)</li> </ul>"},{"location":"docs/getting-started/#text-editors","title":"Text editors","text":"<p>Multiple text editors are available on Sherlock. For beginners, we recommend the use of <code>nano</code>. And for more advanced uses, you'll also find below some resources about using <code>vim</code></p> <ul> <li><code>nano</code> guide (Gentoo wiki)</li> <li><code>vim</code> guide (Gentoo wiki)</li> </ul> <p>Note: you can also create/edit files with the Sherlock OnDemand File editor</p>"},{"location":"docs/getting-started/#shell-scripting","title":"Shell scripting","text":"<p>Compute jobs launched on Sherlock are most often initialized by user-written shell scripts. Beyond that, many common operations can be simplified and automated using shell scripts.</p> <p>For an introduction to shell scripting, you can refer to:</p> <ul> <li>Bash Programming - Introduction HOWTO</li> </ul>"},{"location":"docs/getting-started/connecting/","title":"Connecting to Sherlock","text":"<p>Sherlock account required</p> <p>To be able to connect to Sherlock, you must first obtain a Sherlock account.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#credentials","title":"Credentials","text":"<p>All users must have a Stanford SUNet ID and a Sherlock account to log in to Sherlock. Your Sherlock account uses the same username/password as your SUnet ID:</p> <pre><code>Username: SUNet ID\nPassword: SUNet ID password\n</code></pre> <p>To request a Sherlock account, please see the Prerequisites page.</p> <p>Resetting passwords</p> <p>Sherlock does not store your SUNet ID password. As a consequence, we are unable to reset your password. If you require password assistance, please see the SUNet Account page.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#connection","title":"Connection","text":"<p>Access to Sherlock is provided via Secure Shell (SSH) login. Most Unix-like operating systems provide an SSH client by default that can be accessed by typing the <code>ssh</code> command in a terminal window.</p> <p>To login to Sherlock, open a terminal and type the following command, where <code>&lt;sunetid&gt;</code> should be replaced by your actual SUNet ID:</p> <pre><code>$ ssh &lt;sunetid&gt;@login.sherlock.stanford.edu\n</code></pre> <p>Upon logging in, you will be connected to one of Sherlock's load-balanced login node. You should be automatically directed to the least-loaded login node at the moment of your connection, which should give you the best possible environment to work.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#host-keys","title":"Host keys","text":"<p>Upon your very first connection to Sherlock, you will be greeted by a warning such as :</p> <pre><code>The authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>The same warning will be displayed if your try to connect to one of the Data Transfer Node (DTN):</p> <pre><code>The authenticity of host 'dtn.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>This warning is normal: your SSH client warns you that it is the first time it sees that new computer. To make sure you are actually connecting to the right machine, you should compare the ECDSA key fingerprint shown in the message with one of the fingerprints below:</p> Key type Key Fingerprint RSA <code>SHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA</code>legacy format: <code>f5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41</code> ECDSA <code>SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg</code>legacy format: <code>70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64</code> <p>If they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store that key and will verify it for every subsequent SSH connection, to make sure that the server you're connecting to is indeed Sherlock.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#host-keys-warning","title":"Host keys warning","text":"<p>If you've connected to Sherlock 1.0 before, there's a good chance the Sherlock 1.0 keys were stored by your local SSH client. In that case, when connecting to Sherlock 2.0 using the <code>sherlock.stanford.edu</code> alias, you will be presented with the following message:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: POSSIBLE DNS SPOOFING DETECTED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe RSA host key for sherlock.stanford.edu has changed, and the key for\nthe corresponding IP address 171.66.97.101 is unknown. This could\neither mean that DNS SPOOFING is happening or the IP address for the\nhost and its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle\nattack)!  It is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA.\nPlease contact your system administrator.\n</code></pre> <p>You can just check that the SHA256 key listed in that warning message correctly matches the one listed in the table above, and if that's the case, you can safely remove the <code>sherlock.stanford.edu</code> entry from your <code>~/.ssh/known_hosts</code> file with the following command on your local machine:</p> <pre><code>$ ssh-keygen -R sherlock.stanford.edu\n</code></pre> <p>and then connect again. You'll see the first-connection prompt mentioned above, and your SSH client will store the new keys for future connections.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#authentication","title":"Authentication","text":"","tags":["connection"]},{"location":"docs/getting-started/connecting/#password","title":"Password","text":"<p>To ease access and increase compatibility<sup>1</sup> with different platforms, Sherlock allows a simple password-based authentication mechanism for SSH.<sup>2</sup>.</p> <p>Upon connection, you will be asked for your SUNet ID password with the following prompt:</p> <pre><code>&lt;sunetid&gt;@login.sherlock.stanford.edu's password:\n</code></pre> <p>Enter your password, and if it's correct, you should see the following line:</p> <pre><code>Authenticated with partial success.\n</code></pre>","tags":["connection"]},{"location":"docs/getting-started/connecting/#second-factor-2fa","title":"Second factor (2FA)","text":"<p>Sherlock implements Stanford's Minimum Security Standards policies which mandate two-step authentication to access the cluster.</p> <p>Two-step authentication protects your personal information and credentials by combining something only you know (your password) with something only you have (your phone, tablet or token). This prevents an attacker who would steal your password to actually use it to impersonate you. For more details about two-step authentication at Stanford, please refer to the University IT two-step page.</p> <p>After successfully entering your password, you'll be prompted for your second authentication factor with a message like this:</p> <pre><code>Duo two-factor login for &lt;sunetid&gt;\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 9)\n\nPasscode or option (1-3):\n</code></pre> <p>Avoiding two-factor prompt on each connection</p> <p>If you routinely open multiple sessions to Sherlock, having to confirm each one of them with a second authentication factor could rapidely become cumbersome. To work around this, the OpenSSH client allows multiplexing channels and re-using existing authenticated for opening new sessions. Please see the Advanced Connection Options page for more details.</p> <p>If your second factor is accepted, you'll see the following message:</p> <pre><code>Success. Logging you in...\n</code></pre>","tags":["connection"]},{"location":"docs/getting-started/connecting/#troubleshooting","title":"Troubleshooting","text":"","tags":["connection"]},{"location":"docs/getting-started/connecting/#timeouts","title":"Timeouts","text":"<p>If you ever encounter timeout errors when connecting to Sherlock, like these:</p> <pre><code>$ ssh login.sherlock.stanford.edu\nssh: connect to host login.sherlock.stanford.edu port 22: Operation timed out\n</code></pre> <p>you can try to either:</p> <ul> <li>switch to a wired connection if you're connecting over wifi,</li> <li>connect via the Stanford VPN</li> </ul>","tags":["connection"]},{"location":"docs/getting-started/connecting/#authentication-failures","title":"Authentication failures","text":"<p>Excessive authentication failures</p> <p>Entering an invalid password multiple times will result in a (temporary) ban of your IP address.</p> <p>To prevent brute-force password guessing attacks on Sherlock login nodes, we automatically block IP addresses that generate too many authentication failures in a given time span. This results in a temporary ban of the infringing IP address, and the impossibility for the user to connect to Sherlock from that IP address.</p> <p>When this happens, your SSH connection attempts will result in the following error:</p> <pre><code>ssh: connect to host login.sherlock.stanford.edu port 22: Connection refused\n</code></pre> <p>IP blocked by this mechanism will automatically be authorized again after a few minutes.</p> <p>SSHFS on macOS</p> <p>SSHFS on macOS is known to try to automatically reconnect filesystem mounts after resuming from sleep or uspend, even without any valid credentials.  As a result, it will generate a lot of failed connection attempts and likely make your IP address blacklisted on login nodes.</p> <p>Make sure to unmount your SSHFS drives before putting your macOS system to sleep to avoid this situation.</p> <p>VPN</p> <p>If your IP got blocked and you have an urgent need to connect, before the automatic blacklist expiration, we recommend trying to connect through Stanford's VPN: your computer will then use a different IP address and will not be affected by the ban on your regular IP address.</p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#login","title":"Login","text":"<p>Congratulations! You've successfully connected to Sherlock. You'll be greeted by the following message of the day:</p> <pre><code>             --*-*- Stanford Research Computing Center -*-*--\n                  ____  _               _            _\n                 / ___|| |__   ___ _ __| | ___   ___| | __\n                 \\___ \\| '_ \\ / _ \\ '__| |/ _ \\ / __| |/ /\n                  ___) | | | |  __/ |  | | (_) | (__|   &lt;\n                 |____/|_| |_|\\___|_|  |_|\\___/ \\___|_|\\_\\\n\n-----------------------------------------------------------------------------\n  This system is for authorized users only and users must comply with all\n  Stanford computing, network and research policies. All activity may be\n  recorded for security and monitoring purposes. For more information, see\n  https://doresearch.stanford.edu/policies/research-policy-handbook and\n  https://adminguide.stanford.edu/chapter-6/subchapter-2/policy-6-2-1\n-----------------------------------------------------------------------------\n  Sherlock is *NOT* approved for storing or processing HIPAA, PHI, PII nor\n  any kind of High Risk data. Users are responsible for the compliance of\n  their data.\n  See https://uit.stanford.edu/guide/riskclassifications for details.\n-----------------------------------------------------------------------------\n\n        Docs         https://www.sherlock.stanford.edu/docs\n        Support      https://www.sherlock.stanford.edu/docs/#support\n\n        Web          https://www.sherlock.stanford.edu\n        News         https://news.sherlock.stanford.edu\n        Status       https://status.sherlock.stanford.edu\n\n-----------------------------------------------------------------------------\n</code></pre> <p>Once authenticated to Sherlock, you'll see the following prompt:</p> <p><code> [&lt;sunetid&gt;@sh03-ln01 login! ~]$ </code></p> <p>It indicates the name of the login node you've been connected to, and a reminder that you're actually connected to a login node, not a compute node.</p> <p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p> <p>By contrast, the shell prompt on a compute node looks like this:</p> <p><code> [&lt;sunetid&gt;@sh03-01n01 ~]$ </code></p>","tags":["connection"]},{"location":"docs/getting-started/connecting/#start-computing","title":"Start computing","text":"<p>To start computing, there's still a extra step required, which is requesting resources to run your application. It's all described in the next section.</p> <ol> <li> <p>On Sherlock 1.0, GSSAPI tokens (based on Kerberos tickets) were the only allowed authentication method, which could cause some interoperability with third-party SSH clients.\u00a0\u21a9</p> </li> <li> <p>For other methods of authentication, see the Advanced  Connection Options page.\u00a0\u21a9</p> </li> </ol>","tags":["connection"]},{"location":"docs/getting-started/submitting/","title":"Submitting jobs","text":"","tags":["slurm"]},{"location":"docs/getting-started/submitting/#principle","title":"Principle","text":"<p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#requesting-resources","title":"Requesting resources","text":"<p>A mandatory prerequisite for running computational tasks on Sherlock is to request computing resources. This is done via a resource scheduler, whose very purpose is to match compute resources in the cluster (CPUs, GPUs, memory, ...) with user resource requests.</p> <p>The scheduler provides three key functions:</p> <ol> <li>it allocates access to resources (compute nodes) to users for some duration    of time so they can perform work.</li> <li>it provides a framework for starting, executing, and monitoring work    (typically a parallel job such as MPI) on a set of allocated nodes.</li> <li>it arbitrates contention for resources by managing a queue of pending jobs</li> </ol>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#slurm","title":"Slurm","text":"<p>Sherlock uses Slurm, an open-source resource manager and job scheduler, used by many of the world's supercomputers and computer clusters.</p> <p>Slurm supports a variety of job submission techniques. By accurately requesting the resources you need, you\u2019ll be able to get your work done.</p> <p>Wait times in queue</p> <p>As a quick rule of thumb, it's important to keep in mind that the more resources your job requests (CPUs, GPUs, memory, nodes, and time), the longer it may have to wait in queue before it could start.</p> <p>In other words: accurately requesting resources to match your job's needs will minimize your wait times.</p>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#how-to-submit-a-job","title":"How to submit a job","text":"A job consists in two parts: resource requests and job steps. <p>Resource requests describe the amount of computing resource (CPUs, GPUs, memory, expected run time, etc.) that the job will need to successfully run.</p> <p>Job steps describe tasks that must be executed.</p>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#batch-scripts","title":"Batch scripts","text":"<p>The typical way of creating a job is to write a job submission script. A submission script is a shell script (e.g. a Bash script) whose first comments, if they are prefixed with <code>#SBATCH</code>, are interpreted by Slurm as parameters describing resource requests and submissions options<sup>1</sup>.</p> <p>The submission script itself is a job step. Other job steps are created with the <code>srun</code> command.</p> <p>For instance, the following script would request one task with one CPU for 10 minutes, along with 2 GB of memory, in the default partition:</p> submit.sh<pre><code>#!/bin/bash\n#\n#SBATCH --job-name=test\n#\n#SBATCH --time=10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2G\n\nsrun hostname\nsrun sleep 60\n</code></pre> <p>When started, the job would run a first job step <code>srun hostname</code>, which will launch the command <code>hostname</code> on the node on which the requested CPU was allocated. Then, a second job step will start the <code>sleep</code> command.</p> <p>You can create this job submission script on Sherlock using a text editor such as <code>nano</code> or <code>vim</code>, and save it as <code>submit.sh</code>.</p> <p><code>#SBATCH</code> directives syntax</p> <code>#SBATCH</code> directives must be at the top of the script <p>Slurm will ignore all <code>#SBATCH</code> directives after the first non-comment line (that is, the first line in the script that doesn't start with a <code>#</code> character). Always put your <code>#SBATCH</code> parameters at the top of your batch script.</p> Spaces in parameters will cause <code>#SBATCH</code> directives to be ignored <p>Slurm will ignore all <code>#SBATCH</code> directives after the first white space. For instance directives like those: <pre><code>#SBATCH --job-name=big job\n</code></pre> <pre><code>#SBATCH --mem=16 G\n</code></pre> <pre><code>#SBATCH --partition=normal, owners\n</code></pre> will cause all following <code>#SBATCH</code> directives to be ignored and the job to be submitted with the default parameters.</p>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#job-submission","title":"Job submission","text":"<p>Once the submission script is written properly, you can submit it to the scheduler with the <code>sbatch</code> command. Upon success, <code>sbatch</code> will return the ID it has assigned to the job (the jobid).</p> <pre><code>$ sbatch submit.sh\nSubmitted batch job 1377\n</code></pre>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#check-the-job","title":"Check the job","text":"<p>Once submitted, the job enters the queue in the <code>PENDING</code> state. When resources become available and the job has sufficient priority, an allocation is created for it and it moves to the <code>RUNNING</code> state. If the job completes correctly, it goes to the <code>COMPLETED</code> state, otherwise, its state is set to <code>FAILED</code>.</p> <p>You'll be able to check the status of your job and follow its evolution with the <code>squeue -u $USER</code> command:</p> <pre><code>$ squeue -u $USER\n     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n      1377    normal     test   kilian  R       0:12      1 sh02-01n01\n</code></pre> <p>The scheduler will automatically create an output file that will contain the result of the commands run in the script file. That output file is names <code>slurm-&lt;jobid&gt;.out</code> by default, but can be customized via submission options. In the above example, you can list the contents of that output file with the following commands:</p> <pre><code>$ cat slurm-1377.out\nsh02-01n01\n</code></pre> <p>Congratulations, you've submitted your first batch job on Sherlock!</p>","tags":["slurm"]},{"location":"docs/getting-started/submitting/#whats-next","title":"What's next?","text":"<p>Actually, quite a lot. Although you now know how to submit a simple batch job, there are many other options and areas to explore in the next sections:</p> <ul> <li>Data transfer</li> <li>Storage</li> <li>Running jobs</li> </ul> <ol> <li> <p>You can get the complete list of parameters by referring to the   <code>sbatch</code> manual page (<code>man sbatch</code>).\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"docs/software/","title":"Software on Sherlock","text":""},{"location":"docs/software/#available-software","title":"Available software","text":"<p>A set of supported software installations is provided for use on Sherlock. This software is made available through a Software Modules system. For the complete list of available software, please refer to the Software List page.</p> <p>Licensed software can be used on Sherlock, under certain conditions. Feel free to contact us for more details or if you have questions. For more information about purchasing software licenses, you can contact the Stanford Software Licensing office.</p>"},{"location":"docs/software/#installation-requests","title":"Installation requests","text":"<p>Installation requests</p> <p>The Stanford Research Computing team installs, for general use, a set of libraries, tools and software applications that are commonly used across many research groups.  However, our staff resources are quite limited and don't allow us to build nor maintain custom software applications that may be requested by or be of use to a small number of users.</p> <p>We strongly encourage users to build custom and field- or domain-specific software themselves, and install it in their own personal or group shared directories. That way, they can share the software installations with the rest of the users in their group, if necessary.</p> <p>Users may even maintain and publish their own local module files to dynamically configure a running environment to use the software. They could share those modules with other users to simplify the use of their own custom software installations.</p> <p>Installing your own software</p> <p>For more information about building your own software on Sherlock, please see the Software Installation page</p> <p>If the software you need is not in the list of available software, and you have trouble installing it on your own, please contact us with as much details about the package as possible, and we will try to help you install it.</p> <p>If it's a widely used software that could benefit multiple users across different scientific communities, we will consider install it globally as resources permit<sup>1</sup>.</p>"},{"location":"docs/software/#contributed-software","title":"Contributed software","text":"<p>PI groups and labs can share their software installations and modules with the whole Sherlock user community, and let everyone benefit from their tuning efforts and software developments.</p> <p>Contributed software is supported and maintained by each lab, and contact information is usually provided in the <code>contribs</code> module. See the Modules page for more information about using software modules on Sherlock.</p> <p>If you're interested in sharing your software installations beyond your own group on Sherlock, please let us know, and we'll get in touch.</p> <ol> <li> <p>Software requests, including version upgrades, are fulfilled in   the order they are received, and as time permits. We don't have any dedicated   team for software installations, and requests are handled along with other   duties, typically within two to three weeks of being received.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/install/","title":"Installation","text":"<p>Software installation requests</p> <p>For more information about software installation requests, please see the Software Overview page</p> <p>If the software package or version you need is not available in the list of provided software, you may compile and install it yourself. The recommended location for user-installed software is the <code>$GROUP_HOME</code> group shared directory, which is snapshotted and replicated off-site, and can easily be shared with members of a research group.</p> <p> Work in progress </p> <p>This page is a work in progress and is not complete yet. We are actively working on adding more content and information.</p>"},{"location":"docs/software/list/","title":"List","text":""},{"location":"docs/software/list/#software-list","title":"Software list","text":"<p>The full list of software centrally installed and managed on Sherlock is in the tables below.</p> <p>Permanent work in progress</p> <p>Software installations on Sherlock are an ever ongoing process. We're continuously adding new software to the list. If you're looking for something that is not in the list, there may be other options.</p> <p>Subscribe to updates</p> <p> Never want to miss a software update again? Stay up-to-date with new software updates by following the  Sherlock software update RSS feed.</p>"},{"location":"docs/software/list/#categories","title":"Categories","text":"<p>Software modules on Sherlock are organized in categories, by scientific field or functional class. It means that you will have to first load a category module before getting access to individual modules.  The <code>math</code> and <code>devel</code> categories are loaded by default. See the Modules page for further details and examples.</p> <p>We currently provide 732 software modules, in 8 categories, covering 115 fields of science:</p> <ul> <li> <p><code>biology</code>      clinical science, computational biology, cryo-em, genomics, microscopy, molecular biology, neurology, neurosciences, pathology, phylogenetics, population genetics, radiology, single-cell omics, workflow management   </p> </li> <li> <p><code>chemistry</code>      cheminformatics, computational chemistry, crystallography, docking, electrostatics, modelling, molecular dynamics, quantum chemistry, tools, x-ray spectroscopy   </p> </li> <li> <p><code>devel</code>      AI, analytics, build, communications, compiler, data, data analytics, debug, engine, framework, IDE, language, lib, library, mpi, networking, package management, parser, profiling, runtime, scm, static analysis   </p> </li> <li> <p><code>humsci</code>      NLP   </p> </li> <li> <p><code>math</code>      computational geometry, data science, deep learning, graph computing, lib, linear algebra, machine learning, numerical analysis, numerical library, optimization, scientific computing, speech analysis, statistics, symbolic, technical computing, topic modelling   </p> </li> <li> <p><code>physics</code>      astronomy, CFD, climate modeling, fuild dynamics, geophysics, geoscience, lib, magnetism, materials science, micromagnetics, particle, photonics, physics-ml, quantum information science, quantum mechanics   </p> </li> <li> <p><code>system</code>      assembler, backup, benchmark, checkpointing, CI/CD, cloud interface, compiler, compression, containers, database, document management, document processing, file management, file transfer, framework, graphics, hardware, hashing, job management, language, libs, media, performance, resource monitoring, scm, shell, testing, tools   </p> </li> <li> <p><code>viz</code>      data, gis, graphs, image processing, imaging, molecular visualization, plotting, remote display, toolkit   </p> </li> </ul> <p>Licensed software</p> <p>Access to software modules marked with  in the tables below is restricted to properly licensed user groups.</p> <p>Stanford Research Computing is not funded to provide commercial software on Sherlock and researchers are responsible for the costs of purchasing and renewing commercial software licenses. For more information, please feel free to contact us and see the Stanford Software Licensing page for purchasing information.</p> <p>Additional flags and features</p> <p>Some of the modules listed below have been built to support specific architectures or parallel execution modes:</p> <ul> <li>versions marked with  support GPU acceleration</li> <li>versions marked with  support MPI parallel     execution</li> <li>versions marked with  are the default version for     the module</li> </ul>"},{"location":"docs/software/list/#biology","title":"biology","text":"Field Module\u00a0name Version(s) URL Description clinical science <code>simvascular</code> <code>20180704</code> Website Simvascular is a blood flow simulation and analysis toolkit. This module provides the svFSI (Fluid Solid Interaction) solver. computational biology <code>py-biopython</code> <code>1.70_py27</code><code>1.79_py36</code><code>1.79_py39</code><code>1.84_py312</code> Website Biopython is a set of freely available tools for biological computation written in Python. computational biology <code>py-cellpose</code> <code>4.0.4_py312</code> Website A generalist algorithm for cellular segmentation with human-in-the-loop capabilities computational biology <code>rosetta</code> <code>3.8</code> <code>3.14</code> Website Rosetta is the premier software suite for modeling macromolecular structures. As a flexible, multi-purpose application, it includes tools for structure prediction, design, and remodeling of proteins and nucleic acids. cryo-em <code>ctffind</code> <code>4.1.13</code> Website ctffind is a program for finding CTFs of electron micrographs. cryo-em <code>eman2</code> <code>2.2</code> <code>2.91</code> Website EMAN2 is a broadly based greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes. cryo-em <code>imod</code> <code>4.9.12</code> <code>4.11.5</code> Website IMOD is a set of image processing, modeling and display programs used for tomographic reconstruction and for 3D reconstruction of EM serial sections and optical sections. cryo-em <code>motioncor2</code> <code>1.3.1</code> <code>1.5.0</code> <code>1.6.4</code> Website MotionCor2 is a multi-GPU accelerated program which corrects anisotropic image motion at the single pixel level. cryo-em <code>py-topaz</code> <code>0.2.4_py36</code> <code>0.2.5_py39</code> Website A pipeline for particle detection in cryo-electron microscopy images using convolutional neural networks trained from positive and unlabeled examples. cryo-em <code>relion</code> <code>2.0.3</code> <code>2.1</code> <code>4.0.1</code> Website RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class averages in electron cryo-microscopy (cryo-EM). genomics <code>abpoa</code> <code>1.5.4</code> Website abPOA is a SIMD-based C library for fast partial order alignment using adaptive band. genomics <code>angsd</code> <code>0.919</code><code>0.931</code> Website ANGSD is a software for analyzing next generation sequencing data. genomics <code>augustus</code> <code>3.3.2</code> Website AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences. genomics <code>bamtools</code> <code>2.5.1</code> Website BamTools is a project that provides both a C++ API and a command-line toolkit for reading, writing, and manipulating BAM (genome alignment) files. genomics <code>bases2fastq</code> <code>2.0.0</code> Website The Bases2Fastq Software demultiplexess sequencing data from Element instruments and converts base calls into FASTQ files for secondary analysis with the FASTQ-compatible software of your choice. genomics <code>bcftools</code> <code>1.6</code><code>1.8</code><code>1.16</code> Website BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF. genomics <code>bcl-convert</code> <code>4.2.7</code><code>4.3.6</code> Website The BCL Convert App generates demultiplexed FASTQ files from a run as input. genomics <code>bcl2fastq</code> <code>2.20</code> Website The bcl2fastq2 conversion software can be used to convert BCL files from MiniSeq, MiSeq, NextSeq, HiSeq, iSeq and NovaSeq sequening systems. genomics <code>bedops</code> <code>2.4.40</code> Website BEDOPS is an open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale. genomics <code>bedtools</code> <code>2.27.1</code><code>2.30.0</code> Website The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. genomics <code>bgen</code> <code>1.1.4</code> Website bgen is the reference implementation of the BGEN format, a binary file format for imputed genotype and haplotype data. genomics <code>bowtie</code> <code>1.2.2</code> Website Bowtie is an ultrafast, memory-efficient short read aligner. genomics <code>bowtie2</code> <code>2.3.4.1</code><code>2.5.4</code> Website Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. genomics <code>breseq</code> <code>0.38.1</code> Website breseq is a computational pipeline for finding mutations relative to a reference sequence in short-read DNA resequencing data. genomics <code>bwa</code> <code>0.7.17</code> Website BWA (Burrows-Wheeler Aligner) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. genomics <code>canu</code> <code>1.8</code> Website A single molecule sequence assembler for genomes large and small. genomics <code>cellranger</code> <code>7.1.0</code> Website Cell Ranger is a set of analysis pipelines that process Chromium  single-cell RNA-seq output to align reads, generate gene-cell matrices and perform clustering and gene expression analysis. genomics <code>cellranger-arc</code> <code>2.1.0</code> Website Cell Ranger ARC is an advanced analytical suite designed for the Chromium Single Cell Multiome ATAC + Gene Expression sequencing. genomics <code>cellranger-atac</code> <code>2.1.0</code> Website Cell Ranger ATAC is a set of analysis pipelines that process Chromium Single Cell ATAC data. genomics <code>cellsnp-lite</code> <code>1.2.3</code> Website Cellsnp-lite is a C/C++ tool for efficient genotyping bi-allelic SNPs on single cells. genomics <code>cufflinks</code> <code>2.2.1</code> Website Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. genomics <code>dorado</code> <code>0.3.4</code><code>0.5.3</code><code>0.7.3</code><code>0.9.6</code><code>1.0.0</code><code>1.1.0</code> Website Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads. genomics <code>fastqc</code> <code>0.11.8</code> Website FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. genomics <code>fasttree</code> <code>2.2.0</code> Website FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences. genomics <code>fastx_toolkit</code> <code>0.0.14</code> Website The FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing. genomics <code>freebayes</code> <code>1.2.0</code> Website FreeBayes is a Bayesian genetic variant detector designed to find small polymorphisms. genomics <code>gatk</code> <code>4.1.0.0</code><code>4.1.4.1</code><code>4.6.0.0</code> Website GATK (Genome Analysis Toolkit) offers a wide variety of tools with a primary focus on variant discovery and genotyping. genomics <code>gemma</code> <code>0.98.5</code> Website GEMMA is a software toolkit for fast application of linear mixed models (LMMs) and related models to genome-wide association studies (GWAS) and other large-scale data sets. genomics <code>hic-pro</code> <code>2.10.0</code> Website HiC-Pro: An optimized and flexible pipeline for Hi-C data processing. genomics <code>hifiasm</code> <code>0.25.0</code> Website A haplotype-resolved assembler for accurate Hifi reads. genomics <code>hisat2</code> <code>2.1.0</code> Website HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome). genomics <code>hmmer</code> <code>3.4</code> Website HMMER is used for searching sequence databases for sequence homologs, and for making sequence alignments. genomics <code>htslib</code> <code>1.6</code><code>1.8</code><code>1.10.2</code><code>1.14</code><code>1.16</code> Website C library for high-throughput sequencing data formats. genomics <code>jasmine</code> <code>2.4.0</code> Website Call select base modifications in PacBio HiFi reads genomics <code>jellyfish</code> <code>2.2.10</code> Website A fast multi-threaded k-mer counter. genomics <code>kalign</code> <code>3.4.0</code> Website A fast multiple sequence alignment program. genomics <code>kallisto</code> <code>0.44.0</code> <code>0.46.1</code><code>0.50.1</code> Website kallisto is a program for quantifying abundances of transcripts from RNA-Seq data using high-throughput sequencing reads. genomics <code>longshot</code> <code>1.0.0</code> Website Longshot is a variant calling tool for diploid genomes using long error prone reads. genomics <code>mafft</code> <code>7.525</code> Website Multiple alignment program for amino acid or nucleotide sequences. genomics <code>metal</code> <code>20110325</code> Website The METAL software is designed to facilitate meta-analysis of large datasets (such as several whole genome scans) in a convenient, rapid and memory efficient manner. genomics <code>minimap2</code> <code>2.30</code> Website A versatile pairwise aligner for genomic and spliced nucleotide sequences. genomics <code>mixcr</code> <code>2.1.12</code><code>4.6.0</code> Website MiXCR is a universal framework that processes big immunome data from raw sequences to quantitated clonotypes. genomics <code>mmseqs2</code> <code>18-8cc5c</code> Website MMseqs2 (Many-against-Many sequence searching) is a software suite to search and cluster huge protein and nucleotide sequence sets. genomics <code>mummer</code> <code>4.0.1</code> Website MUMmer is a versatile alignment tool for DNA and protein sequences. genomics <code>ncbi-blast+</code> <code>2.6.0</code><code>2.7.1</code><code>2.11.0</code><code>2.16.0</code> Website NCBI BLAST+ is a suite of command-line tools to run BLAST (Basic Local Alignment Search Tool), an algorithm for comparing primary biological sequence information. genomics <code>ncbi-vdb</code> <code>3.0.7</code> Website NCBI VDB is the database engine used by NCBI SRA tools. genomics <code>plink</code> <code>1.07</code><code>1.90b5.3</code><code>2.0a1</code><code>2.0a2</code><code>2.0a7</code> Website PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. genomics <code>popscle</code> <code>0.1</code> Website popscle is a suite of population scale analysis tools for single-cell genomics data. genomics <code>py-abpoa</code> <code>1.5.4_py312</code> Website Python bindings for abPOA, a SIMD-based C library for fast partial order alignment using adaptive band. genomics <code>py-busco</code> <code>3.0.2_py27</code> Website Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs (BUSCO). genomics <code>py-bx-python</code> <code>0.8.1_py27</code><code>0.8.13_py39</code> Website Tools for manipulating biological data, particularly multiple sequence alignments. genomics <code>py-crested</code> <code>1.2.1_py312</code> Website CREsted (Cis-Regulatory Element Sequence Training, Explanation, and Design) is an easy-to-use deep learning package for training enhancer models on single-cell ATAC sequencing (scATAC-seq) data. genomics <code>py-cutadapt</code> <code>1.18_py27</code> <code>1.18_py36</code><code>5.1_py312</code> Website Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads. genomics <code>py-cutesv</code> <code>2.1.2_py312</code> Website Long read based human genomic structural variation detection with cuteSV genomics <code>py-deeplabcut</code> <code>2.2.3_py39</code> <code>2.3.10_py39</code> Website A software package for animal pose estimation. genomics <code>py-deeptools</code> <code>3.3.1_py36</code><code>3.5.6_py312</code> Website Tools to process and analyze deep sequencing data. genomics <code>py-demuxalot</code> <code>0.4.2_py312</code> Website Reliable, scalable, efficient demultiplexing for single-cell RNA sequencing. genomics <code>py-fithic</code> <code>1.1.3_py27</code> Website Fit-Hi-C is a tool for assigning statistical confidence estimates to chromosomal contact maps produced by genome architecture assays. genomics <code>py-htseq</code> <code>2.0.1_py39</code> Website HTSeq is a Python library to facilitate processing and analysis of data from high-throughput sequencing (HTS) experiments. genomics <code>py-macs2</code> <code>2.1.1_py27</code><code>2.2.9.1_py39</code> Website MACS (Model-based Analysis of ChIP-Seq) implements a novel ChIP-Seq analysis method. genomics <code>py-mageck</code> <code>0.5.9.4_py36</code> Website Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout (MAGeCK) is a computational tool to identify important genes from the recent genome-scale CRISPR-Cas9 knockout screens technology. genomics <code>py-mapdamage</code> <code>2.2.1_py36</code> Website mapDamage2 is a computational framework which tracks and quantifies DNA damage patterns among ancient DNA sequencing reads generated by Next-Generation Sequencing platforms. genomics <code>py-medaka</code> <code>2.1.0_py312</code> Website medaka is a tool to create consensus sequences and variant calls from nanopore sequencing data. genomics <code>py-multiqc</code> <code>1.6_py27</code> <code>1.6_py36</code><code>1.30_py312</code> Website MultiQC is a reporting tool that parses summary statistics from results and log files generated by other bioinformatics tools. genomics <code>py-obitools</code> <code>1.2.13_py27</code> Website OBITools is a set of programs designed for analyzing NGS data in a DNA metabarcoding context. genomics <code>py-orthofinder</code> <code>2.5.4_py39</code> Website OrthoFinder is a fast, accurate and comprehensive platform for comparative genomics. genomics <code>py-pybedtools</code> <code>0.8.0_py27</code><code>0.8.2_py36</code><code>0.9.0_py39</code> Website Pybedtools wraps and extends BEDTools and offers feature-level manipulations from within Python. genomics <code>py-pydeseq2</code> <code>0.5.2_py312</code> Website A Python implementation of the DESeq2 pipeline for bulk RNA-seq DEA. genomics <code>py-pysam</code> <code>0.14.1_py27</code><code>0.15.3_py36</code><code>0.18.0_py39</code><code>0.22.1_py312</code> Website Pysam is a python module for reading, manipulating and writing genomic data sets. genomics <code>py-pyspoa</code> <code>0.3.1_py312</code> Website Python bindings to spoa. genomics <code>py-scanpy</code> <code>1.8.2_py39</code><code>1.10.2_py312</code> Website Scanpy is a scalable toolkit for analyzing single-cell gene expression data. genomics <code>py-scenicplus</code> <code>1.0.0_py39</code> Website SCENIC+ is a python package to build enhancer driven gene regulatory networks (GRNs) using combined or separate single-cell gene expression (scRNA-seq) and single-cell chromatin accessibility (scATAC-seq) data. genomics <code>py-sniffles</code> <code>2.6.3_py312</code> Website Structural variation caller using third generation sequencing. genomics <code>py-svim</code> <code>2.0.0_py312</code> Website Structural Variant Identification Method using Long Reads. genomics <code>py-vcf2gwas</code> <code>0.8.9_py39</code> Website Python API for comprehensive GWAS analysis using GEMMA. genomics <code>py-vispr</code> <code>0.4.17_py36</code> Website A visualization framework for CRISPR/Cas9 knockout screens, analyzed with MAGeCK. genomics <code>racon</code> <code>1.5.0</code> Website Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads. genomics <code>regenie</code> <code>2.2.4</code> Website regenie is a C++ program for whole genome regression modelling of large genome-wide association studies. genomics <code>rsem</code> <code>1.3.3</code> Website RSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data. genomics <code>salmon</code> <code>0.12.0</code><code>1.10.0</code> Website Highly-accurate &amp; wicked fast transcript-level quantification from RNA-seq reads using lightweight alignments. genomics <code>samtools</code> <code>1.6</code><code>1.8</code><code>1.16.1</code> Website Tools (written in C using htslib) for manipulating next-generation sequencing data. genomics <code>sentieon</code> <code>202503.01</code> Website Sentieon Genomics software is a set of software tools that perform analysis of genomic data obtained from DNA sequencing. genomics <code>shapeit</code> <code>4.0.0</code> <code>4.2.2</code><code>5.1.1</code> Website SHAPEIT4 is a fast and accurate method for estimation of haplotypes (aka phasing) for SNP array and high coverage sequencing data. genomics <code>spoa</code> <code>4.1.0</code> Website SIMD partial order alignment tool/library. genomics <code>sra-tools</code> <code>2.11.0</code><code>3.0.7</code> Website The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives. genomics <code>star</code> <code>2.5.4b</code><code>2.7.10b</code> Website STAR: ultrafast universal RNA-seq aligner. genomics <code>stringtie</code> <code>2.2.1</code> Website StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. genomics <code>tophat</code> <code>2.1.1</code> Website TopHat is a fast splice junction mapper for RNA-Seq reads. genomics <code>trim_galore</code> <code>0.5.0</code><code>0.6.10</code> Website Trim Galore! is a wrapper script to automate quality and adapter trimming as well as quality control, with some added functionality to remove biased methylation positions for RRBS sequence files. genomics <code>trinity</code> <code>2.8.4</code><code>2.13.1</code> Website Trinity RNA-Seq de novo transcriptome assembly. genomics <code>vcflib</code> <code>1.0.0</code> Website A C++ library for parsing and manipulating VCF files. genomics <code>vcftools</code> <code>0.1.15</code> Website VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. genomics <code>viennarna</code> <code>2.5.1</code> Website A C code library and several stand-alone programs for the prediction and comparison of RNA secondary structures. microscopy <code>deconwolf</code> <code>0.4.5</code> Website Deconvolution of widefield microscopy images and generation of point spread functions. molecular biology <code>dssp</code> <code>4.0.3</code> Website DSSP is an application to assign secondary structure to proteins. molecular biology <code>libcifpp</code> <code>3.0.0</code> Website Library to work with mmCIF and PDB files. neurology <code>afni</code> <code>17.2.07</code><code>18.2.04</code><code>21.3.00</code> Website AFNI (Analysis of Functional NeuroImages) is a set of C programs for processing, analyzing, and displaying functional MRI (FMRI) data - a technique for mapping human brain activity. neurology <code>freesurfer</code> <code>6.0.1</code><code>7.1.1</code><code>7.2.0</code><code>7.3.2</code><code>7.4.1</code><code>8.1.0</code> Website An open source software suite for processing and analyzing (human) brain MRI images. neurosciences <code>ants</code> <code>2.1.0</code><code>2.3.1</code><code>2.4.0</code> Website ANTs computes high-dimensional mappings to capture the statistics of brain structure and function. neurosciences <code>bart</code> <code>0.7.00</code> Website BART is a toolbox for Computational Magnetic Resonance Imaging. neurosciences <code>dcm2niix</code> <code>1.0.20171215</code><code>1.0.20211006</code> Website dcm2niix is a program esigned to convert neuroimaging data from the DICOM format to the NIfTI format. neurosciences <code>fsl</code> <code>5.0.10</code> <code>6.0.7.10</code> Website FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. neurosciences <code>mricron</code> <code>20160502</code> Website MRIcron is a cross-platform NIfTI format image viewer. neurosciences <code>mrtrix</code> <code>0.3.16</code><code>3.0.3</code> Website MRtrix3 provides a set of tools to perform various types of diffusion MRI analyses, from various forms of tractography through to next-generation group-level analyses. neurosciences <code>py-mdt</code> <code>0.10.9_py36</code> Website The Maastricht Diffusion Toolbox, MDT, is a framework and library for parallelized (GPU and multi-core CPU) diffusion Magnetic Resonance Imaging (MRI) modeling. neurosciences <code>py-nipype</code> <code>1.1.3_py27</code><code>1.1.3_py36</code> Website Nipype is a Python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these packages within a single workflow. neurosciences <code>py-sleap</code> <code>1.3.4_py39</code> Website SLEAP is an open source deep-learning based framework for multi-animal pose tracking. neurosciences <code>spm</code> <code>12</code> Website The SPM software package has been designed for the analysis of brain imaging data sequences. The sequences can be a series of images from different cohorts, or time-series from the same subject. neurosciences <code>workbench</code> <code>1.3.1</code> Website Connectome Workbench is an open source, freely available visualization and discovery tool used to map neuroimaging data, especially data generated by the Human Connectome Project. pathology <code>openslide</code> <code>3.4.1</code> Website OpenSlide is a C library that provides a simple interface to read whole-slide images (also known as virtual slides). pathology <code>py-openslide-python</code> <code>1.1.1_py27</code> <code>1.1.1_py36</code> Website OpenSlide Python is a Python interface to the OpenSlide library. phylogenetics <code>py-ete</code> <code>3.0.0_py27</code> Website A Python framework for the analysis and visualization of trees. population genetics <code>py-admixfrog</code> <code>0.6.1_py36</code> Website Admixfrog is a HMM to infer ancestry frogments (fragments) from low-coverage, contaminated data. radiology <code>nbia-data-retriever</code> <code>4.2</code> Website The NBIA Data Retriever is an application to download radiology images from the TCIA Radiology Portal. single-cell omics <code>py-pertpy</code> <code>1.0.0_py312</code> Website Perturbation Analysis in the scverse ecosystem. single-cell omics <code>py-scvi-tools</code> <code>1.2.1_py312</code> Website scvi-tools (single-cell variational inference tools) is a package for probabilistic modeling of single-cell omics data. workflow management <code>nextflow</code> <code>23.04.3</code><code>24.10.4</code><code>25.04.7</code> Website Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows."},{"location":"docs/software/list/#chemistry","title":"chemistry","text":"Field Module\u00a0name Version(s) URL Description cheminformatics <code>py-rdkit</code> <code>2018.09.1_py27</code> <code>2018.09.1_py36</code><code>2022.09.1_py39</code><code>2024.3.2_py312</code> Website RDKit is a collection of cheminformatics and machine-learning software written in C++ and Python. computational chemistry <code>gaussian</code> <code>g16.A03</code> <code>g16.B01</code> Website Gaussian is a general purpose computational chemistry software package. computational chemistry <code>libint</code> <code>1.1.4</code><code>2.0.3</code><code>2.6.0</code> Website Libint computes molecular integrals. computational chemistry <code>libxc</code> <code>3.0.0</code><code>5.2.2</code> Website Libxc is a library of exchange-correlation functionals for density-functional theory. computational chemistry <code>nwchem</code> <code>6.8</code> <code>7.0.2</code> <code>7.2.3</code> Website NWChem is an ab initio computational chemistry software package which also includes quantum chemical and molecular dynamics functionality. computational chemistry <code>py-ase</code> <code>3.14.1_py27</code><code>3.22.1_py39</code> Website The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations. computational chemistry <code>py-chai-lab</code> <code>0.6.1_py312</code> Website Chai-1, SOTA model for biomolecular structure prediction computational chemistry <code>schrodinger</code> <code>2021-1</code> <code>2017-3</code> <code>2018-1</code> <code>2018-2</code> <code>2019-2</code> <code>2020-2</code> <code>2022-3</code> <code>2024-1</code> Website Schr\u00f6dinger Suites (Small-molecule Drug Discovery Suite, Material Science Suite, Biologics Suite) provide a set of molecular modelling software. computational chemistry <code>vasp</code> <code>5.4.1</code> <code>6.1.1</code> <code>6.3.2</code> <code>6.4.1</code> <code>6.4.3</code> <code>6.5.1</code> Website The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. crystallography <code>clipper</code> <code>2.1.20180802</code> Website Crystallographic automation and complex data manipulation libraries. crystallography <code>mmdb2</code> <code>2.0.20</code> Website A C++ toolkit for working with macromolecular coordinate files. crystallography <code>ssm</code> <code>1.4</code> Website A macromolecular superposition library. crystallography <code>vesta</code> <code>3.4.4</code> Website VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. docking <code>gnina</code> <code>1.0.2</code> Website A deep learning framework for molecular docking electrostatics <code>apbs</code> <code>1.5</code> Website APBS solves the equations of continuum electrostatics for large biomolecular assemblages. modelling <code>py-boltz</code> <code>0.3.2_py312</code> Website Boltz-1 is the state-of-the-art open-source model that predicts the 3D structure of proteins, RNA, DNA, and small molecules. molecular dynamics <code>gromacs</code> <code>2016.3</code> <code>2018</code> <code>2021.3</code> <code>2023.1</code> <code>2025.1</code> Website GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. molecular dynamics <code>lammps</code> <code>20180316</code> <code>20200303</code> <code>20230802</code> Website LAMMPS is a classical molecular dynamics code that models an ensemble of particles in a liquid, solid, or gaseous state. molecular dynamics <code>mosaics</code> <code>1.0.0</code> Website A collection of tools for characterizing membrane structure and dynamics within simulated trajectories of molecular systems. molecular dynamics <code>plumed</code> <code>2.3.2</code> Website PLUMED is an open source library for free energy calculations in molecular systems. molecular dynamics <code>py-fatslim</code> <code>0.2.2_py39</code> Website FATSLiM stands for \u201cFast Analysis Toolbox for Simulations of Lipid Membranes\u201d and its goal is to provide an efficient, yet robust, tool to extract physical parameters from MD trajectories. molecular dynamics <code>py-openmm</code> <code>7.1.1_py27</code> <code>8.1.1_py312</code> Website A high performance toolkit for molecular simulation. molecular dynamics <code>py-raspa2</code> <code>2.0.3_py27</code> Website RASPA2 is a general purpose classical simulation package that can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields. molecular dynamics <code>qbox</code> <code>1.65.0</code> Website Qbox is a First-Principles Molecular Dynamics code. molecular dynamics <code>quip</code> <code>20170901</code> <code>20220426</code> Website The QUIP package is a collection of software tools to carry out molecular dynamics simulations. quantum chemistry <code>cp2k</code> <code>4.1</code> <code>9.1</code> Website CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. quantum chemistry <code>ocean</code> <code>2.9.7</code> Website OCEAN is a versatile and user-friendly package for calculating core edge spectroscopy including excitonic effects. quantum chemistry <code>orca</code> <code>4.2.1</code> <code>5.0.0</code> <code>5.0.3</code> <code>6.0.0</code> <code>6.0.1</code> <code>6.1.0</code> Website ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry. quantum chemistry <code>quantum-espresso</code> <code>6.2.1</code> <code>6.6</code> <code>7.0</code> <code>7.1</code> Website Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. quantum chemistry <code>quantum-espresso_gpu</code> <code>1.1</code> <code>7.0</code> <code>7.1</code> Website Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. quantum chemistry <code>terachem</code> <code>1.95A</code> <code>1.96H-beta</code> Website TeraChem is general purpose quantum chemistry software designed to run on NVIDIA GPU architectures. tools <code>openbabel</code> <code>3.1.1</code> Website Open Babel is a chemical toolbox designed to speak the many languages of chemical data. tools <code>py-openbabel</code> <code>3.1.1.1_py39</code> Website Python bindings for Open Babel. x-ray spectroscopy <code>py-xraylarch</code> <code>0.9.80_py312</code> Website Larch is a open-source library and set of applications for processing and analyzing X-ray absorption and fluorescence spectroscopy data and X-ray fluorescence and diffraction image data from synchrotron beamlines."},{"location":"docs/software/list/#devel","title":"devel","text":"Field Module\u00a0name Version(s) URL Description AI <code>claude-code</code> <code>1.0.43</code> Website Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows. AI <code>codex</code> <code>0.48.0</code> Website Codex CLI is a coding agent from OpenAI that runs locally on your computer. AI <code>copilot-cli</code> <code>0.0.335</code> <code>0.0.2</code><code>0.0.353</code> Website GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal. AI <code>crush</code> <code>0.6.3</code><code>0.12.1</code> Website The glamourous AI coding agent for your favourite terminal \ud83d\udc98C AI <code>gemini-cli</code> <code>0.1.22</code> Website An open-source AI agent that brings the power of Gemini directly into your terminal. AI <code>llama.cpp</code> <code>b5760</code> Website LLM inference in C/C++ AI <code>ollama</code> <code>0.5.7</code> <code>0.9.2</code> <code>0.11.5</code> <code>0.12.6</code> Website Ollama is a free, open-source platform that allows users to run large language models (LLMs) locally. analytics <code>arrow</code> <code>19.0.1</code> Website Apache Arrow is a universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. analytics <code>py-pyarrow</code> <code>18.1.0_py312</code> Website Python library for Apache Arrow, a development platform for in-memory analytics. build <code>bazel</code> <code>0.16.1</code><code>0.26.1</code><code>0.29.1</code><code>8.2.0</code> Website Bazel is a fast, scalable, multi-language and extensible build system. build <code>bazelisk</code> <code>1.3.0</code><code>1.8.0</code> Website Bazelisk is a wrapper for Bazel written in Go. build <code>binutils</code> <code>2.38</code><code>2.45</code> Website The GNU Binutils are a collection of binary tools. build <code>cmake</code> <code>3.8.1</code><code>3.11.1</code><code>3.13.1</code><code>3.20.3</code><code>3.24.2</code><code>3.31.4</code> Website CMake is an extensible, open-source system that manages the build process in an operating system and in a compiler-independent manner. build <code>kerl</code> <code>1.8.5</code> Website Kerl is a tool to easily build and install Erlang/OTP instances. build <code>libconfig</code> <code>1.7.3</code> Website C/C++ library for processing structured configuration files. build <code>make</code> <code>4.4</code> Website GNU Make is a tool which controls the generation of executables and other non-source files of a program from the program's source files. build <code>ninja</code> <code>1.9.0</code><code>1.13.1</code> Website Ninja is a small build system with a focus on speed. build <code>py-meson</code> <code>0.51.1_py36</code><code>1.6.1_py312</code> Website Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible. build <code>py-scons</code> <code>3.0.5_py27</code><code>3.0.5_py36</code><code>4.7.0_py312</code> Website SCons is an Open Source software construction tool. communications <code>grpc</code> <code>1.17.0</code> Website gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere. compiler <code>aocc</code> <code>2.1.0</code><code>2.2.0</code> Website AMD Optimizing C/C++ Compiler - AOCC is a highly optimized C, C++ and Fortran compiler for x86 targets especially for Zen based AMD processors. compiler <code>gcc</code> <code>6.3.0</code> <code>7.1.0</code><code>7.3.0</code><code>8.1.0</code><code>9.1.0</code><code>10.1.0</code><code>10.3.0</code><code>12.4.0</code><code>14.2.0</code> Website The GNU Compiler Collection includes front ends for C, C++, Fortran, Java, and Go, as well as libraries for these languages (libstdc++, libgcj,...). compiler <code>icc</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel C++ Compiler, also known as icc or icl, is a group of C and C++ compilers from Intel compiler <code>ifort</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel Fortran Compiler, also known as ifort, is a group of Fortran compilers from Intel compiler <code>ldc</code> <code>1.26.0</code><code>1.40.0</code> Website The LLVM-based D Compiler. compiler <code>llvm</code> <code>7.0.0</code> <code>3.8.1</code><code>4.0.0</code><code>5.0.0</code><code>9.0.1</code><code>15.0.3</code><code>17.0.6</code><code>19.1.7</code> Website The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Clang is an LLVM native C/C++/Objective-C compiler, compiler <code>nvhpc</code> <code>21.5</code> <code>21.7</code> <code>22.3</code> <code>23.3</code> <code>24.7</code> <code>25.1</code> Website NVIDIA HPC Software Development Kit (SDK) including C, C++, and Fortran compilers. compiler <code>pgi</code> <code>19.10</code> Website PGI compilers and tools, including Open MPI (Community Edition). compiler <code>smlnj</code> <code>110.81</code> Website Standard ML of New Jersey (abbreviated SML/NJ) is a compiler for the Standard ML '97 programming language. data <code>h5utils</code> <code>1.12.1</code> Website h5utils is a set of utilities for visualization and conversion of scientific data in the free, portable HDF5 format. data <code>hdf5</code> <code>1.10.6</code> <code>1.10.0p1</code><code>1.10.2</code> <code>1.12.0</code><code>1.12.2</code> <code>1.14.4</code> Website HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. data <code>hiredis</code> <code>0.13.3</code> Website Hiredis is a minimalistic C client library for the Redis database. data <code>ncl</code> <code>6.4.0</code><code>6.6.2</code> Website NCL is a free interpreted language designed specifically for scientific data processing and visualization. data <code>nco</code> <code>4.8.0</code> <code>5.0.6</code> Website The NCO toolkit manipulates and analyzes data stored in netCDF-accessible formats. data <code>netcdf</code> <code>4.4.1.1</code><code>4.8.1</code> Website NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. data <code>netcdf-c</code> <code>4.9.0</code> Website NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. This module provides C libraries. data <code>netcdf-cxx</code> <code>4.3.1</code> Website NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. This module provides C++ libraries. data <code>netcdf-fortran</code> <code>4.5.4</code> Website NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. This module provides Fortran libraries. data <code>pnetcdf</code> <code>1.8.1</code> <code>1.12.3</code> Website Parallel netCDF (PnetCDF) is a parallel I/O library for accessing NetCDF files in CDF-1, 2, and 5 formats. data <code>protobuf</code> <code>3.4.0</code> <code>3.20.0</code><code>21.9</code><code>29.1</code> Website Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. data <code>py-anndata</code> <code>0.11.1_py312</code> Website anndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. data <code>py-dali</code> <code>1.44.0_py312</code> Website The NVIDIA Data Loading Library (DALI) is a GPU-accelerated library for data loading and pre-processing to accelerate deep learning applications. data <code>py-kaggle</code> <code>1.6.3_py312</code><code>1.7.4.5_py312</code> Website Official API for https://www.kaggle.com, accessible using a command line tool implemented in Python 3. data <code>py-pandas</code> <code>0.23.0_py27</code><code>0.23.0_py36</code><code>1.0.3_py36</code><code>1.3.1_py39</code><code>2.0.1_py39</code><code>2.2.1_py312</code> Website pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. data <code>py-protobuf</code> <code>3.4.0_py27</code> <code>3.4.0_py36</code><code>3.6.1_py27</code><code>3.6.1_py36</code><code>3.15.8_py36</code><code>3.20.1_py39</code><code>4.21.9_py39</code><code>5.29.1_py312</code> Website Python bindings for Google's Protocol Buffers data interchange format. data <code>py-redivis</code> <code>0.18.10_py312</code> Website Redivis client library for python. data <code>py-tables</code> <code>3.10.1_py312</code> Website A Python package to manage extremely large amounts of data. data <code>redis</code> <code>4.0.1</code> Website Redis is an open source, in-memory data structure store, used as a database, cache and message broker. data <code>zfp</code> <code>1.0.0</code> Website zfp is an open-source library for compressed floating-point and integer arrays that support high throughput read and write random access. data analytics <code>hadoop</code> <code>3.1.0</code> <code>3.3.1</code> Website The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. data analytics <code>py-pyspark</code> <code>3.2.1_py39</code><code>3.2.1_py312</code> Website Launching and controlling spark on HPC clusters data analytics <code>py-sparkhpc</code> <code>0.3_py27</code> Website Launching and controlling spark on HPC clusters data analytics <code>spark</code> <code>2.3.0</code> <code>3.2.1</code> Website Apache Spark\u2122 is a unified analytics engine for large-scale data processing. data analytics <code>xan</code> <code>0.48.0</code> Website xan is a command line tool that can be used to process CSV files directly from the shell. debug <code>gdb</code> <code>8.2.1</code> Website GDB is the GNU Project debugger. debug <code>valgrind</code> <code>3.14.0</code> Website Valgrind is an instrumentation framework for building dynamic analysis tools. engine <code>v8</code> <code>8.4.371.22</code> Website V8 is Google\u2019s open source high-performance JavaScript and WebAssembly engine, written in C++. framework <code>dotnet</code> <code>2.1.500</code><code>6.0.413</code> Website .NET is a free, cross-platform, open source developer platform for building many different types of applications. framework <code>ga</code> <code>5.8.2</code> Website Global Arrays (GA) is a Partitioned Global Address Space (PGAS) programming model. framework <code>py-kedro</code> <code>0.18.0_py39</code> Website Kedro is an open-source Python framework for creating reproducible, maintainable and modular data science code. framework <code>py-warp-lang</code> <code>1.5.0_py312</code> Website Warp is a Python framework for writing high-performance simulation and graphics code. IDE <code>code-server</code> <code>4.16.1</code><code>4.93.1</code> Website Run VS Code on any machine anywhere and access it in the browser. IDE <code>py-jupytext</code> <code>1.16.1_py39</code> Website Jupyter Notebooks as Markdown Documents, Julia, Python or R scripts. language <code>bun</code> <code>1.2.20</code> Website Bun is a fast JavaScript runtime, bundler, test runner, and package manager \u2013 all in one. language <code>cuda</code> <code>9.0.176</code> <code>8.0.61</code> <code>9.1.85</code> <code>9.2.88</code> <code>9.2.148</code> <code>10.0.130</code> <code>10.1.105</code> <code>10.1.168</code> <code>10.2.89</code> <code>11.0.3</code> <code>11.1.1</code> <code>11.2.0</code> <code>11.3.1</code> <code>11.4.1</code> <code>11.5.0</code> <code>11.7.1</code> <code>12.0.0</code> <code>12.1.1</code> <code>12.2.0</code> <code>12.4.0</code> <code>12.6.1</code> <code>12.8.0</code> Website CUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing. language <code>erlang</code> <code>21.3</code> Website Erlang is a programming language used to build massively scalable soft real-time systems with requirements on high availability. language <code>gcl</code> <code>2.6.14</code> Website GCL is the official Common Lisp for the GNU project. language <code>go</code> <code>1.9</code><code>1.14</code><code>1.18.2</code><code>1.22.7</code> Website Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. language <code>guile</code> <code>2.0.11</code><code>2.2.2</code> Website GNU Guile is the preferred extension system for the GNU Project, which features an implementation of the Scheme programming language. language <code>haskell</code> <code>8.6.5</code> Website Haskell is a statically typed, purely functional programming language with type inference and lazy evaluation. language <code>java</code> <code>1.8.0_131</code> <code>11.0.11</code><code>12.0.2</code><code>17.0.4</code><code>18.0.2</code><code>21.0.4</code> Website Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible. language <code>julia</code> <code>1.6.2</code><code>1.7.2</code><code>1.8.4</code><code>1.9.4</code><code>1.10.5</code><code>1.11.4</code> Website Julia is a high-level, high-performance dynamic programming language for numerical computing. language <code>lua</code> <code>5.3.4</code> Website Lua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description. language <code>luarocks</code> <code>2.4.3</code> Website LuaRocks is the package manager for Lua modules. language <code>manticore</code> <code>20180301</code> Website Manticore is a high-level parallel programming language aimed at general-purpose applications running on multi-core processors. language <code>nodejs</code> <code>8.9.4</code><code>9.5.0</code><code>16.13.0</code><code>18.15.0</code><code>20.18.0</code><code>24.9.0</code> Website Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. It provides the npm package manager. language <code>perl</code> <code>5.26.0</code><code>5.36.1</code> Website Perl 5 is a highly capable, feature-rich programming language with over 29 years of development. language <code>php</code> <code>7.3.0</code> Website PHP (recursive acronym for PHP: Hypertext Preprocessor) is an open source general-purpose scripting language that is especially suited for web development. language <code>py-cython</code> <code>0.27.3_py27</code><code>0.27.3_py36</code><code>0.29.21_py36</code><code>0.29.28_py39</code> Website Cython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex). language <code>py-ipython</code> <code>5.4.1_py27</code> <code>6.1.0_py36</code><code>8.3.0_py39</code><code>8.22.2_py312</code> Website IPython is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language. language <code>py-jupyter</code> <code>1.0.0_py27</code> <code>1.0.0_py36</code><code>1.0.0_py39</code> Website Jupyter is a browser-based interactive notebook for programming, mathematics, and data science. It supports a number of languages via plugins. language <code>py-jupyterlab</code> <code>2.3.2_py36</code><code>4.0.8_py39</code><code>4.3.2_py312</code> Website Jupyter is a browser-based interactive notebook for programming, mathematics, and data science. It supports a number of languages via plugins. language <code>python</code> <code>2.7.13</code> <code>3.6.1</code><code>3.9.0</code><code>3.12.1</code> Website Python is an interpreted, interactive, object-oriented programming language. language <code>ruby</code> <code>2.4.1</code><code>2.7.1</code><code>3.1.2</code> Website A dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. language <code>rust</code> <code>1.35.0</code><code>1.56.1</code><code>1.63.0</code><code>1.72.0</code><code>1.81.0</code><code>1.90.0</code> Website A language empowering everyone to build reliable and efficient software. language <code>scala</code> <code>2.12.6</code> Website Scala combines object-oriented and functional programming in one concise, high-level language. lib <code>ant</code> <code>1.10.1</code> Website Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. lib <code>boost</code> <code>1.64.0</code><code>1.69.0</code> <code>1.75.0</code> <code>1.76.0</code> <code>1.79.0</code> <code>1.87.0</code> Website Boost is a set of libraries for the C++ programming language that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing. lib <code>chai</code> <code>2.2.2</code> Website Copy-hiding array abstraction to automatically migrate data between memory spaces. lib <code>cnmem</code> <code>1.0.0</code> Website CNMeM is a simple library to help the Deep Learning frameworks manage CUDA memory. lib <code>conduit</code> <code>0.5.1</code> Website Simplified Data Exchange for HPC Simulations. lib <code>cub</code> <code>1.7.3</code> <code>1.10.0</code> Website CUB is a flexible library of cooperative threadblock primitives and other utilities for CUDA kernel programming. lib <code>cutlass</code> <code>0.1.0</code><code>3.1.0</code> Website CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA. lib <code>cvcuda</code> <code>0.13.0</code> Website CV-CUDA is an open-source project that enables building efficient cloud-scale Artificial Intelligence (AI) imaging and computer vision (CV) applications. lib <code>dtcmp</code> <code>1.1.3</code><code>1.1.5</code> Website Datatype Compare (DTCMP) Library for sorting and ranking distributed data using MPI. lib <code>eigen</code> <code>3.3.3</code><code>3.4.0</code> Website Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. lib <code>json-c</code> <code>0.18</code> Website JSON-C implements a reference counting object model that allows you to easily construct JSON objects in C, output them as JSON formatted strings and parse JSON formatted strings back into the C representation of JSON objects. lib <code>libcircle</code> <code>0.3.0</code> Website libcircle is an API for distributing embarrassingly parallel workloads using self-stabilization. lib <code>libctl</code> <code>3.2.2</code><code>4.0.1</code><code>4.5.0</code> Website libctl is a library for supporting flexible control files in scientific simulations. lib <code>libevent</code> <code>2.1.12</code> Website The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. lib <code>libgpuarray</code> <code>0.7.5</code> Website Library to manipulate tensors on the GPU. lib <code>libtree</code> <code>2.0.0</code> Website libtree prints shared object dependencies as a tree. lib <code>lwgrp</code> <code>1.0.4</code> <code>1.0.6</code> Website The Light-weight Group Library provides methods for MPI codes to quickly create and destroy process groups. lib <code>nccl</code> <code>1.3.4</code> <code>2.0.4</code> <code>2.1.15</code> <code>2.2.13</code> <code>2.3.7</code> <code>2.4.8</code> <code>2.5.6</code> <code>2.8.4</code> <code>2.11.4</code> <code>2.17.1</code> <code>2.20.5</code> <code>2.23.4</code> <code>2.27.7</code> Website NCCL (pronounced 'Nickel') is a stand-alone library of standard collective communication routines, such as all-gather, reduce, broadcast, etc., that have been optimized to achieve high bandwidth over PCIe. lib <code>pio</code> <code>2.6.2</code> Website A high-level Parallel I/O Library for structured grid applications. lib <code>pugixml</code> <code>1.12.1</code> Website Light-weight, simple and fast XML parser for C++ with XPath support. lib <code>py-cutlass</code> <code>3.1.0_py39</code> Website Python interface for CUTLASS lib <code>py-cvcuda</code> <code>0.13.0_py312</code> Website Python bindings for CV_CUDA. lib <code>py-h5py</code> <code>2.7.1_py27</code> <code>2.8.0_py36</code><code>2.10.0_py36</code><code>3.1.0_py36</code><code>3.7.0_py39</code><code>3.10.0_py312</code> Website The h5py package is a Pythonic interface to the HDF5 binary data format. lib <code>py-netcdf4</code> <code>1.3.1_py27</code> <code>1.3.1_py36</code> Website netcdf4-python is a Python interface to the netCDF C library. lib <code>py-nose</code> <code>1.3.7_py39</code> Website nose is nicer testing for python. lib <code>py-numba</code> <code>0.35.0_py27</code> <code>0.35.0_py36</code><code>0.53.1_py36</code><code>0.54.1_py39</code><code>0.59.1_py39</code><code>0.60.0_py312</code> Website Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.. lib <code>py-parsl</code> <code>1.2.0_py39</code> Website Parsl is a flexible and scalable parallel programming library for Python. lib <code>py-pycuda</code> <code>2017.1.1_py27</code> <code>2021.1_py36</code> Website PyCUDA lets you access Nvidia\u2018s CUDA parallel computation API from Python. lib <code>py-regex</code> <code>20247.24_py36</code><code>20247.24_py39</code><code>20247.24_py312</code> Website Alternative regular expression module for Python, to replace re. lib <code>py-rmm</code> <code>23.04.00_py39</code> Website Python interface for RMM lib <code>py-schwimmbad</code> <code>0.3.1_py36</code> <code>0.3.2_py39</code> Website schwimmbad provides a uniform interface to parallel processing pools and enables switching easily between local development (e.g., serial processing or with multiprocessing) and deployment on a cluster or supercomputer (via, e.g., MPI or JobLib). lib <code>py-scikit-image</code> <code>0.13.0_py27</code><code>0.14.0_py27</code><code>0.15.0_py27</code><code>0.15.0_py36</code><code>0.17.2_py36</code><code>0.19.3_py39</code><code>0.20.0_py39</code><code>0.24.0_py312</code> Website scikit-image is a collection of algorithms for image processing. lib <code>rabbitmq</code> <code>3.7.13</code> Website RabbitMQ is an open-source message broker. lib <code>raja</code> <code>0.12.1</code> Website Collection of C++ software abstractions that enable architecture portability for HPC applications. lib <code>rmm</code> <code>23.04.00</code> Website RAPIDS Memory Manager library lib <code>swig</code> <code>3.0.12</code> Website SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. lib <code>tbb</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel\u00ae Threading Building Blocks (Intel\u00ae TBB) is a widely used C++ library for shared-memory parallel programming and heterogeneous computing (intra-node distributed memory programming). lib <code>trilinos</code> <code>12.12.1</code> Website Trilinos is a collection of open-source software libraries, called packages, intended to be used as building blocks for the development of scientific applications. lib <code>xsimd</code> <code>7.6.0</code><code>8.1.0</code> Website C++ wrappers for SIMD intrinsics and parallelized, optimized mathematical functions (SSE, AVX, NEON, AVX512) lib <code>zeromq</code> <code>4.2.2</code> Website ZeroMQ (also spelled \u00d8MQ, 0MQ or ZMQ) is a high-performance asynchronous messaging library, aimed at use in distributed or concurrent applications. library <code>py-pybind11</code> <code>3.0.1_py312</code> Website Seamless operability between C++11 and Python. mpi <code>hpcx</code> <code>2.6.0</code> <code>2.7.0</code> <code>2.8.1</code> Website Mellanox HPC-X toolkit is a comprehensive software package that includes MPI and SHMEM/PGAS communications libraries. mpi <code>impi</code> <code>2017.u2</code> <code>2018.u1</code> <code>2018</code> <code>2019</code> Website Intel\u00ae MPI Library is a multi-fabric message passing library that implements the Message Passing Interface, version 3.1 (MPI-3.1) specification. mpi <code>openmpi</code> <code>4.1.2</code> <code>2.0.2</code> <code>2.1.1</code> <code>3.1.2</code> <code>4.0.3</code> <code>4.1.0</code> <code>4.1.6</code> <code>5.0.5</code> Website The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners. mpi <code>py-mpi4py</code> <code>3.0.0_py27</code> <code>3.0.3_py36</code> <code>3.1.3_py39</code> <code>3.1.5_py312</code> <code>4.0.3_py312</code> Website MPI for Python provides Python bindings for the Message Passing Interface (MPI) standard. It is implemented on top of the MPI-\u00bd/3 specification and exposes an API which grounds on the standard MPI-2 C++ bindings. networking <code>gasnet</code> <code>1.30.0</code> Website GASNet is a language-independent, low-level networking layer that provides network-independent, high-performance communication primitives tailored for implementing parallel global address space SPMD languages and libraries. networking <code>libfabric</code> <code>1.6.0</code><code>1.6.2</code><code>1.7.1</code><code>1.9.1</code><code>1.10.1</code><code>1.11.1</code><code>1.14.0</code> Website The Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric communication services to applications. Libfabric is the library that defines and exports the user-space API of OFI. networking <code>py-ucx-py</code> <code>0.24.0_py39</code> Website Python bindinbgs for UCX. networking <code>ucc</code> <code>1.3.0</code> Website UCC is a collective communication operations API and library that is flexible, complete, and feature-rich for current and emerging programming models and runtimes. networking <code>ucx</code> <code>1.3.1</code><code>1.8.1</code> <code>1.9.0</code> <code>1.10.0</code> <code>1.12.1</code> <code>1.15.0</code> <code>1.17.0</code> Website UCX is a communication library implementing high-performance messaging for MPI/PGAS frameworks. package management <code>pixi</code> <code>0.53.0</code> Website Pixi is a package management tool for developers. It allows the developer to install libraries and applications in a reproducible way. package management <code>uv</code> <code>0.8.4</code><code>0.9.5</code> Website An extremely fast Python package and project manager, written in Rust. parser <code>antlr</code> <code>2.7.7</code> Website ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files. parser <code>py-markitdown</code> <code>0.0.2_py312</code> Website Python tool for converting files and office documents to Markdown. parser <code>xerces-c</code> <code>3.2.1</code> Website Xerces-C++ is a validating XML parser written in a portable subset of C++. profiling <code>amd-uprof</code> <code>3.3.462</code> Website AMD uProf is a performance analysis tool for applications. profiling <code>darshan</code> <code>3.4.4</code><code>3.4.6</code> Website Darshan is a scalable HPC I/O characterization tool. profiling <code>nsight-systems</code> <code>2024.4</code> Website NVIDIA Nsight\u2122 Systems is a system-wide performance analysis tool designed to visualize an application\u2019s algorithms, identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest systems-on-a-chip (SoCs). profiling <code>py-scalene</code> <code>1.55.4_py39</code> <code>1.55.4_py312</code> Website Scalene is a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals runtime <code>starpu</code> <code>1.3.2</code> Website StarPU is a unified runtime system that offers support for heterogeneous multicore architectures scm <code>gh</code> <code>2.49.2</code><code>2.76.2</code> Website gh is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code. static analysis <code>shellcheck</code> <code>0.11.0</code> Website A shell script static analysis tool"},{"location":"docs/software/list/#humsci","title":"humsci","text":"Field Module\u00a0name Version(s) URL Description NLP <code>py-booknlp</code> <code>1.0.8_py312</code> Website BookNLP, a natural language processing pipeline for books NLP <code>py-spacy</code> <code>3.8.7_py312</code> Website Industrial-strength Natural Language Processing (NLP) in Python"},{"location":"docs/software/list/#math","title":"math","text":"Field Module\u00a0name Version(s) URL Description computational geometry <code>cgal</code> <code>4.10</code> Website The Computational Geometry Algorithms Library (CGAL) is a C++ library that aims to provide easy access to efficient and reliable algorithms in computational geometry. computational geometry <code>dealii</code> <code>9.4.1</code> Website deal.II is a C++ program library targeted at the computational solution of partial differential equations using adaptive finite elements. computational geometry <code>gmsh</code> <code>4.10.1</code> Website Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. computational geometry <code>opencascade</code> <code>7.6.2</code> Website Open CASCADE Technology (OCCT) is an open-source full-scale 3D geometry library computational geometry <code>polymake</code> <code>4.10</code> Website polymake is open source software for research in polyhedral geometry. computational geometry <code>qhull</code> <code>2015.2</code><code>2020.2</code> Website Qhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram. computational geometry <code>silo</code> <code>4.11</code> Website A mesh and field I/O library and scientific database. data science <code>py-rapids</code> <code>24.04_py39</code> Website NVIDIA RAPIDS is an open-source suite of GPU-accelerated data science and AI libraries with APIs that match the most popular open-source data tools. deep learning <code>cudnn</code> <code>6.0</code> <code>7.0.1</code> <code>7.0.4</code> <code>7.0.5</code> <code>7.1.4</code> <code>7.4.1.5</code> <code>7.6.4</code> <code>7.6.5</code> <code>8.1.1.33</code> <code>8.3.3.40</code> <code>8.6.0.163</code> <code>8.9.0.131</code> <code>9.0.0.312</code> <code>9.4.0</code> <code>9.13.1.26</code> <code>9.14.0.64</code> Website NVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks. deep learning <code>cutensor</code> <code>1.2.0</code> <code>1.5.0.3</code> <code>2.0.2.5</code> Website GPU-accelerated tensor linear algebra library. deep learning <code>py-deepsparse</code> <code>1.8.0_py39</code> Website Sparsity-aware deep learning inference runtime for CPUs. deep learning <code>py-deepspeed</code> <code>0.15.4_py312</code> Website DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. deep learning <code>py-flash-attention</code> <code>2.6.3_py312</code> <code>2.8.3_py312</code> Website Fast and memory-efficient exact attention deep learning <code>py-gym</code> <code>0.21.0_py39</code> Website Gym is a toolkit for developing and comparing reinforcement learning algorithms. deep learning <code>py-horovod</code> <code>0.12.1_py27</code> <code>0.12.1_py36</code> Website Horovod is a distributed training framework for TensorFlow. The goal of Horovod is to make distributed Deep Learning fast and easy to use. deep learning <code>py-keras</code> <code>2.1.5_py27</code> <code>2.0.8_py27</code> <code>2.1.5_py36</code> <code>2.2.4_py27</code> <code>2.2.4_py36</code> <code>2.3.1_py36</code> <code>3.7.0_py312</code> Website Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. deep learning <code>py-onnx</code> <code>1.0.1_py27</code><code>1.8.1_py36</code><code>1.12.0_py39</code><code>1.17.0_py312</code> Website ONNX is a open format to represent deep learning models. deep learning <code>py-pytorch</code> <code>0.3.0_py27</code> <code>0.2.0_py27</code> <code>0.2.0_py36</code> <code>0.3.0_py36</code> <code>1.0.0_py27</code> <code>1.0.0_py36</code> <code>1.4.0_py36</code> <code>1.6.0_py36</code> <code>1.8.1_py39</code> <code>1.11.0_py39</code> <code>2.0.0_py39</code> <code>2.2.1_py312</code> <code>2.4.1_py312</code> Website PyTorch is a deep learning framework that puts Python first. deep learning <code>py-tensorboardx</code> <code>1.8_py27</code> Website TensorboardX is TensorBoard\u2122 for PyTorch (and Chainer, MXNet, NumPy...) deep learning <code>py-tensorflow</code> <code>2.1.0_py36</code> <code>1.4.0_py27</code> <code>1.5.0_py27</code> <code>1.5.0_py36</code> <code>1.9.0_py27</code> <code>1.9.0_py36</code> <code>2.4.1_py36</code> <code>2.6.2_py36</code> <code>2.9.1_py39</code> <code>2.10.0_py39</code> <code>2.18.0_py312</code> Website TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs. deep learning <code>py-tensorlayer</code> <code>1.6.3_py27</code> Website TensorLayer is a Deep Learning (DL) and Reinforcement Learning (RL) library extended from Google TensorFlow. deep learning <code>py-tensorrt</code> <code>8.5.1.7_py39</code> <code>10.0.1_py312</code> Website Python bindings for the TensorRT library. deep learning <code>py-theano</code> <code>1.0.1_py27</code> Website Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. deep learning <code>py-torch-scatter</code> <code>2.1.2_py312</code> Website PyTorch Extension Library of Optimized Scatter Operations. deep learning <code>py-torchvision</code> <code>0.15.1_py39</code><code>0.17.1_py312</code><code>0.19.1_py312</code> Website Datasets, model architectures, and common image transformations for computer vision for PyTorch. deep learning <code>py-triton</code> <code>1.0.0_py39</code> <code>3.1.0_py312</code> Website Triton is a language and compiler for writing highly efficient custom Deep-Learning primitives. deep learning <code>py-ultralytics</code> <code>8.3.14_py312</code> Website Ultra YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. deep learning <code>py-wandb</code> <code>0.18.7_py312</code> Website WandB (Weights and Biases) is a machine learning development platform that enables data scientists and machine learning engineers to track, visualize, and reproduce their experiments in real-time. deep learning <code>tensorrt</code> <code>3.0.1</code> <code>3.0.4</code> <code>4.0.1.6</code> <code>5.0.2.6</code> <code>6.0.1.8</code> <code>7.0.0.11</code> <code>7.2.3.4</code> <code>8.5.1.7</code> <code>10.0.1.6</code> Website NVIDIA TensorRT\u2122 is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications. deep learning <code>torch</code> <code>20180202</code> Website Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. graph computing <code>bliss</code> <code>0.73</code> Website A tool for computing automorphism groups and canonical forms of graphs. lib <code>nvidia-mathdx</code> <code>25.06</code> Website Nvidia MathDX APIs are device side API extensions for performing mathematical calculations inside your CUDA kernel. lib <code>opencv</code> <code>3.3.0</code> <code>4.5.2</code> <code>4.5.5</code> <code>4.7.0</code> <code>4.9.0</code> <code>4.10.0</code> Website OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. lib <code>py-cuequivariance</code> <code>0.6.0_py312</code> Website cuEquivariance is an NVIDIA Python library designed to facilitate the construction of high-performance geometric neural networks using segmented polynomials and triangular operations. lib <code>py-pynvjitlink</code> <code>0.5.0_py312</code> Website Python bindings for the nvJitLink library lib <code>py-tensorstore</code> <code>0.1.71_py312</code><code>0.1.78_py312</code> Website Library for reading and writing large multi-dimensional arrays. lib <code>py-xgboost</code> <code>3.0.0_py312</code> Website Python bindings for xgboost lib <code>xgboost</code> <code>3.0.0</code> Website XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. linear algebra <code>armadillo</code> <code>8.200.1</code> Website Armadillo is a high quality linear algebra library (matrix maths) for the C++ language, aiming towards a good balance between speed and ease of use. linear algebra <code>cudss</code> <code>0.3.0.9</code> Website NVIDIA cuDSS is an optimized, first-generation GPU-accelerated Direct Sparse Solver library for solving linear systems with very sparse matrices. linear algebra <code>cusparselt</code> <code>0.2.0.1</code> <code>0.6.3.2</code> Website NVIDIA cuSPARSELt is a high-performance CUDA library for sparse matrix-matrix multiplication. machine learning <code>lightgbm</code> <code>4.5.0</code> Website LightGBM is a gradient boosting framework that uses tree based learning algorithms machine learning <code>onnxruntime</code> <code>1.22.0</code> Website ONNX Runtime is a cross-platform machine-learning model accelerator, with a flexible interface to integrate hardware-specific libraries. machine learning <code>py-accelerate</code> <code>0.29.3_py312</code> Website Huggingface Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration. machine learning <code>py-datasets</code> <code>2.18.0_py312</code> Website Hugging Face Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. machine learning <code>py-evaluate</code> <code>0.4.3_py312</code> Website Hugging Face Evaluate is a library for easily evaluating machine learning models and datasets. machine learning <code>py-huggingface-hub</code> <code>0.22.1_py312</code> Website The huggingface_hub library allows you to interact with the Hugging Face Hub, a machine learning platform for creators and collaborators. machine learning <code>py-kaolin</code> <code>0.15.0_py39</code><code>0.17.0_py312</code> Website A PyTorch Library for Accelerating 3D Deep Learning Research. machine learning <code>py-lightgbm</code> <code>4.5.0_py312</code> Website Python bindings for LIghtGBM machine learning <code>py-ml_dtypes</code> <code>0.3.2_py312</code><code>0.5.1_py312</code> Website A stand-alone implementation of several NumPy dtype extensions used in machine learning. machine learning <code>py-onnxruntime</code> <code>1.22.0_py312</code> Website Python bindings for ONNX Runtime machine learning <code>py-safetensors</code> <code>0.4.2_py312</code> Website Simple, safe way to store and distribute tensors. machine learning <code>py-scikit-learn</code> <code>0.19.1_py27</code> <code>0.19.1_py36</code><code>0.24.2_py36</code><code>1.0.2_py39</code><code>1.3.2_py39</code><code>1.5.1_py312</code> Website Scikit-learn is a free software machine learning library for the Python programming language. machine learning <code>py-timm</code> <code>1.0.12_py312</code> Website timm is a library containing SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts. machine learning <code>py-tinygrad</code> <code>0.8.0_py312</code> Website tinygrad is a deep learning framework that aims to provide a balance between simplicity and functionality. machine learning <code>py-tokenizers</code> <code>0.15.2_py312</code> Website Hugging Face Tokenizers provides an implementation of today\u2019s most used tokenizers, with a focus on performance and versatility.T machine learning <code>py-torch-nvidia-apex</code> <code>23.08_py312</code> Website A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch. machine learning <code>py-torchtune</code> <code>0.1.1_py312</code> Website torchtune is a PyTorch-native library for easily authoring, fine-tuning and experimenting with LLMs. machine learning <code>py-transformer-engine</code> <code>2.8_py312</code> Website Transformer Engine (TE) is a library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision. machine learning <code>py-transformers</code> <code>4.39.1_py312</code> Website Hugging Face Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. machine learning <code>py-vllm</code> <code>0.7.0_py312</code> Website vLLM is a fast and easy-to-use library for LLM inference and serving. machine learning <code>py-xgrammar</code> <code>0.1.8_py312</code> Website XGrammar is open-source solution for flexible, portable, and fast structured generations, aiming at bring flexible zero-overhead structure generation everywhere. numerical analysis <code>matlab</code> <code>R2022b</code> <code>R2017a</code> <code>R2017b</code> <code>R2018a</code> <code>R2019a</code> <code>R2020a</code> <code>R2023b</code> Website MATLAB is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. numerical analysis <code>octave</code> <code>4.2.1</code> Website GNU Octave is a high-level language primarily intended for numerical computations. numerical analysis <code>runmat</code> <code>0.0.3</code> Website RunMat is a modern, high-performance runtime for MATLAB\u00ae and GNU Octave code that eliminates license fees, vendor lock-in, and performance bottlenecks. numerical library <code>arpack</code> <code>3.5.0</code><code>3.7.0</code> <code>3.9.0</code> Website Collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. numerical library <code>blis</code> <code>2.1</code><code>2.2.4</code><code>3.1.0</code> Website BLIS is a portable software framework for instantiating high-performance BLAS-like dense linear algebra libraries. numerical library <code>fftw</code> <code>2.1.5</code><code>3.3.6</code> <code>3.3.8</code> <code>3.3.9</code><code>3.3.10</code> Website The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs). numerical library <code>flexiblas</code> <code>3.1.3</code> Website FlexiBLAS is a BLAS and LAPACK wrapper library with runtime exchangeable backends. numerical library <code>flint</code> <code>2.9.0</code> Website FLINT is a C library for doing number theory. numerical library <code>glpk</code> <code>4.63</code> Website The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. numerical library <code>gmp</code> <code>6.1.2</code><code>6.2.1</code> Website GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating-point numbers. numerical library <code>gsl</code> <code>1.16</code><code>2.3</code><code>2.7</code> Website The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. numerical library <code>harminv</code> <code>1.4.1</code> Website harminv is a program designed to solve the problem of harmonic inversion: given a time series consisting of a sum of sinusoids (modes), extract their frequencies and amplitudes. numerical library <code>hypre</code> <code>2.20.0</code> <code>2.32.0</code> Website HYPRE is a library of high performance preconditioners and solvers featuring multigrid methods for the solution of large, sparse linear systems of equations on massively parallel computers. numerical library <code>imkl</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel Math Kernel Library (Intel MKL) is a library of optimized math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.[3] The routines in MKL are hand-optimized specifically for Intel processors numerical library <code>libflame</code> <code>2.1</code><code>2.2.4</code><code>3.1.0</code> Website libflame is a portable library for dense matrix computations, providing much of the functionality present in LAPACK numerical library <code>libxsmm</code> <code>1.8.1</code><code>1.17</code> Website LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications as well as for deep learning primitives such as small convolutions numerical library <code>metis</code> <code>5.1.0</code> Website METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. numerical library <code>mpc</code> <code>1.2.1</code> Website GNU MPC is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. numerical library <code>mpfr</code> <code>3.1.5</code><code>4.1.0</code> Website The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. numerical library <code>mumps</code> <code>5.1.2</code> Website A parallel sparse direct solver. numerical library <code>openblas</code> <code>0.3.10</code> <code>0.2.19</code><code>0.3.4</code><code>0.3.9</code><code>0.3.20</code><code>0.3.26</code><code>0.3.28</code> Website OpenBLAS is an optimized BLAS library numerical library <code>parmetis</code> <code>4.0.3</code> Website ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. numerical library <code>petsc</code> <code>3.10.3</code> <code>3.18.5</code> Website PETSc, the Portable, Extensible Toolkit for Scientific Computation, is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. numerical library <code>py-autograd</code> <code>1.0_py39</code> Website Autograd can automatically differentiate native Python and Numpy code. numerical library <code>py-cupy</code> <code>7.8.0_py36</code> <code>10.2.0_py39</code> <code>12.1.0_py39</code> <code>13.3.0_py312</code> Website CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA. numerical library <code>py-gmpy2</code> <code>2.0.8_py36</code> Website gmpy2 is a C-coded Python extension module that supports multiple-precision arithmetic. numerical library <code>py-jax</code> <code>0.4.7_py39</code><code>0.4.36_py312</code> Website JAX is Autograd and XLA, brought together for high-performance numerical computing. numerical library <code>py-jaxlib</code> <code>0.4.7_py39</code><code>0.4.36_py312</code> Website XLA library for Jax. numerical library <code>py-numpy</code> <code>1.14.3_py27</code> <code>1.14.3_py36</code><code>1.17.2_py36</code><code>1.18.1_py36</code><code>1.19.2_py36</code><code>1.20.3_py39</code><code>1.24.2_py39</code><code>1.26.3_py312</code><code>2.2.6_py312</code> Website NumPy is the fundamental package for scientific computing with Python. numerical library <code>py-nvmath-python</code> <code>0.2.1_py312</code> Website NVIDIA Math Libraries for the Python Ecosystem numerical library <code>py-ott-jax</code> <code>0.5.0_py312</code> Website Optimal transport tools implemented with the JAX framework, to solve large scale matching problems of any flavor. numerical library <code>py-petsc4py</code> <code>3.18.5_py39</code> Website Python bindings for PETSc, the Portable, Extensible Toolkit for Scientific Computation. numerical library <code>py-psbody-mesh</code> <code>0.4_py39</code> Website The MPI-IS Mesh Processing Library contains core functions for manipulating meshes and visualizing them. numerical library <code>py-pyublas</code> <code>2017.1_py27</code> Website PyUblas provides a seamless glue layer between Numpy and Boost.Ublas for use with Boost.Python. numerical library <code>py-pywavelets</code> <code>1.6.0_py39</code><code>1.6.0_py312</code> Website PyWavelets is a free Open Source library for wavelet transforms in Python. numerical library <code>py-scipy</code> <code>1.1.0_py27</code> <code>1.1.0_py36</code><code>1.4.1_py36</code><code>1.6.3_py39</code><code>1.10.1_py39</code><code>1.12.0_py312</code><code>1.16.0_py312</code> Website The SciPy library provides many user-friendly and efficient numerical routines such as routines for numerical integration and optimization. numerical library <code>py-slepc4py</code> <code>3.18.2_py39</code> Website Python bindings for SLEPc. numerical library <code>py-tabmat</code> <code>3.1.2_py39</code> Website Efficient matrix representations for working with tabular data. numerical library <code>qrupdate</code> <code>1.1.2</code> Website qrupdate is a Fortran library for fast updates of QR and Cholesky decompositions. numerical library <code>scalapack</code> <code>2.0.2</code> <code>2.1</code> <code>2.2.0</code> Website ScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines. numerical library <code>scotch</code> <code>6.0.4</code> Website Software package and libraries for sequential and parallel graph partitioning, static mapping and clustering, sequential mesh and hypergraph partitioning, and sequential and parallel sparse matrix block ordering. numerical library <code>slepc</code> <code>3.18.2</code> Website SLEPc is a Scalable Library for Eigenvalue Problem Computations. numerical library <code>suitesparse</code> <code>7.4.0</code> Website SuiteSparse is a suite of sparse matrix algorithms. numerical library <code>superlu</code> <code>5.2.1</code> Website SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. numerical library <code>tetgen</code> <code>1.6.0</code> Website TetGen provides various features to generate good quality and adaptive tetrahedral meshes suitable for numerical methods, such as finite element or finite volume methods. numerical library <code>xblas</code> <code>1.0.248</code> Website Extra precise basic linear algebra subroutines. optimization <code>ceres-solver</code> <code>2.2.0</code> Website Ceres Solver is an open source C++ library for modeling and solving large, complicated optimization problems. optimization <code>gurobi</code> <code>7.5.1</code><code>8.0.1_py27</code><code>8.0.1_py36</code><code>9.0.3_py36</code><code>10.0.1_py39</code><code>11.0.2</code> Website The Gurobi Optimizer is a commercial optimization solver for mathematical programming. optimization <code>knitro</code> <code>10.3.0</code> <code>12.4.0</code> Website Artelys Knitro is an optimization solver for difficult large-scale nonlinear problems. optimization <code>nlopt</code> <code>2.6.2</code> Website NLopt is a free/open-source library for nonlinear optimization. optimization <code>octeract</code> <code>3.3.0</code> Website Octeract Engine is a proprietary massively parallel deterministic global optimization solver for general Mixed-Integer Nonlinear Programs (MINLP). optimization <code>py-optuna</code> <code>2.10.0_py39</code> Website Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. optimization <code>sundials</code> <code>6.4.1</code> Website SUNDIALS is a family of software packages providing robust and efficient time integrators and nonlinear solvers that can easily be incorporated into existing simulation codes. scientific computing <code>py-scipystack</code> <code>1.0_py27</code> <code>1.0_py36</code> Website The SciPy Stack is a collection of open source software for scientific computing in Python. It provides the following packages: numpy, scipy, matplotlib, ipython, jupyter, pandas, sympy and nose. speech analysis <code>praat</code> <code>6.4.26</code> Website Praat is a speech analysis tool used for doing phonetics by computer. statistics <code>datamash</code> <code>1.3</code> Website GNU datamash is a command-line program which performs basic numeric, textual and statistical operations on input textual data files. statistics <code>jags</code> <code>4.3.0</code><code>4.3.1</code> Website Just another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC). statistics <code>py-emcee</code> <code>3.1.4_py39</code> Website The Python ensemble sampling toolkit for affine-invariant MCMC statistics <code>py-glum</code> <code>2.1.2_py39</code> Website glum is a fast, modern, Python-first GLM estimation library. statistics <code>py-rpy2</code> <code>2.8.6_py27</code><code>2.9.2_py36</code> Website rpy2 is an interface to R running embedded in a Python process. statistics <code>R</code> <code>4.2.0</code> <code>3.4.0</code><code>3.5.1</code><code>3.6.1</code><code>4.0.2</code><code>4.1.2</code><code>4.3.2</code><code>4.4.2</code> Website R is a free software environment for statistical computing and graphics. statistics <code>rstudio</code> <code>1.3.1093</code> <code>2023.09.1</code> Website RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. statistics <code>rstudio-desktop</code> <code>2022.02.2-485</code> Website RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. This is the X11/GUI version. statistics <code>sas</code> <code>9.4</code> Website SAS is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics. statistics <code>stata</code> <code>15</code> <code>14</code> <code>16</code> <code>17</code> <code>18</code> <code>19</code> Website Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. symbolic <code>libmatheval</code> <code>1.1.11</code> Website GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. symbolic <code>maxima</code> <code>5.47.0</code> Website Maxima is a system for the manipulation of symbolic and numerical expressions. symbolic <code>py-pysr</code> <code>0.12.3_py39</code> Website High-Performance Symbolic Regression in Python and Julia. symbolic <code>py-sympy</code> <code>1.1.1_py27</code><code>1.1.1_py36</code><code>1.11.1_py39</code><code>1.14.0_py312</code> Website SymPy is a Python library for symbolic mathematics. technical computing <code>mathematica</code> <code>13.1.0</code> Website A symbolic language and platform for modern technical computing. topic modelling <code>py-gensim</code> <code>4.2.0_py39</code> Website Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora."},{"location":"docs/software/list/#physics","title":"physics","text":"Field Module\u00a0name Version(s) URL Description astronomy <code>cfitsio</code> <code>4.0.0</code> Website FITSIO is a library of C and Fortran subroutines for reading and writing data files in FITS (Flexible Image Transport System) data format. astronomy <code>heasoft</code> <code>6.22.1</code><code>6.26.1</code> Website HEAsoft is a Unified Release of the FTOOLS (General and mission-specific tools to manipulate FITS files) and XANADU (High-level, multi-mission tasks for X-ray astronomical spectral, timing, and imaging data analysis) software packages. astronomy <code>py-astropy</code> <code>4.0.1_py36</code> Website The Astropy Project is a community effort to develop a common core package for Astronomy in Python and foster an ecosystem of interoperable astronomy packages. astronomy <code>py-lenstools</code> <code>1.0_py36</code> Website This python package collects together a suite of widely used analysis tools in Weak Gravitational Lensing. astronomy <code>py-namaster</code> <code>1.2.2_py36</code> Website NaMaster is a C library, Python module and standalone program to compute full-sky angular cross-power spectra of masked fields with arbitrary spin and an arbitrary number of known contaminants using a pseudo-Cl (aka MASTER) approach. CFD <code>amr-wind</code> <code>3.1.5</code> <code>3.2.0</code> Website AMR-Wind is a massively parallel, block-structured adaptive-mesh, incompressible flow solver for wind turbine and wind farm simulations. CFD <code>openfast</code> <code>3.5.4</code> Website OpenFAST is a multi-physics, multi-fidelity tool for simulating the coupled dynamic response of wind turbines. CFD <code>su2</code> <code>7.0.3</code> Website SU2: An Open-Source Suite for Multiphysics Simulation and Design climate modeling <code>cdo</code> <code>1.9.7.1</code><code>2.1.1</code> Website CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data. climate modeling <code>fre-nctools</code> <code>2022.01</code> Website FRE-NCtools is a collection of tools to help with the creation and manipulation of netCDF files used for climate modeling. fuild dynamics <code>dualsphysics</code> <code>5.4.0</code> Website DualSPHysics is based on the Smoothed Particle Hydrodynamics model named SPHysics, and is developed to study free-surface flow phenomena. geophysics <code>opensees</code> <code>2.5.0</code> Website OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes. geoscience <code>gdal</code> <code>3.4.1</code> <code>2.2.1</code><code>3.5.2</code><code>3.10.2</code> Website GDAL is a translator library for raster and vector geospatial data formats. geoscience <code>geos</code> <code>3.6.2</code> <code>3.11.0</code><code>3.12.1</code><code>3.13.1</code> Website GEOS (Geometry Engine - Open Source) is a C++ port of Java Topology Suite (JTS). geoscience <code>geosx</code> <code>0.2.0-20220523</code> Website GEOSX is a simulation framework for modeling coupled flow, transport, and geomechanics in the subsurface. geoscience <code>gmtsar</code> <code>6.2.2</code> Website An InSAR processing system based on GMT (Generic Mapping Tools). geoscience <code>postgis</code> <code>3.5.3</code> Website PostGIS extends the capabilities of the PostgreSQL relational database by adding support for storing, indexing, and querying geospatial data. geoscience <code>proj</code> <code>8.2.1</code> <code>4.9.3</code><code>9.1.0</code><code>9.5.1</code> Website PROJ is a generic coordinate transformation software that transforms geospatial coordinates from one coordinate reference system (CRS) to another. geoscience <code>py-gdal-utils</code> <code>3.4.1_py39</code> Website gdal-utils is the GDAL Python Utilities distribution. geoscience <code>py-opendrift</code> <code>1.0.3_py27</code> Website OpenDrift is a software for modeling the trajectories and fate of objects or substances drifting in the ocean, or even in the atmosphere. geoscience <code>py-pyproj</code> <code>1.9.5.1_py27</code> <code>1.9.5.1_py36</code><code>3.4.0_py39</code><code>3.7.2_py312</code> Website Python interface to PROJ4 library for cartographic transformations. geoscience <code>swash</code> <code>9.01a</code> Website SWASH (an acronym of Simulating WAves till SHore) is a non-hydrostatic wave-flow model. geoscience <code>udunits</code> <code>2.2.26</code> Website The UDUNITS package from Unidata is a C-based package for the programatic handling of units of physical quantities. lib <code>libgdsii</code> <code>0.21</code> Website libGDSII C++ is a library and command-line utility for reading GDSII geometry files. magnetism <code>mumax</code> <code>3.10</code> Website mumax3 is a GPU-accelerated micromagnetic simulation program. materials science <code>atat</code> <code>3.36</code> Website Alloy Theoretic Automated Toolkit: a software toolkit for modeling coupled configurational and vibrational disorder in alloy systems. materials science <code>py-megnet</code> <code>1.3.0_py39</code> Website The MatErials Graph Network (MEGNet) is an implementation of DeepMind's graph networks[1] for universal machine learning in materials science. materials science <code>py-pymatgen</code> <code>2022.5.26_py39</code> Website Pymatgen (Python Materials Genomics) is a robust, open-source Python library for materials analysis. micromagnetics <code>oommf</code> <code>1.2b4</code> Website OOMMF is a set of portable, extensible public domain micromagnetic program and associated tools. particle <code>openmc</code> <code>0.10.0</code> Website OpenMC is a Monte Carlo particle transport simulation code focused on neutron criticality calculations. photonics <code>meep</code> <code>1.3</code> <code>1.4.3</code> <code>1.24.0</code> Website Meep is a free finite-difference time-domain (FDTD) simulation software package to model electromagnetic systems. photonics <code>mpb</code> <code>1.5</code> <code>1.6.2</code> <code>1.11.1</code> Website MPB is a free software package for computing the band structures, or dispersion relations, and electromagnetic modes of periodic dielectric structures, on both serial and parallel computers. physics-ml <code>py-nvidia-modulus</code> <code>0.9.0_py312</code> Website NVIDIA Modulus is an open-source framework for building, training, and fine-tuning Physics-ML models with a simple Python interface. quantum information science <code>cuquantum</code> <code>22.03.0.40</code> Website NVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows. quantum information science <code>py-cuquantum-python</code> <code>22.3.0_py39</code> Website NVIDIA cuQuantum Python provides Python bindings and high-level object-oriented models for accessing the full functionalities of NVIDIA cuQuantum SDK from Python. quantum mechanics <code>py-quspin</code> <code>0.3.5_py36</code> Website QuSpin is an open-source Python package for exact diagonalization and quantum dynamics of arbitrary boson, fermion and spin many-body systems. quantum mechanics <code>py-qutip</code> <code>4.5.2_py36</code> Website QuTiP is open-source software for simulating the dynamics of closed and open quantum systems."},{"location":"docs/software/list/#system","title":"system","text":"Field Module\u00a0name Version(s) URL Description assembler <code>nasm</code> <code>2.16.03</code> Website A cross-platform x86 assembler with an Intel-like syntax. backup <code>restic</code> <code>0.9.5</code><code>0.12.1</code><code>0.16.3</code><code>0.17.0</code><code>0.18.0</code> Website Fast, secure, efficient backup program. benchmark <code>elbencho</code> <code>3.0.25</code> Website A distributed storage benchmark for file systems, object stores and block devices with support for GPUs benchmark <code>hp2p</code> <code>3.2</code> Website Heavy Peer To Peer: a MPI based benchmark for network diagnostic. benchmark <code>mpibench</code> <code>20190729</code> Website Times MPI collectives over a series of message sizes. benchmark <code>mprime</code> <code>29.4</code> Website mprime is used by GIMPS, a distributed computing project dedicated to finding new Mersenne prime numbers, and which is commonly used as a stability testing utility. benchmark <code>osu-micro-benchmarks</code> <code>5.6.1</code> <code>5.7</code> <code>5.9</code> Website The OSU MicroBenchmarks carry out a variety of message passing performance tests using MPI. benchmark <code>py-linktest</code> <code>2.1.19_py39</code> Website LinkTest is a communication API benchmarking tool that tests point-to-point connections. checkpointing <code>dmtcp</code> <code>2.6.0</code> Website DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space -- with no modifications to user code or to the O/S. CI/CD <code>gitlab-runner</code> <code>18.0.2</code> Website GitLab Runner is the open source project that is used to run your CI/CD jobs and send the results back to GitLab. cloud interface <code>aws-cli</code> <code>2.27.49</code> Website This package provides a unified command line interface to Amazon Web Services. cloud interface <code>google-cloud-sdk</code> <code>400.0.0</code><code>448.0.0</code> Website Command-line interface for Google Cloud Platform products and services. cloud interface <code>s5cmd</code> <code>2.0.0</code> Website Parallel S3 and local filesystem execution tool. cloud interface <code>steampipe</code> <code>0.14.6</code> Website Steampipe is an open source tool for querying cloud APIs in a universal way and reasoning about the data in SQL. compiler <code>mrc</code> <code>1.3.3</code> Website MRC is a resource compiler that can create self-contained applications, by including all the required data inside executable files. compression <code>c-blosc</code> <code>1.21.6</code> Website A blocking, shuffling and loss-less compression library that can be faster than memcpy(). compression <code>c-blosc2</code> <code>2.15.2</code> Website A fast, compressed, persistent binary data store library for C. compression <code>libarchive</code> <code>3.3.2</code><code>3.4.2</code><code>3.5.2</code><code>3.7.9</code> Website The libarchive project develops a portable, efficient C library that can read and write streaming archives in a variety of formats. compression <code>libtar</code> <code>1.2.20</code> Website C library for manipulating tar files. compression <code>libzip</code> <code>1.5.1</code> Website libzip is a C library for reading, creating, and modifying zip archives. compression <code>lz4</code> <code>1.8.0</code> Website LZ4 is lossless compression algorithm. compression <code>lzo</code> <code>2.10</code> Website LZO is a portable lossless data compression library written in ANSI C. compression <code>mpibzip2</code> <code>0.6</code> Website MPIBZIP2 is a parallel implementation of the bzip2 block-sorting file compressor that uses MPI and achieves significant speedup on cluster machines. compression <code>openzl</code> <code>0.1.0</code> Website OpenZL delivers high compression ratios while preserving high speed, a level of performance that is out of reach for generic compressors. compression <code>p7zip</code> <code>16.02</code> Website p7zip is a Linux port of 7zip, a file archiver with high compression ratio. compression <code>pbzip2</code> <code>1.1.12</code> Website PBZIP2 is a parallel implementation of the bzip2 block-sorting file compressor that uses pthreads and achieves near-linear speedup on SMP machines. compression <code>pigz</code> <code>2.4</code> Website A parallel implementation of gzip for modern multi-processor, multi-core machines. compression <code>szip</code> <code>2.1.1</code> Website Szip compression software, providing lossless compression of scientific data, is an implementation of the extended-Rice lossless compression algorithm. compression <code>tarsplitter</code> <code>1.0.1</code> Website Split tar files into parts on file boundaries. compression <code>xz</code> <code>5.2.3</code> Website XZ Utils, the successor to LZMA Utils, is free general-purpose data compression software with a high compression ratio. compression <code>zlib</code> <code>1.2.11</code> <code>1.3.1</code> Website zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. compression <code>zstd</code> <code>1.5.2</code> Website Zstandard, or zstd, is a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios. containers <code>libnvidia-container</code> <code>1.0.0rc2</code> Website libnvidia-container is a library and a simple CLI utility to automatically configure GNU/Linux containers leveraging NVIDIA hardware. containers <code>proot</code> <code>5.2.0</code> <code>5.1.0</code> Website PRoot is a user-space implementation of chroot, mount --bind, and binfmt_misc. containers <code>py-spython</code> <code>0.3.13_py39</code><code>0.3.13_py312</code> Website Singularity Python (spython) is the Python API for working with Singularity containers. database <code>bdb</code> <code>6.2.32</code> Website Berkeley DB (BDB) is a software library intended to provide a high-performance embedded database for key/value data. database <code>duckdb</code> <code>1.1.3</code> Website DuckDB is an analytical in-process SQL database management system. database <code>mariadb</code> <code>10.2.11</code> <code>10.6.9</code> Website MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. database <code>postgresql</code> <code>10.5</code><code>14.5</code> Website PostgreSQL is a powerful, open source object-relational database system with a strong focus on reliability, feature robustness, and performance. database <code>py-duckdb</code> <code>1.1.3_py312</code> Website Python bindings for DuckDB database <code>sqlite</code> <code>3.18.0</code><code>3.37.2</code><code>3.44.2</code> Website SQLite is a self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine. database <code>sqliteodbc</code> <code>0.9998</code> Website ODBC driver for SQLite database <code>unixodbc</code> <code>2.3.9</code> Website unixODBC is an open-source project that implements the ODBC API. document management <code>pandoc</code> <code>2.7.3</code> Website Pandoc is a universal document converter. document processing <code>ghostscript</code> <code>9.53.2</code> Website Ghostscript is an interpreter for the PostScript language and PDF files. document processing <code>groff</code> <code>1.23.0</code> Website groff (GNU roff) is a typesetting system that reads plain text input files that include formatting commands to produce output in PostScript, PDF, HTML, or DVI formats or for display to a terminal. document processing <code>lyx</code> <code>2.3.2</code> Website LyX is a document processor. document processing <code>poppler</code> <code>0.47.0</code> Website Poppler is a PDF rendering library. document processing <code>texinfo</code> <code>6.6</code> Website Texinfo is the official documentation format of the GNU project. document processing <code>texlive</code> <code>2019</code> Website TeX Live is an easy way to get up and running with the TeX document production system. file management <code>dua-cli</code> <code>2.20.1</code> Website dua (-&gt; Disk Usage Analyzer) is a tool to conveniently learn about the usage of disk space of a given directory. file management <code>duc</code> <code>1.4.4</code> Website Duc is a collection of tools for indexing, inspecting and visualizing disk usage. file management <code>exa</code> <code>0.8.0</code> Website exa is a replacement for ls written in Rust. file management <code>fdupes</code> <code>2.2.1</code> Website FDUPES is a program for identifying or deleting duplicate files residing within specified directories. file management <code>fpart</code> <code>0.9.3</code> Website fpart sorts files and packs them into partitions. file management <code>jdupes</code> <code>1.28.0</code> Website jdupes is a program for identifying and taking actions upon duplicate files. file management <code>midnight-commander</code> <code>4.8.29</code> Website GNU Midnight Commander is a visual file manager. file management <code>ncdu</code> <code>1.15.1</code><code>1.18.1</code><code>2.2.1</code><code>2.8.1</code><code>2.9.1</code> Website Ncdu is a disk usage analyzer with an ncurses interface. file management <code>py-pcircle</code> <code>0.17_py27</code> Website pcircle contains a suite of file system tools developed at OLCF to take advantage of highly scalable parallel file system such as Lustre. file management <code>rmlint</code> <code>2.8.0</code> Website rmlint finds space waste and other broken things on your filesystem and offers to remove it. file management <code>tdu</code> <code>1.36</code> Website tdu estimates the disk space occupied by all files in a given path. file transfer <code>aria2</code> <code>1.35.0</code> Website aria2 is a lightweight multi-protocol &amp; multi-source command-line download utility. file transfer <code>aspera-cli</code> <code>3.9.6</code> <code>4.24.1</code> Website The IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. file transfer <code>lftp</code> <code>4.8.1</code> Website LFTP is a sophisticated file transfer program supporting a number of network protocols (ftp, http, sftp, fish, torrent). file transfer <code>minio-client</code> <code>20250715</code> Website MinIO Client (mc) supports filesystems and Amazon S3 compatible cloud storage services. file transfer <code>mpifileutils</code> <code>0.10.1</code> <code>0.11</code> <code>0.11.1</code> <code>0.12</code> Website mpiFileUtils is a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files. file transfer <code>py-globus-cli</code> <code>3.19.0_py39</code><code>3.33.1_py312</code><code>3.35.2_py39</code><code>3.35.2_py312</code><code>3.38.0_py312</code> Website A command line wrapper over the Globus SDK for Python. file transfer <code>py-httpie</code> <code>3.2.1_py39</code> Website HTTPie is a command-line HTTP client designed for testing, debugging, and generally interacting with APIs and HTTP servers. file transfer <code>rclone</code> <code>1.55.1</code><code>1.59.1</code><code>1.65.0</code> Website Rclone is a command line program to sync files and directories to and from: Google Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft One Drive, Hubic, Backblaze B2, Yandex Disk, or the local filesystem. framework <code>mono</code> <code>5.12.0.301</code><code>5.20.1.19</code> Website Mono is an open source implementation of Microsoft's .NET Framework based on the ECMA standards for C# and the Common Language Runtime. graphics <code>lcms2</code> <code>2.16</code> Website Little CMS is an open-source, small-footprint color management engine, with special focus on accuracy and performance. hardware <code>hwloc</code> <code>2.7.0</code><code>2.9.3</code> Website The Portable Hardware Locality (hwloc) software package provides a portable abstraction of the hierarchical topology of modern architectures. hardware <code>libpciaccess</code> <code>0.16</code> Website Generic PCI access library. hashing <code>b3sum</code> <code>1.8.2</code> Website b3sum is a command-line implementation of the BLAKE3 hash function. job management <code>slurm-drmaa</code> <code>1.1.2</code> Website DRMAA for Slurm Workload Manager (Slurm) is an implementation of Open Grid Forum Distributed Resource Management Application API (DRMAA) version 1 for submission and control of jobs to Slurm. language <code>tcltk</code> <code>8.6.6</code> Website Tcl (Tool Command Language) is a dynamic programming language, suitable for web and desktop applications, networking, administration, testing. Tk is a graphical user interface toolkit. libs <code>apr</code> <code>1.6.3</code> Website The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system. libs <code>apr-util</code> <code>1.6.1</code> Website The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system. libs <code>atk</code> <code>2.24.0</code> Website ATK is the Accessibility Toolkit. It provides a set of generic interfaces allowing accessibility technologies such as screen readers to interact with a graphical user interface. libs <code>benchmark</code> <code>1.2.0</code> Website A microbenchmark support library libs <code>cairo</code> <code>1.14.10</code> Website Cairo is a 2D graphics library with support for multiple output devices. libs <code>cups</code> <code>2.2.4</code> Website CUPS is the standards-based, open source printing system. libs <code>dbus</code> <code>1.10.22</code> Website D-Bus is a message bus system, a simple way for applications to talk to one another. libs <code>enchant</code> <code>1.6.1</code><code>2.2.3</code> Website Enchant is a library (and command-line program) that wraps a number of different spelling libraries and programs with a consistent interface. libs <code>fltk</code> <code>1.3.4</code> Website FLTK (pronounced 'fulltick') is a cross-platform C++ GUI toolkit. libs <code>fontconfig</code> <code>2.12.4</code> Website Fontconfig is a library for configuring and customizing font access. libs <code>freeglut</code> <code>3.0.0</code> Website FreeGLUT is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library. libs <code>freetype</code> <code>2.8.1</code><code>2.9.1</code> Website FreeType is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). libs <code>fribidi</code> <code>1.0.12</code> Website The Free Implementation of the Unicode Bidirectional Algorithm. libs <code>ftgl</code> <code>2.1.2</code> Website FTGL is a free cross-platform Open Source C++ library that uses Freetype2 to simplify rendering fonts in OpenGL applications. libs <code>gc</code> <code>7.6.0</code> Website The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. libs <code>gconf</code> <code>2.9.91</code> Website GConf is a system for storing application preferences. libs <code>gdk-pixbuf</code> <code>2.36.8</code> Website The GdkPixbuf library provides facilities for loading images in a variety of file formats. libs <code>gflags</code> <code>2.2.1</code><code>2.2.2</code> Website The gflags package contains a C++ library that implements commandline flags processing. libs <code>giflib</code> <code>5.1.4</code> Website GIFLIB is a package of portable tools and library routines for working with GIF images. libs <code>glib</code> <code>2.52.3</code> Website The GLib library provides core non-graphical functionality such as high level data types, Unicode manipulation, and an object and type system to C programs. libs <code>glog</code> <code>0.3.5</code> Website C++ implementation of the Google logging module. libs <code>gnutls</code> <code>3.5.9</code> Website GnuTLS is a secure communications library implementing the SSL, TLS and DTLS protocols and technologies around them. libs <code>gobject-introspection</code> <code>1.52.1</code> Website GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. libs <code>googletest</code> <code>1.8.0</code> Website Google Test is Google's C++ test framework. libs <code>gstreamer</code> <code>1.12.0</code> Website GStreamer is a library for constructing graphs of media-handling components. libs <code>gtk+</code> <code>2.24.30</code><code>3.22.18</code> Website GTK+, or the GIMP Toolkit, is a multi-platform toolkit for creating graphical user interfaces. libs <code>harfbuzz</code> <code>1.4.8</code> Website HarfBuzz is an OpenType text shaping engine. libs <code>hunspell</code> <code>1.6.2</code> Website Hunspell is a spell checker. libs <code>hyphen</code> <code>2.8.8</code> Website Hyphen is a hyphenation library to use converted TeX hyphenation patterns. libs <code>icu</code> <code>59.1</code> Website ICU is a set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. libs <code>jansson</code> <code>2.13.1</code> Website C library for encoding, decoding and manipulating JSON data. libs <code>jemalloc</code> <code>5.3.0</code> Website jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. libs <code>json-glib</code> <code>1.4.4</code> Website JSON-GLib is a library providing serialization and deserialization support for the JavaScript Object Notation (JSON) format described by RFC 4627. libs <code>leptonica</code> <code>1.82.0</code> Website Leptonica is an open source library containing software that is broadly useful for image processing and image analysis applications. libs <code>libaio</code> <code>0.3.111</code> Website libaio provides the Linux-native API for async I/O. libs <code>libart_lgpl</code> <code>2.3.21</code> Website Libart is a library for high-performance 2D graphics. libs <code>libcroco</code> <code>0.6.13</code> Website Libcroco is a standalone css2 parsing and manipulation library. libs <code>libepoxy</code> <code>1.4.1</code> Website Epoxy is a library for handling OpenGL function pointer management for you. libs <code>libexif</code> <code>0.6.21</code> Website A library for parsing, editing, and saving EXIF data. libs <code>libffi</code> <code>3.2.1</code> Website libffi is a portable Foreign Function Interface library. libs <code>libgcrypt</code> <code>1.8.2</code> Website Libgcrypt is a general purpose cryptographic library originally based on code from GnuPG. libs <code>libgd</code> <code>2.2.5</code> Website GD is an open source code library for the dynamic creation of images by programmers. libs <code>libgdiplus</code> <code>5.6</code> Website C-based implementation of the GDI+ API libs <code>libglvnd</code> <code>1.2.0</code> Website libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libs <code>libgnomecanvas</code> <code>2.30.3</code> Website Library for the GNOME canvas, an engine for structured graphics that offers a rich imaging model, high performance rendering, and a powerful, high-level API. libs <code>libgpg-error</code> <code>1.27</code> Website Libgpg-error is a small library that originally defined common error values for all GnuPG components. libs <code>libiconv</code> <code>1.16</code> Website libiconv is a conversion library for string encoding. libs <code>libidl</code> <code>0.8.14</code> Website The libIDL package contains libraries for Interface Definition Language files. This is a specification for defining portable interfaces. libs <code>libidn</code> <code>2.3.7</code> Website GNU Libidn is a fully documented implementation of the Stringprep, Punycode and IDNA 2003 specifications. libs <code>libjpeg-turbo</code> <code>1.5.1</code> <code>2.1.4</code> Website libjpeg-turbo is a JPEG image codec that uses SIMD instructions (MMX, SSE2, AVX2, NEON, AltiVec) to accelerate baseline JPEG compression and decompression on x86, x86-64, ARM, and PowerPC systems libs <code>libmetalink</code> <code>0.1.3</code> Website Libmetalink is a library to read Metalink XML download description format. libs <code>libmng</code> <code>2.0.3</code> Website THE reference library for reading, displaying, writing and examining Multiple-Image Network Graphics. MNG is the animation extension to the popular PNG image-format. libs <code>libpng</code> <code>1.2.57</code><code>1.6.29</code><code>1.6.44</code> Website libpng is the official PNG reference library. It supports almost all PNG features, is extensible, and has been extensively tested for over 20 years. libs <code>libproxy</code> <code>0.4.15</code> Website libproxy is a library that provides automatic proxy configuration management. libs <code>libressl</code> <code>2.5.3</code><code>3.2.1</code> Website LibreSSL is a version of the TLS/crypto stack forked from OpenSSL in 2014, with goals of modernizing the codebase, improving security, and applying best practice development processes. libs <code>librsvg</code> <code>2.36.4</code> Website Librsvg is a library to render SVG files using cairo as a rendering engine. libs <code>libseccomp</code> <code>2.3.3</code> Website The libseccomp library provides an easy to use, platform independent, interface to the Linux Kernel's syscall filtering mechanism.. libs <code>libsodium</code> <code>1.0.18</code> Website Sodium is a modern, easy-to-use software library for encryption, decryption, signatures, password hashing and more. libs <code>libsoup</code> <code>2.61.2</code> Website libsoup is an HTTP client/server library for GNOME. libs <code>libtasn1</code> <code>4.13</code> Website Libtasn1 is the ASN.1 library used by GnuTLS, p11-kit and some other packages. libs <code>libtiff</code> <code>4.0.8</code> <code>4.4.0</code><code>4.5.0</code> Website libtiff provides support for the Tag Image File Format (TIFF), a widely used format for storing image data. libs <code>libtool</code> <code>2.4.7</code> Website GNU Libtool is a generic library support script that hides the complexity of using shared libraries behind a consistent, portable interface. libs <code>libunistring</code> <code>0.9.7</code> Website Libunistring provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libs <code>libuuid</code> <code>1.0.3</code> Website Portable uuid C library. libs <code>libuv</code> <code>1.38.1</code> Website libuv is a multi-platform support library with a focus on asynchronous I/O. libs <code>libwebp</code> <code>0.6.1</code><code>1.3.0</code> Website WebP is a modern image format that provides superior lossless and lossy compression for images on the web. libs <code>libxkbcommon</code> <code>0.9.1</code> Website libxkbcommon is a keyboard keymap compiler and support library which processes a reduced subset of keymaps as defined by the XKB (X Keyboard Extension) specification. libs <code>libxml2</code> <code>2.9.4</code> Website Libxml2 is a XML C parser and toolkit. libs <code>libxslt</code> <code>1.1.32</code> Website Libxslt is the XSLT C library developed for the GNOME project. XSLT itself is a an XML language to define transformation for XML. libs <code>mesa</code> <code>17.1.6</code> Website Mesa is an open-source implementation of the OpenGL, Vulkan and other specifications. libs <code>minipmi</code> <code>1.0</code> Website Implementation of a minimal subset of the PMI1 and PMI2 specifications. libs <code>ncurses</code> <code>6.0</code><code>6.4</code> Website The ncurses (new curses) library is a free software emulation of curses in System V Release 4.0 (SVr4), and more. libs <code>nettle</code> <code>3.3</code> Website Nettle is a cryptographic library that is designed to fit easily in more or less any context. libs <code>openjpeg</code> <code>2.3.1</code><code>2.5.3</code> Website OpenJPEG is an open-source JPEG 2000 codec written in C language. libs <code>openssl</code> <code>3.0.7</code> Website OpenSSL is a full-featured toolkit for general-purpose cryptography and secure communication. libs <code>orbit</code> <code>2.14.19</code> Website ORBit2 is a CORBA 2.4-compliant Object Request Broker (ORB) featuring mature C, C++ and Python bindings. libs <code>pango</code> <code>1.40.10</code> Website Pango is a library for laying out and rendering of text, with an emphasis on internationalization. libs <code>pcre</code> <code>8.40</code> Website The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. libs <code>pcre2</code> <code>10.35</code><code>10.40</code> Website The PCRE22 library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. libs <code>popt</code> <code>1.16</code> Website Library for parsing command line options. libs <code>py-lmdb</code> <code>0.93</code> Website Universal Python binding for the LMDB 'Lightning' Database. libs <code>py-mako</code> <code>1.0.7_py27</code> <code>1.0.7_py36</code> Website Mako is a template library written in Python. It provides a familiar, non-XML syntax which compiles into Python modules for maximum performance. libs <code>py-pygobject</code> <code>3.32.2_py36</code> Website PyGObject is a Python package which provides bindings for GObject based libraries such as GTK, GStreamer, WebKitGTK, GLib, GIO and many more. libs <code>py-pyopengl</code> <code>3.1.5_py39</code> Website Standard OpenGL bindings for Python. libs <code>py-pyqt5</code> <code>5.9.1_py36</code> Website PyQt5 is a comprehensive set of Python bindings for Qt v5. libs <code>readline</code> <code>7.0</code><code>8.2</code> Website The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. libs <code>serf</code> <code>1.3.9</code> Website The serf library is a high performance C-based HTTP client library built upon the Apache Portable Runtime (APR) library. libs <code>sionlib</code> <code>1.7.7</code> Website Scalable I/O library for parallel access to task-local files. libs <code>snappy</code> <code>1.1.7</code> Website A fast compressor/decompressor. libs <code>talloc</code> <code>2.1.14</code> Website talloc is a hierarchical, reference counted memory pool system with destructors. libs <code>tesseract</code> <code>5.1.0</code> Website Tesseract is an open source text recognition (OCR) Engine. libs <code>utf8proc</code> <code>2.4.0</code> Website iutf8proc is a small, clean C library that provides Unicode normalization, case-folding, and other operations for data in the UTF-8 encoding. libs <code>webkitgtk</code> <code>2.28.4</code> Website WebKitGTK is a full-featured port of the WebKit rendering engine, suitable for projects requiring any kind of web integration, from hybrid HTML/CSS applications to full-fledged web browsers. libs <code>wxwidgets</code> <code>3.0.4</code> Website wxWidgets is a C++ library that lets developers create applications for Windows, macOS, Linux and other platforms with a single code base. libs <code>yaml-cpp</code> <code>0.7.0</code> Website yaml-cpp is a YAML parser and emitter in C++ matching the YAML 1.2 spec. media <code>ffmpeg</code> <code>4.0</code><code>4.2.1</code><code>5.0</code><code>7.1</code> Website FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created. media <code>flac</code> <code>1.4.3</code> Website FLAC is the Free Lossless Audio Codec media <code>lame</code> <code>3.100</code> Website LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. media <code>libogg</code> <code>1.3.5</code> Website OGG media container. media <code>libsndfile</code> <code>1.0.28</code><code>1.2.2</code> Website Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. media <code>libvorbis</code> <code>1.3.7</code> Website Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format. media <code>mpg123</code> <code>1.33.0</code> Website The mpg123 distribution contains a real time MPEG 1.0/2.0/2.5 audio player/decoder for layers 1,2 and 3 (most commonly MPEG 1.0 layer 3 aka MP3), as well as re-usable decoding and output libraries. media <code>opus</code> <code>1.5.2</code> Website Opus is a totally open, royalty-free, highly versatile audio codec. media <code>portaudio</code> <code>19.7.0</code> Website PortAudio is a free, cross-platform, open-source, audio I/O library. media <code>sox</code> <code>14.4.2</code> Website SoX, Swiss Army knife of sound processing. media <code>wavpack</code> <code>5.7.0</code> Website WavPack is a completely open audio compression format providing lossless, high-quality lossy, and a unique hybrid compression mode. performance <code>likwid</code> <code>4.3.2</code><code>5.2.1</code> Website Likwid is a simple toolsuite of command line applications for performance oriented programmers. resource monitoring <code>btop</code> <code>1.4.4</code> Website Resource monitor that shows usage and stats for processor, memory, disks, network and processes. resource monitoring <code>nvtop</code> <code>1.1.0</code> <code>2.0.3</code> <code>3.0.2</code> Website Nvtop stands for NVidia TOP, a (h)top like task monitor for NVIDIA GPUs. resource monitoring <code>py-nvitop</code> <code>1.3.2_py39</code> <code>1.3.2_py312</code> Website An interactive NVIDIA-GPU process viewer and beyond. resource monitoring <code>remora</code> <code>1.8.5</code> Website Remora is a tool to monitor runtime resource utilization. resource monitoring <code>ruse</code> <code>2.0</code> Website A command line tool to measure process resource usage. scm <code>git</code> <code>2.45.1</code> Website Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. scm <code>git-annex</code> <code>8.20210622</code> Website git-annex allows managing files with git, without checking the file contents into git. scm <code>git-credential-manager</code> <code>2.0.696</code> Website Secure, cross-platform Git credential storage with authentication to GitHub, Azure Repos, and other popular Git hosting services. scm <code>git-lfs</code> <code>2.4.0</code> Website Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server. scm <code>libgit2</code> <code>1.1.0</code><code>1.9.1</code> Website libgit2 is a portable, pure C implementation of the Git core methods provided as a re-entrant linkable library with a solid API scm <code>mercurial</code> <code>4.5.3</code> Website Mercurial is a free, distributed source control management tool. scm <code>py-dvc</code> <code>0.91.1_py36</code> Website Data Version Control or DVC is an open-source tool for data science and machine learning projects. scm <code>subversion</code> <code>1.9.7</code><code>1.12.2</code> Website Subversion is an open source version control system. shell <code>powershell</code> <code>7.1.5</code> Website PowerShell Core is a cross-platform automation and configuration tool/framework. testing <code>py-pytest</code> <code>7.1.3_py39</code> Website pytest is a full-featured Python testing framework tools <code>clinfo</code> <code>2.2.18.04.06</code> Website clinfo is a simple command-line application that enumerates all possible (known) properties of the OpenCL platform and devices available on the system. tools <code>curl</code> <code>8.4.0</code> Website curl is an open source command line tool and library for transferring data with URL syntax. tools <code>depot_tools</code> <code>20200731</code> Website Tools for working with Chromium development. tools <code>expat</code> <code>2.2.3</code> Website Expat is a stream-oriented XML parser library written in C. tools <code>graphicsmagick</code> <code>1.3.26</code> Website GraphicsMagick is the swiss army knife of image processing. tools <code>imagemagick</code> <code>7.0.7-2</code><code>7.1.1-43</code> Website ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files. tools <code>jq</code> <code>1.6</code> Website jq is a lightweight and flexible command-line JSON processor. tools <code>leveldb</code> <code>1.20</code> Website LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. tools <code>libxcb</code> <code>1.17.0</code> Website The X protocol C-language Binding (XCB) is a replacement for Xlib featuring a small footprint, latency hiding, direct access to the protocol, improved threading support, and extensibility. tools <code>lmdb</code> <code>0.9.21</code> Website Symas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project. tools <code>motif</code> <code>2.3.7</code> Website Motif is the toolkit for the Common Desktop Environment. tools <code>munge</code> <code>0.5.16</code> Website MUNGE (MUNGE Uid 'N' Gid Emporium) is an authentication service for creating and validating user credentials. tools <code>parallel</code> <code>20180122</code><code>20200822</code> Website GNU parallel is a shell tool for executing jobs in parallel using one or more computers. tools <code>password-store</code> <code>1.7.4</code> Website Simple password manager using gpg and ordinary unix directories. tools <code>patchelf</code> <code>0.17.2</code><code>0.18</code> Website A small utility to modify the dynamic linker and RPATH of ELF executables. tools <code>polyfill-glibc</code> <code>0.1</code> Website Patch Linux executables for compatibility with older glibc. tools <code>py-clustershell</code> <code>1.9.0_py39</code> Website ClusterShell is an event-driven open source Python library, designed to run local or distant commands in parallel on server farms or on large Linux clusters. tools <code>py-matlab-proxy</code> <code>0.9.1_py39</code><code>0.10.0_py39</code><code>0.24.2_py312</code> Website matlab-proxy is a Python package which enables you to launch MATLAB and access it from a web browser. tools <code>py-nvidia-ml-py</code> <code>12.550.52_py39</code> <code>12.550.52_py312</code> <code>12.560.30_py39</code> <code>12.560.30_py312</code> Website Python bindings to the NVIDIA Management Library. tools <code>py-pyside</code> <code>5.15.2.1_py39</code> Website PySide is the official Python module from the Qt for Python project, which provides access to the complete Qt framework. tools <code>py-wxpython</code> <code>4.0.7_py39</code><code>4.2.0_py39</code><code>4.2.1_py312</code> Website wxPython is the cross-platform GUI toolkit for the Python language, tools <code>qt</code> <code>5.9.1</code> <code>6.4.0</code> Website QT is a cross-platform application framework that is used for developing application software that can be run on various software and hardware platforms. tools <code>ripgrep</code> <code>11.0.1</code> Website ripgrep recursively searches directories for a regex pattern. tools <code>rocksdb</code> <code>5.7.3</code> Website A library that provides an embeddable, persistent key-value store for fast storage. tools <code>unifdef</code> <code>2.12</code> Website The unifdef utility selectively processes conditional C preprocessor #if and #ifdef directives. tools <code>wget</code> <code>1.25.0</code> Website GNU Wget is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS, the most widely used Internet protocols. tools <code>x11</code> <code>7.7</code> Website The X.Org project provides an open source implementation of the X Window System. tools <code>xcb-proto</code> <code>1.17.0</code> Website xcb-proto provides the XML-XCB protocol descriptions that libxcb uses to generate the majority of its code and API. tools <code>xcb-util-cursor</code> <code>0.1.5</code> Website The XCB util modules provides a number of libraries which sit on top of libxcb, the core X protocol library, and some of the extension libraries. tools <code>xkeyboard-config</code> <code>2.21</code> Website The non-arch keyboard configuration database for X Window."},{"location":"docs/software/list/#viz","title":"viz","text":"Field Module\u00a0name Version(s) URL Description data <code>ncview</code> <code>2.1.7</code> Website Ncview is a visual browser for netCDF format files. data <code>open3d</code> <code>0.19.0</code> Website A modern library for 3D data processing data <code>py-open3d</code> <code>0.19.0_py312</code> Website Python bindings for Open3D gis <code>gmt</code> <code>6.4.0</code> Website GMT (The Generic Mapping Tools) is an open source collection of command-line tools for manipulating geographic and Cartesian data sets. gis <code>panoply</code> <code>4.10.8</code> Website Panoply plots geo-referenced and other arrays from netCDF, HDF, GRIB, and other datasets. gis <code>py-cartopy</code> <code>0.21.0_py39</code> Website Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses. graphs <code>graphviz</code> <code>2.40.1</code><code>2.44.1</code> Website Graphviz is open source graph visualization software. image processing <code>vips</code> <code>8.16.0</code> Website A fast image processing library with low memory needs. imaging <code>py-pillow</code> <code>5.1.0_py27</code> <code>5.1.0_py36</code><code>7.0.0_py36</code><code>8.2.0_py39</code><code>9.3.0_py39</code><code>10.2.0_py312</code> Website Pillow is a friendly PIL (Python Imaging Library) fork. imaging <code>py-pillow-simd</code> <code>7.0.0.post3_py36</code><code>9.2.0_py39</code><code>10.2.0_py312</code> Website Pillow-SIMD is an optimized version of Pillow molecular visualization <code>ovito</code> <code>3.7.11</code> Website OVITO is a scientific visualization and data analysis solution for atomistic and other particle-based models. molecular visualization <code>pymol</code> <code>1.8.6.2</code> <code>2.5.3</code> Website PyMOL is a Python-enhanced molecular graphics tool. plotting <code>gnuplot</code> <code>5.2.0</code> Website Gnuplot is a portable command-line driven graphing utility for Linux, OS/2, MS Windows, OSX, VMS, and many other platforms. plotting <code>grace</code> <code>5.1.25</code> Website Grace is a WYSIWYG tool to make two-dimensional plots of numerical data. plotting <code>mathgl</code> <code>8.0.1</code> Website MathGL is a library to make high-quality scientific graphics. plotting <code>py-basemap</code> <code>1.1.0_py27</code> <code>1.1.0_py36</code> Website The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. plotting <code>py-matplotlib</code> <code>2.2.2_py27</code> <code>2.1.2_py27</code><code>2.1.2_py36</code><code>2.2.2_py36</code><code>3.1.1_py36</code><code>3.2.1_py36</code><code>3.4.2_py39</code><code>3.7.1_py39</code><code>3.8.3_py312</code><code>3.10.3_py312</code> Website Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. plotting <code>py-plotly</code> <code>2.4.1_py27</code><code>5.19.0_py39</code><code>5.19.0_py312</code> Website Plotly's Python graphing library makes interactive, publication-quality graphs online. plotting <code>py-seaborn</code> <code>0.12.1_py39</code><code>0.13.2_py312</code> Website Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. plotting <code>veusz</code> <code>3.3.1</code> Website Veusz is a scientific plotting and graphing program with a graphical user interface, designed to produce publication-ready 2D and 3D plots. remote display <code>virtualgl</code> <code>2.5.2</code> Website VirtualGL is an open source toolkit that gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration. toolkit <code>py-vtk</code> <code>9.4.1_py312</code> Website Python bindings for VTK, The Visualization Toolkit. toolkit <code>vtk</code> <code>9.4.1</code> Website The Visualization Toolkit (VTK) is open source software for manipulating and displaying scientific data."},{"location":"docs/software/modules/","title":"Modules","text":""},{"location":"docs/software/modules/#environment-modules","title":"Environment modules","text":"<p>Software is provided on Sherlock under the form of loadable environment modules.</p> <p>Software is only accessible via modules</p> <p>The use of a module system means that most software is not accessible by default and has to be loaded using the <code>module</code> command. This mechanism allows us to provide multiple versions of the same software concurrently, and gives users the possibility to easily switch between software versions.</p> <p>Sherlock uses Lmod to manage software installations. The modules system helps setting up the user's shell environment to give access to applications, and make running and compiling software easier. It also allows us to provide multiple versions of the same software, that would otherwise conflict with each other, and abstract things from the OS sometimes rigid versions and dependencies.</p> <p>When you first log into Sherlock, you'll be presented with a default, bare bone environment with minimal software available. The module system is used to manage the user environment and to activate software packages on demand. In order to use software installed on Sherlock, you must first load the corresponding software module.</p> <p>When you load a module, the system will set or modify your user environment variables to enable access to the software package provided by that module. For instance, the <code>$PATH</code> environment variable might be updated so that appropriate executables for that package can be used.</p>"},{"location":"docs/software/modules/#module-categories","title":"Module categories","text":"<p>Modules on Sherlock are organized by scientific field, in distinct categories. This is to limit the information overload that can result when displaying the full list of available modules. Given the large diversity of the Sherlock user population,  all users are not be interested in the same kind of software, and high-energy physicists may not want to see their screens cluttered with the latest bioinformatics packages.</p> <p>Module categories</p> <p>You will first have to load a category module before getting access to individual modules. The <code>math</code> and <code>devel</code> categories are loaded by default, and modules in those categories can be loaded directly</p> <p>For instance, to be able to load the <code>gromacs</code> module, you'll first need to load the <code>chemistry</code> module. This can be done in a single command, by specifying first the category, then the actual application module name:</p> <pre><code>$ module load chemistry gromacs\n</code></pre> <p>The <code>math</code> and <code>devel</code> categories, which are loaded by default, provide direct access to compilers, languages, and MPI and numerical libraries.</p> <p>For a complete list of software module categories, please refer to the list of available software</p> <p>Searching for a module</p> <p>To know how to access a module, you can use the <code>module spider &lt;module_name&gt;</code> command. It will search through all the installed modules, even if they're masked, and display instructions to load them. See the Examples section for details.</p>"},{"location":"docs/software/modules/#module-usage","title":"Module usage","text":"<p>The most common <code>module</code> commands are outlined in the following table. <code>module</code> commands may be shortened with the <code>ml</code> alias, with slightly different semantics.</p> <p>Module names auto-completion</p> <p>The <code>module</code> command supports auto-completion, so you can just start typing the name of a module, and press Tab to let the shell automatically complete the module name and/or version.</p> Module\u00a0command Short\u00a0version Description <code>module avail</code> <code>ml av</code> List\u00a0available\u00a0software<sup>1</sup> <code>module spider gromacs</code> <code>ml spider gromacs</code> Search for particular software <code>module keyword blas</code> <code>ml key blas</code> Search for <code>blas</code> in module names and descriptions <code>module whatis gcc</code> <code>ml whatis gcc</code> Display information about the <code>gcc</code> module <code>module help gcc</code> <code>ml help gcc</code> Display module specific help <code>module load gcc</code> <code>ml gcc</code> Load a module to use the associated software <code>module load gsl/2.3</code> <code>ml gsl/2.3</code> Load specific version of a module <code>module unload gcc</code> <code>ml -gcc</code> Unload a module <code>module swap gcc icc</code> <code>ml -gcc icc</code> Swap a module (unload <code>gcc</code> and replace it with <code>icc</code>) <code>module purge</code> <code>ml purge</code> Remove all modules<sup>2</sup> <code>module save foo</code> <code>ml save foo</code> Save the state of all loaded modules in a collection named <code>foo</code> <code>module restore foo</code> <code>ml restore foo</code> Restore the state of saved modules from the <code>foo</code> collection <p>Additional module sub-commands are documented in the <code>module help</code> command. For complete reference, please refer to the official Lmod documentation.</p>"},{"location":"docs/software/modules/#module-properties","title":"Module properties","text":"<p>Multiple versions</p> <p>When multiple versions of the same module exist, <code>module</code> will load the one marked as <code>Default (D)</code>. For the sake of reproducibility, we recommend always specifying the module version you want to load, as defaults may evolve over time.</p> <p>To quickly see some of the modules characteristics, <code>module avail</code> will display colored property attributes next to the module names. The main module properties are:</p> <ul> <li><code>S</code>: Module is sticky, requires <code>--force</code> to unload or purge</li> <li><code>L</code>: Indicate currently loaded module</li> <li><code>D</code>: Default module that will be loaded when multiple versions are available</li> <li><code>r</code>: Restricted access, typically software under license.  Contact   us for details</li> <li><code>g</code>: GPU-accelerated software, will only run on GPU nodes</li> <li><code>m</code>: Software supports parallel execution using MPI</li> </ul>"},{"location":"docs/software/modules/#searching-for-modules","title":"Searching for modules","text":"<p>You can search through all the available modules for either:</p> <ul> <li>a module name (if you already know it), using <code>module spider</code></li> <li>any string within modules names and descriptions, using <code>module keyword</code></li> </ul> <p>For instance, if you want to know how to load the <code>gromacs</code> module, you can do:</p> <pre><code>$ module spider gromacs\n</code></pre> <p>If you don't know the module name, or want to list all the modules that contain a specific string of characters in their name or description, you can use <code>module keyword</code>. For instance, the following command will list all the modules providing a BLAS library:</p> <pre><code>$ module keyword blas\n</code></pre>"},{"location":"docs/software/modules/#examples","title":"Examples","text":""},{"location":"docs/software/modules/#listing","title":"Listing","text":"<p>To list all the modules that can be loaded, you can do:</p> <pre><code>$ ml av\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology      devel (S,L)    physics    system\n   chemistry    math  (S,L)    staging    viz\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n   r:  Restricted access\n   g:  GPU support\n   L:  Module is loaded\n   m:  MPI support\n   D:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>"},{"location":"docs/software/modules/#searching","title":"Searching","text":"<p>To search for a specific string in modules names and descriptions, you can run:</p> <pre><code>$ module keyword numpy\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"numpy\"\n---------------------------------------------------------------------------\n\n  py-scipystack: py-scipystack/1.0_py27, py-scipystack/1.0_py36\n    The SciPy Stack is a collection of open source software for scientific\n    computing in Python. It provides the following packages: numpy, scipy,\n    matplotlib, ipython, jupyter, pandas, sympy and nose.\n\n---------------------------------------------------------------------------\n[...]\n$ ml key compiler\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"compiler\"\n---------------------------------------------------------------------------\n\n  cmake: cmake/3.8.1\n    CMake is an extensible, open-source system that manages the build\n    process in an operating system and in a compiler-independent manner.\n\n  gcc: gcc/6.3.0, gcc/7.1.0\n    The GNU Compiler Collection includes front ends for C, C++, Fortran,\n    Java, and Go, as well as libraries for these languages (libstdc++,\n    libgcj,...).\n\n  icc: icc/2017.u2\n    Intel C++ Compiler, also known as icc or icl, is a group of C and C++\n    compilers from Intel\n\n  ifort: ifort/2017.u2\n    Intel Fortran Compiler, also known as ifort, is a group of Fortran\n    compilers from Intel\n\n  llvm: llvm/4.0.0\n    The LLVM Project is a collection of modular and reusable compiler and\n    toolchain technologies. Clang is an LLVM native C/C++/Objective-C\n    compiler,\n\n---------------------------------------------------------------------------\n</code></pre> <p>To get information about a specific module, especially how to load it, the following command can be used:</p> <pre><code>$ module spider gromacs\n\n-------------------------------------------------------------------------------\n  gromacs: gromacs/2016.3\n-------------------------------------------------------------------------------\n    Description:\n      GROMACS is a versatile package to perform molecular dynamics, i.e.\n      simulate the Newtonian equations of motion for systems with hundreds to\n      millions of particles.\n\n    Properties:\n      GPU support      MPI support\n\n    You will need to load all module(s) on any one of the lines below before\n    the \"gromacs/2016.3\" module is available to load.\n\n      chemistry\n</code></pre>"},{"location":"docs/software/modules/#loading","title":"Loading","text":"<p>Loading a category module allows to get access to field-specific software:</p> <pre><code>$ ml chemistry\n$ ml av\n\n------------- chemistry -- quantum chemistry, molecular dynamics --------------\n   gromacs/2016.3 (g,m)    vasp/5.4.1 (g,r,m)\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology          devel (S,L)    physics    system\n   chemistry (L)    math  (S,L)    staging    viz\n\n[...]\n</code></pre>"},{"location":"docs/software/modules/#resetting-the-modules-environment","title":"Resetting the modules environment","text":"<p>If you want to reset your modules environment as it was when you initially connected to Sherlock, you can use the <code>ml reset</code> command: it will remove all the modules you have loaded, and restore the original state where only the <code>math</code> and <code>devel</code> categories are accessible.</p> <p>If you want to remove all modules from your environment, including the default <code>math</code> and <code>devel</code> modules, you can use <code>ml --force purge</code>.</p>"},{"location":"docs/software/modules/#loading-modules-in-jobs","title":"Loading modules in jobs","text":"<p>In order for an application running in a Slurm job to have access to any necessary module-provided software packages, we recommend loading those modules in the job script directly. Since Slurm propagates all user environment variables by default, this is not strictly necessary, as jobs will inherit the modules loaded at submission time. But to make sure things are reproducible and avoid issues, it is preferable to explicitly load the modules in the batch scripts.</p> <p><code>module load</code> commands should be placed right after <code>#SBATCH</code> directives and before the actual executable calls. For instance:</p> <pre><code>#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n#SBATCH ...\n\nml reset\nml load gromacs/2016.3\n\nsrun gmx_mpi ...\n</code></pre>"},{"location":"docs/software/modules/#custom-modules","title":"Custom modules","text":"<p>Users are welcome and encouraged to build and install their own software on Sherlock. To that end, and to facilitate usage or sharing of their custom software installations, they can create their own module repositories.</p> <p>See the Software Installation page for more details.</p>"},{"location":"docs/software/modules/#contributed-software","title":"Contributed software","text":"<p>PI groups, labs or departments can share their software installations and modules with the whole Sherlock community of users, and let everyone benefit from their tuning efforts and software developments.</p> <p>Those modules are available in the specific <code>contribs</code> category, and organized by contributor name.</p> <p>For instance, listing the available contributed modules can be done with:</p> <pre><code>$ ml contribs\n$ ml av\n-------------------- contribs -- contributed software ----------------------\n   poldrack\n</code></pre> <p>To get information about a specific lab module:</p> <pre><code>$ ml show poldrack\n----------------------------------------------------------------------------\n   /share/software/modules/contribs/poldrack.lua:\n----------------------------------------------------------------------------\nprepend_path(\"MODULEPATH\",\"/home/groups/russpold/modules\")\nwhatis(\"Name:        poldrack\")\nwhatis(\"Version:     1.0\")\nwhatis(\"Category:    contribs\")\nwhatis(\"URL:         https://github.com/poldracklab/lmod_modules\")\nwhatis(\"Description: Software modules contributed by the Poldrack Lab.\")\n</code></pre> <p>And to list the available software modules contributed by the lab:</p> <pre><code>$ ml poldrack\n$ ml av\n\n------------------------ /home/groups/russpold/modules -------------------------\n   afni/17.3.03           freesurfer/6.0.1            gsl/2.3      (D)\n   anaconda/5.0.0-py36    fsl/5.0.9                   pigz/2.4\n   ants/2.1.0.post710     fsl/5.0.11           (D)    remora/1.8.2\n   c3d/1.1.0              git-annex/6.20171109        xft/2.3.2\n[...]\n</code></pre> <ol> <li> <p>If a module is not listed here, it might be unavailable in the   loaded modules categories, and require loading another category module.   Search for not-listed software using the <code>module spider</code> command.\u00a0\u21a9</p> </li> <li> <p>The <code>math</code> and <code>devel</code> category modules will not be unloaded   with <code>module purge</code> as they are \"sticky\". If a user wants to unload a sticky   module, they must specify the <code>--force</code> option.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/containers/","title":"Index","text":""},{"location":"docs/software/containers/#introduction","title":"Introduction","text":"<p>Containers are a solution to the problem of how to get software to run reliably when moved from one computing environment to another. They also resolve installation problems by packaging all the dependencies of an application within a self-sustainable image, a.k.a a container.</p> <p>What's a container?</p> <p>Put simply, a container consists of an entire runtime environment: an application, plus all its dependencies, libraries and other binaries, and configuration files needed to run it, bundled into one package. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away.</p>"},{"location":"docs/software/containers/#container-solutions","title":"Container solutions","text":"<p>There are several ways to run containers in general, and on Sherlock specifically.</p> <ul> <li> <p> Apptainer</p> <p>Apptainer (formerly Singularity) is an open source container platform designed to run complex applications on high-performance computing (HPC) clusters in a simple, portable, and reproducible way.</p> <p> More information</p> </li> <li> <p>More to come...</p> </li> </ul>"},{"location":"docs/software/containers/apptainer/","title":"Singularity","text":"<p>Singularity is an open source container platform designed to run complex applications on high-performance computing (HPC) clusters in a simple, portable, and reproducible way. It's like Docker, but for HPC systems.</p>"},{"location":"docs/software/containers/apptainer/#why-not-docker","title":"Why not Docker?","text":"<p>Docker has long been the reference and the most popular container framework in DevOps and Enterprise IT environments, so why not use Docker on Sherlock? Well, for a variety of technical reasons, mostly related to security.</p> <p>Docker has never been designed nor developed to run in multi-tenants environments, and even less on HPC clusters. Specifically:</p> <ul> <li>Docker requires a daemon running as <code>root</code> on all of the compute nodes, which   has serious security implications,</li> <li>all authenticated actions (such as <code>login</code>, <code>push</code> ...) are also executed as   <code>root</code>, meaning that multiple users can't use those functions on the same   node,</li> <li>Docker uses cgroups to isolate containers, as does the Slurm scheduler, which   uses cgroups to allocate resources to jobs and enforce limits. Those uses are   unfortunately conflicting.</li> <li>but most importantly, allowing users to run Docker containers will give   them <code>root</code> privileges inside that container, which will in turn let them   access any of the clusters' filesystems as <code>root</code>. This opens the door to   user impersonation, inappropriate file tampering or stealing, and is   obviously not something that can be allowed on a shared resource.</li> </ul> <p>That last point is certainly the single most important reason why we won't use Docker on Sherlock.</p>"},{"location":"docs/software/containers/apptainer/#why-singularity","title":"Why Singularity?","text":"<p>Singularity is Docker for HPC systems</p> <p>Singularity allows running Docker containers natively, and is a perfect replacement for Docker on HPC systems such as Sherlock. That means that existing Docker container can be directly imported and natively run with SIngularity.</p> <p>Despite Docker's shortcomings on HPC systems, the appeal of containers for scientific computing is undeniable, which is why we provide Singularity on Sherlock. Singularity is an alternative container framework, especially designed to run scientific applications on HPC clusters.</p> <p>Singularity provides the same functionalities as Docker, without any of the drawbacks listed above. Using a completely different implementation, it doesn't require any privilege to run containers, and allow direct interaction with existing Docker containers.</p> <p>The main motivation to use Singularity over Docker is the fact that it's been developed with HPC systems in mind, to solve those specific problems:</p> <ul> <li>security: a user in the container is the same user as the one running the   container, so no privilege escalation possible,</li> <li>ease of deployment: no daemon running as root on each node, a container is     simply an executable,</li> <li>no need to mount filesystems or do bind mappings to access devices,</li> <li>ability to run MPI jobs based on containers,</li> <li>and more...</li> </ul>"},{"location":"docs/software/containers/apptainer/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Singularity on Sherlock. For more complete documentation about building and running containers with Singularity, please see the Singularity documentation.</p>"},{"location":"docs/software/containers/apptainer/#singularity-on-sherlock","title":"Singularity on Sherlock","text":"<p>As announced during the SC'18 Supercomputing Conference, Singularity is an integral part of the Sherlock cluster, and Singularity commands can be executed natively on any login or compute node, without the need to load any additional module<sup>1</sup>.</p>"},{"location":"docs/software/containers/apptainer/#importing-containers","title":"Importing containers","text":"<p>Pre-built containers can be obtained from a variety of sources. For instance:</p> <ul> <li>DockerHub contains containers for various software   packages, which can be directly used with   Singularity,</li> <li>SingularityHub is a registry for scientific   linux containers,</li> <li>the NVIDIA GPU Cloud registry for   GPU-optimized containers,</li> <li>many individual projects contain specific instructions for installation via   Docker and/or Singularity, and may provide pre-built images in other   locations.</li> </ul> <p>To illustrate how Singularity can import and run Docker containers, here's an example how to install and run the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but Singularity makes it very easy.</p> <p>Interactive or batch usage</p> <p>This example shows how to use Singularity interactively, but Singularity containers can be run in batch jobs as well.</p> <p>The first step is to request an interactive shell. Singularity images can be pulled directly on compute nodes, and Singularity uses multiple CPU cores when assembling the image, so requesting multiple cores in your job can make the pull operation faster:</p> <pre><code>$ sh_dev -c 4\n</code></pre> <p>We recommend storing Singularity images in <code>$GROUP_HOME</code>, as container images can take significant space in your <code>$HOME</code> directory.</p> <pre><code>$ mkdir -p $GROUP_HOME/$USER/simg\n$ cd $GROUP_HOME/$USER/simg\n</code></pre> <p>Then, the OpenFOAM container could be pulled directly from DockerHub by Singularity. This can take a moment to complete:</p> <pre><code>$ singularity pull docker://openfoam/openfoam6-paraview54\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nCopying blob 1be7f2b886e8 done   | ...\n...\nINFO:    Creating SIF file...\n</code></pre>"},{"location":"docs/software/containers/apptainer/#running-containers","title":"Running containers","text":"<p>Once the image is downloaded, you are ready to run OpenFOAM from the container. The <code>singularity shell</code> command can be used to start the container, and run a shell within that image:</p> <p>By default on Sherlock, all the filesystems that are available on the compute node will also be available in the container. If you want to start your shell in a specific directory, you can use the <code>--pwd /path/</code> option. For instance, we'll create a <code>/tmp/openfoam_test/</code> directory to store our tests results (that will be wiped out at the end of the job), and start the container shell there:</p> <pre><code>$ mkdir /tmp/openfoam_test\n$ singularity shell --pwd /tmp/openfoam_test openfoam6-paraview54_latest.sif\n[Apptainer&gt;\n</code></pre> <p>You're now in the container, as denoted by the shell prompt (<code>Apptainer[...].simg:[path]&gt;</code>), which is different from the prompt displayed on the compute node (which usually looks like <code>[login]@[compute_node] [path]$</code>.</p> <p>OpenFOAM provides a convenience script that can be sourced to make OpenFOAM commands directly accessible and set a few useful environment variables:</p> <pre><code>&gt; source /opt/openfoam6/etc/bashrc\n</code></pre> <p>Now, we can run a simple example using OpenFOAM:</p> <pre><code>[Apptainer&gt; cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily .\n[Apptainer&gt; cd pitzDaily\n[Apptainer&gt; blockMesh\n[...]\nEnd\n\n&gt; simpleFoam\n/*---------------------------------------------------------------------------*\\\n  =========                 |\n  \\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n   \\\\    /   O peration     | Website:  https://openfoam.org\n    \\\\  /    A nd           | Version:  6\n     \\\\/     M anipulation  |\n\\*---------------------------------------------------------------------------*/\nBuild  : 6-1a0c91b3baa8\nExec   : simpleFoam\nDate   : Oct 05 2018\nTime   : 23:37:30\nHost   : \"sh01-06n33.int\"\nPID    : 14670\nI/O    : uncollated\nCase   : /tmp/openfoam_test/pitzDaily\nnProcs : 1\nsigFpe : Enabling floating point exception trapping (FOAM_SIGFPE).\nfileModificationChecking : Monitoring run-time modified files using timeStampMaster (fileModificationSkew 10)\nallowSystemOperations : Allowing user-supplied system call operations\n\n// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //\nCreate time\n[...]\nSIMPLE solution converged in 288 iterations\n\nstreamLine streamlines write:\n    seeded 10 particles\n    Tracks:10\n    Total samples:11980\n    Writing data to \"/tmp/openfoam_test/pitzDaily/postProcessing/sets/streamlines/288\"\nEnd\n\n&gt;\n</code></pre> <p>When the simulation is done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/openfoam_test</code> after you exit the container:</p> <pre><code>$ ls /tmp/openfoam_test/pitzDaily/postProcessing/\nsets\n</code></pre>"},{"location":"docs/software/containers/apptainer/#gpu-enabled-containers","title":"GPU-enabled containers","text":"<p>Sherlock also supports the use of container images provided by NVIDIA in the NVIDIA GPU Cloud (NGC). This registry provides GPU-accelerated containers for the most popular HPC and deep-learning scientific applications.</p> <p>GPU support</p> <p>Containers provided on NGC are only supported on Pascal and Volta architectures (TITAN Xp, Tesla P40, P100 or V100). For GPUs from the previous generations (GTX TITAN Black/X, Tesla K20/K80), things may or may not work.</p> <p>We recommend making sure to select a supported GPU generation by adding the following directive to your batch script when submitting a job to run GPU-enabled containers from NGC: <pre><code>#SBATCH -C \"GPU_GEN:PSC|GPU_GEN:VLT\"\n</code></pre></p>"},{"location":"docs/software/containers/apptainer/#pulling-ngc-images","title":"Pulling NGC images","text":"<p>As before, we start by requesting an interactive shell with multiple CPU cores, loading the Singularity module and moving the directory where we'll save those images:</p> <pre><code>$ sh_dev -c 4\n$ cd $GROUP_HOME/simg\n</code></pre> <p>A GPU is not required for pulling GPU-enabled containers</p> <p>GPU-enabled containers can be pulled on any node, including nodes without a GPU. But their execution requires a GPU and thus, they need to be executed within a GPU job. See the GPU job section for more information.</p> <p>To be able to pull an image from NGC, authentication credentials must be set. Users need to register and create an NGC API key, complete details could be found in the NGC Getting Started Guide.</p> <p>You can then set the following environment variable to allow Singularity to authenticate with NGC:</p> <pre><code>$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken'\n$ export SINGULARITY_DOCKER_PASSWORD=&lt;NVIDIA NGC API key&gt;\n</code></pre> <p>Note</p> <p>The <code>SINGULARITY_DOCKER_USERNAME</code> environment variable must be set to the literal <code>$oauthtoken</code> string, for every user. It should not be replaced by anything else. Only the API key is specific to each user.</p> <p>Once credentials are set in the environment, container images can be pulled from the NGC registry normally.</p> <p>The general form of the Singularity command used to pull NGC containers is: <code>$ singularity pull docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;</code></p> <p>For example to pull the NAMD NGC container tagged with version <code>2.12-171025</code> the corresponding command would be:</p> <pre><code>$ singularity pull docker://nvcr.io/hpc/namd:2.12-171025\n</code></pre> <p>After this command has finished, we'll have a Singularity image file in the current directory, named <code>namd-2.12-171025.simg</code>.</p>"},{"location":"docs/software/containers/apptainer/#running-ngc-containers","title":"Running NGC containers","text":"<p>Instructions about running NGC containers are provided on the NGC website, under each application:</p> <p></p> <p>Each application comes with specific running instructions, so we recommend to follow the container's particular guidelines before running it with Singularity.</p> <p>Containers that lack Singularity documentation have not been tested with Singularity.</p> <p>Since all NGC containers are optimized for GPU acceleration, they will always be executed with the <code>--nv</code> Singularity option, to enable GPU support within the container.</p> <p>We also need to submit a job requesting a GPU to run GPU-enabled containers.  For instance:</p> <pre><code>$ srun -p gpu -c 4 --gres gpu:1 --pty bash\n</code></pre> <p>This will start an interactive shell on a GPU node, with 4 CPU cores and 1 GPU.</p> <p>The NAMD container that was pulled just before can now be started with the following commands. We start by creating a temporary directory to hold the execution results, and start the container using this as the current directory:</p> <pre><code>$ mkdir /tmp/namd_test\n$ singularity shell --nv --pwd /tmp/namd_test $GROUP_HOME/simg/namd-2.12-171025.simg\nSingularity: Invoking an interactive shell within container...\n\nSingularity namd-2.12-171025.simg:/tmp/namd_test&gt;\n</code></pre> <p>From there, we can run a NAMD test to verify that everything is working as expected.</p> <pre><code>&gt; cp -r /workspace/examples .\n&gt; /opt/namd/namd-multicore +p4 +idlepoll examples/apoa1/apoa1.namd\nCharm++: standalone mode (not using charmrun)\nCharm++&gt; Running in Multicore mode:  4 threads\nCharm++&gt; Using recursive bisection (scheme 3) for topology aware partitions\nConverse/Charm++ Commit ID: v6.8.2\n[...]\nInfo: Built with CUDA version 9000\nDid not find +devices i,j,k,... argument, using all\nPe 1 physical rank 1 will use CUDA device of pe 2\nPe 3 physical rank 3 will use CUDA device of pe 2\nPe 0 physical rank 0 will use CUDA device of pe 2\nPe 2 physical rank 2 binding to CUDA device 0 on sh02-14n13.int: 'TITAN Xp'  Mem: 12196MB  Rev: 6.1\nInfo: NAMD 2.12 for Linux-x86_64-multicore-CUDA\n[...]\nInfo: SIMULATION PARAMETERS:\nInfo: TIMESTEP               1\n[...]\nENERGY:    2000     20247.5090     20325.4554      5719.0088       183.9328        -340639.3103     25366.3986         0.0000         0.0000     46364.9951        -222432.0107       168.6631   -268797.0057   -222054.5175       168.8733          -1129.9509     -1799.6459    921491.4634     -2007.8380     -2007.4145\n\nWRITING EXTENDED SYSTEM TO OUTPUT FILE AT STEP 2000\nWRITING COORDINATES TO OUTPUT FILE AT STEP 2000\nThe last position output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\nWRITING VELOCITIES TO OUTPUT FILE AT STEP 2000\nThe last velocity output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\n====================================================\n\nWallClock: 17.593451  CPUTime: 17.497925  Memory: 559.843750 MB\n[Partition 0][Node 0] End of program\n</code></pre> <p>The simulation should take a few seconds to run. You can verify that it correctly executed on a GPU in the output above. When it's done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/named_test</code> after you exit the container:</p> <pre><code>$ cd /tmp/namd_test/examples/apoa1/\n$ ls apoa1-out*\napoa1-out.coor  apoa1-out.vel  apoa1-out.xsc\n</code></pre>"},{"location":"docs/software/containers/apptainer/#building-your-own-containers","title":"Building your own containers","text":"<p>For complete details about how to build Singularity containers, please refer to the Singularity documentation.</p> <ol> <li> <p>For more information about using modules on Sherlock,   please see the software modules documentation.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/containers/singularity/","title":"Singularity","text":"<p>Singularity is an open source container platform designed to run complex applications on high-performance computing (HPC) clusters in a simple, portable, and reproducible way. It's like Docker, but for HPC systems.</p>"},{"location":"docs/software/containers/singularity/#why-not-docker","title":"Why not Docker?","text":"<p>Docker has long been the reference and the most popular container framework in DevOps and Enterprise IT environments, so why not use Docker on Sherlock? Well, for a variety of technical reasons, mostly related to security.</p> <p>Docker has never been designed nor developed to run in multi-tenants environments, and even less on HPC clusters. Specifically:</p> <ul> <li>Docker requires a daemon running as <code>root</code> on all of the compute nodes, which   has serious security implications,</li> <li>all authenticated actions (such as <code>login</code>, <code>push</code> ...) are also executed as   <code>root</code>, meaning that multiple users can't use those functions on the same   node,</li> <li>Docker uses cgroups to isolate containers, as does the Slurm scheduler, which   uses cgroups to allocate resources to jobs and enforce limits. Those uses are   unfortunately conflicting.</li> <li>but most importantly, allowing users to run Docker containers will give   them <code>root</code> privileges inside that container, which will in turn let them   access any of the clusters' filesystems as <code>root</code>. This opens the door to   user impersonation, inappropriate file tampering or stealing, and is   obviously not something that can be allowed on a shared resource.</li> </ul> <p>That last point is certainly the single most important reason why we won't use Docker on Sherlock.</p>"},{"location":"docs/software/containers/singularity/#why-singularity","title":"Why Singularity?","text":"<p>Singularity is Docker for HPC systems</p> <p>Singularity allows running Docker containers natively, and is a perfect replacement for Docker on HPC systems such as Sherlock. That means that existing Docker container can be directly imported and natively run with SIngularity.</p> <p>Despite Docker's shortcomings on HPC systems, the appeal of containers for scientific computing is undeniable, which is why we provide Singularity on Sherlock. Singularity is an alternative container framework, especially designed to run scientific applications on HPC clusters.</p> <p>Singularity provides the same functionalities as Docker, without any of the drawbacks listed above. Using a completely different implementation, it doesn't require any privilege to run containers, and allow direct interaction with existing Docker containers.</p> <p>The main motivation to use Singularity over Docker is the fact that it's been developed with HPC systems in mind, to solve those specific problems:</p> <ul> <li>security: a user in the container is the same user as the one running the   container, so no privilege escalation possible,</li> <li>ease of deployment: no daemon running as root on each node, a container is     simply an executable,</li> <li>no need to mount filesystems or do bind mappings to access devices,</li> <li>ability to run MPI jobs based on containers,</li> <li>and more...</li> </ul>"},{"location":"docs/software/containers/singularity/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Singularity on Sherlock. For more complete documentation about building and running containers with Singularity, please see the Singularity documentation.</p>"},{"location":"docs/software/containers/singularity/#singularity-on-sherlock","title":"Singularity on Sherlock","text":"<p>As announced during the SC'18 Supercomputing Conference, Singularity is an integral part of the Sherlock cluster, and Singularity commands can be executed natively on any login or compute node, without the need to load any additional module<sup>1</sup>.</p>"},{"location":"docs/software/containers/singularity/#importing-containers","title":"Importing containers","text":"<p>Pre-built containers can be obtained from a variety of sources. For instance:</p> <ul> <li>DockerHub contains containers for various software   packages, which can be directly used with   Singularity,</li> <li>SingularityHub is a registry for scientific   linux containers,</li> <li>the NVIDIA GPU Cloud registry for   GPU-optimized containers,</li> <li>many individual projects contain specific instructions for installation via   Docker and/or Singularity, and may provide pre-built images in other   locations.</li> </ul> <p>To illustrate how Singularity can import and run Docker containers, here's an example how to install and run the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but Singularity makes it very easy.</p> <p>Interactive or batch usage</p> <p>This example shows how to use Singularity interactively, but Singularity containers can be run in batch jobs as well.</p> <p>The first step is to request an interactive shell. Singularity images can be pulled directly on compute nodes, and Singularity uses multiple CPU cores when assembling the image, so requesting multiple cores in your job can make the pull operation faster:</p> <pre><code>$ sh_dev -c 4\n</code></pre> <p>We recommend storing Singularity images in <code>$GROUP_HOME</code>, as container images can take significant space in your <code>$HOME</code> directory.</p> <pre><code>$ mkdir -p $GROUP_HOME/$USER/simg\n$ cd $GROUP_HOME/$USER/simg\n</code></pre> <p>Then, the OpenFOAM container could be pulled directly from DockerHub by Singularity. This can take a moment to complete:</p> <pre><code>$ singularity pull docker://openfoam/openfoam6-paraview54\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nCopying blob 1be7f2b886e8 done   | ...\n...\nINFO:    Creating SIF file...\n</code></pre>"},{"location":"docs/software/containers/singularity/#running-containers","title":"Running containers","text":"<p>Once the image is downloaded, you are ready to run OpenFOAM from the container. The <code>singularity shell</code> command can be used to start the container, and run a shell within that image:</p> <p>By default on Sherlock, all the filesystems that are available on the compute node will also be available in the container. If you want to start your shell in a specific directory, you can use the <code>--pwd /path/</code> option. For instance, we'll create a <code>/tmp/openfoam_test/</code> directory to store our tests results (that will be wiped out at the end of the job), and start the container shell there:</p> <pre><code>$ mkdir /tmp/openfoam_test\n$ singularity shell --pwd /tmp/openfoam_test openfoam6-paraview54_latest.sif\n[Apptainer&gt;\n</code></pre> <p>You're now in the container, as denoted by the shell prompt (<code>Apptainer[...].simg:[path]&gt;</code>), which is different from the prompt displayed on the compute node (which usually looks like <code>[login]@[compute_node] [path]$</code>.</p> <p>OpenFOAM provides a convenience script that can be sourced to make OpenFOAM commands directly accessible and set a few useful environment variables:</p> <pre><code>&gt; source /opt/openfoam6/etc/bashrc\n</code></pre> <p>Now, we can run a simple example using OpenFOAM:</p> <pre><code>[Apptainer&gt; cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily .\n[Apptainer&gt; cd pitzDaily\n[Apptainer&gt; blockMesh\n[...]\nEnd\n\n&gt; simpleFoam\n/*---------------------------------------------------------------------------*\\\n  =========                 |\n  \\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n   \\\\    /   O peration     | Website:  https://openfoam.org\n    \\\\  /    A nd           | Version:  6\n     \\\\/     M anipulation  |\n\\*---------------------------------------------------------------------------*/\nBuild  : 6-1a0c91b3baa8\nExec   : simpleFoam\nDate   : Oct 05 2018\nTime   : 23:37:30\nHost   : \"sh01-06n33.int\"\nPID    : 14670\nI/O    : uncollated\nCase   : /tmp/openfoam_test/pitzDaily\nnProcs : 1\nsigFpe : Enabling floating point exception trapping (FOAM_SIGFPE).\nfileModificationChecking : Monitoring run-time modified files using timeStampMaster (fileModificationSkew 10)\nallowSystemOperations : Allowing user-supplied system call operations\n\n// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //\nCreate time\n[...]\nSIMPLE solution converged in 288 iterations\n\nstreamLine streamlines write:\n    seeded 10 particles\n    Tracks:10\n    Total samples:11980\n    Writing data to \"/tmp/openfoam_test/pitzDaily/postProcessing/sets/streamlines/288\"\nEnd\n\n&gt;\n</code></pre> <p>When the simulation is done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/openfoam_test</code> after you exit the container:</p> <pre><code>$ ls /tmp/openfoam_test/pitzDaily/postProcessing/\nsets\n</code></pre>"},{"location":"docs/software/containers/singularity/#gpu-enabled-containers","title":"GPU-enabled containers","text":"<p>Sherlock also supports the use of container images provided by NVIDIA in the NVIDIA GPU Cloud (NGC). This registry provides GPU-accelerated containers for the most popular HPC and deep-learning scientific applications.</p> <p>GPU support</p> <p>Containers provided on NGC are only supported on Pascal and Volta architectures (TITAN Xp, Tesla P40, P100 or V100). For GPUs from the previous generations (GTX TITAN Black/X, Tesla K20/K80), things may or may not work.</p> <p>We recommend making sure to select a supported GPU generation by adding the following directive to your batch script when submitting a job to run GPU-enabled containers from NGC: <pre><code>#SBATCH -C \"GPU_GEN:PSC|GPU_GEN:VLT\"\n</code></pre></p>"},{"location":"docs/software/containers/singularity/#pulling-ngc-images","title":"Pulling NGC images","text":"<p>As before, we start by requesting an interactive shell with multiple CPU cores, loading the Singularity module and moving the directory where we'll save those images:</p> <pre><code>$ sh_dev -c 4\n$ cd $GROUP_HOME/simg\n</code></pre> <p>A GPU is not required for pulling GPU-enabled containers</p> <p>GPU-enabled containers can be pulled on any node, including nodes without a GPU. But their execution requires a GPU and thus, they need to be executed within a GPU job. See the GPU job section for more information.</p> <p>To be able to pull an image from NGC, authentication credentials must be set. Users need to register and create an NGC API key, complete details could be found in the NGC Getting Started Guide.</p> <p>You can then set the following environment variable to allow Singularity to authenticate with NGC:</p> <pre><code>$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken'\n$ export SINGULARITY_DOCKER_PASSWORD=&lt;NVIDIA NGC API key&gt;\n</code></pre> <p>Note</p> <p>The <code>SINGULARITY_DOCKER_USERNAME</code> environment variable must be set to the literal <code>$oauthtoken</code> string, for every user. It should not be replaced by anything else. Only the API key is specific to each user.</p> <p>Once credentials are set in the environment, container images can be pulled from the NGC registry normally.</p> <p>The general form of the Singularity command used to pull NGC containers is: <code>$ singularity pull docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;</code></p> <p>For example to pull the NAMD NGC container tagged with version <code>2.12-171025</code> the corresponding command would be:</p> <pre><code>$ singularity pull docker://nvcr.io/hpc/namd:2.12-171025\n</code></pre> <p>After this command has finished, we'll have a Singularity image file in the current directory, named <code>namd-2.12-171025.simg</code>.</p>"},{"location":"docs/software/containers/singularity/#running-ngc-containers","title":"Running NGC containers","text":"<p>Instructions about running NGC containers are provided on the NGC website, under each application:</p> <p></p> <p>Each application comes with specific running instructions, so we recommend to follow the container's particular guidelines before running it with Singularity.</p> <p>Containers that lack Singularity documentation have not been tested with Singularity.</p> <p>Since all NGC containers are optimized for GPU acceleration, they will always be executed with the <code>--nv</code> Singularity option, to enable GPU support within the container.</p> <p>We also need to submit a job requesting a GPU to run GPU-enabled containers.  For instance:</p> <pre><code>$ srun -p gpu -c 4 --gres gpu:1 --pty bash\n</code></pre> <p>This will start an interactive shell on a GPU node, with 4 CPU cores and 1 GPU.</p> <p>The NAMD container that was pulled just before can now be started with the following commands. We start by creating a temporary directory to hold the execution results, and start the container using this as the current directory:</p> <pre><code>$ mkdir /tmp/namd_test\n$ singularity shell --nv --pwd /tmp/namd_test $GROUP_HOME/simg/namd-2.12-171025.simg\nSingularity: Invoking an interactive shell within container...\n\nSingularity namd-2.12-171025.simg:/tmp/namd_test&gt;\n</code></pre> <p>From there, we can run a NAMD test to verify that everything is working as expected.</p> <pre><code>&gt; cp -r /workspace/examples .\n&gt; /opt/namd/namd-multicore +p4 +idlepoll examples/apoa1/apoa1.namd\nCharm++: standalone mode (not using charmrun)\nCharm++&gt; Running in Multicore mode:  4 threads\nCharm++&gt; Using recursive bisection (scheme 3) for topology aware partitions\nConverse/Charm++ Commit ID: v6.8.2\n[...]\nInfo: Built with CUDA version 9000\nDid not find +devices i,j,k,... argument, using all\nPe 1 physical rank 1 will use CUDA device of pe 2\nPe 3 physical rank 3 will use CUDA device of pe 2\nPe 0 physical rank 0 will use CUDA device of pe 2\nPe 2 physical rank 2 binding to CUDA device 0 on sh02-14n13.int: 'TITAN Xp'  Mem: 12196MB  Rev: 6.1\nInfo: NAMD 2.12 for Linux-x86_64-multicore-CUDA\n[...]\nInfo: SIMULATION PARAMETERS:\nInfo: TIMESTEP               1\n[...]\nENERGY:    2000     20247.5090     20325.4554      5719.0088       183.9328        -340639.3103     25366.3986         0.0000         0.0000     46364.9951        -222432.0107       168.6631   -268797.0057   -222054.5175       168.8733          -1129.9509     -1799.6459    921491.4634     -2007.8380     -2007.4145\n\nWRITING EXTENDED SYSTEM TO OUTPUT FILE AT STEP 2000\nWRITING COORDINATES TO OUTPUT FILE AT STEP 2000\nThe last position output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\nWRITING VELOCITIES TO OUTPUT FILE AT STEP 2000\nThe last velocity output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\n====================================================\n\nWallClock: 17.593451  CPUTime: 17.497925  Memory: 559.843750 MB\n[Partition 0][Node 0] End of program\n</code></pre> <p>The simulation should take a few seconds to run. You can verify that it correctly executed on a GPU in the output above. When it's done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/named_test</code> after you exit the container:</p> <pre><code>$ cd /tmp/namd_test/examples/apoa1/\n$ ls apoa1-out*\napoa1-out.coor  apoa1-out.vel  apoa1-out.xsc\n</code></pre>"},{"location":"docs/software/containers/singularity/#building-your-own-containers","title":"Building your own containers","text":"<p>For complete details about how to build Singularity containers, please refer to the Singularity documentation.</p> <ol> <li> <p>For more information about using modules on Sherlock,   please see the software modules documentation.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/using/R/","title":"R","text":""},{"location":"docs/software/using/R/#introduction","title":"Introduction","text":"<p>R is a programming language and software environment for statistical computing and graphics. It is similar to the S language and environment developed at Bell Laboratories. R provides a wide variety of statistical and graphical techniques and is highly extensible.</p>"},{"location":"docs/software/using/R/#more-documentation","title":"More documentation","text":"<p>The following documentation is specifically intended for using R on Sherlock. For more complete documentation about R in general, please see the R documentation.</p>"},{"location":"docs/software/using/R/#r-on-sherlock","title":"R on Sherlock","text":"<p>R is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml R\n</code></pre> <p>For a list of available versions, you can execute <code>ml spider R</code> at the Sherlock prompt, or refer to the Software list page.</p>"},{"location":"docs/software/using/R/#using-r","title":"Using R","text":"<p>Once your environment is configured (ie. when the <code>R</code> module is loaded), R can be started by simply typing R at the shell prompt:</p> <pre><code>$ R\n\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n[...]\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ R --help\n</code></pre>"},{"location":"docs/software/using/R/#running-a-r-script","title":"Running a R script","text":"<p>There are several ways to launch an R script on the command line, which will have different ways of presenting the script's output:</p> Method Output <code>Rscript script.R</code> displayed on screen, on <code>stdout</code> <code>R CMD BATCH script.R</code> redirected to a <code>script.Rout</code> file <code>R --no-save &lt; script.R</code> displayed on screen, on <code>stdout</code>"},{"location":"docs/software/using/R/#submitting-a-r-job","title":"Submitting a R job","text":"<p>Here's an example R batch script that can be submitted via <code>sbatch</code>. It runs a simple matrix multiplication example, and demonstrates how to feed R code as a <code>HEREDOC</code> to R directly, so no intermediate R script is necessary:</p> <code>Rtest.sbatch</code> <pre><code>#!/usr/bin/bash\n#SBATCH --time=00:10:00\n#SBATCH --mem=10G\n#SBATCH --output=Rtest.log\n\n# load the module\nml R\n\n# run R code\nR --no-save &lt;&lt; EOF\nset.seed (1)\nm &lt;- 4000\nn &lt;- 4000\nA &lt;- matrix (runif (m*n),m,n)\nsystem.time (B &lt;- crossprod(A))\nEOF\n</code></pre> <p>You can save this script as <code>Rtest.sbatch</code> and submit it to the scheduler with:</p> <pre><code>$ sbatch Rtest.sbatch\n</code></pre> <p>Once the job is done, you should get a <code>Rtest.out</code> file in the current directory, with the following contents:</p> <pre><code>R version 3.5.1 (2018-07-02) -- \"Feather Spray\"\n[...]\n&gt; set.seed (1)\n&gt; m &lt;- 4000\n&gt; n &lt;- 4000\n&gt; A &lt;- matrix (runif (m*n),m,n)\n&gt; system.time (B &lt;- crossprod(A))\n   user  system elapsed\n  2.649   0.077   2.726\n</code></pre>"},{"location":"docs/software/using/R/#r-packages","title":"R packages","text":"<p>R comes with a single package library in <code>$R_HOME/library</code>, which contains the standard and most common packages. This is usually in a system location and is not writable by end-users.</p> <p>To accommodate individual user's requirements, R provides a way for each user to install packages in the location of their choice. The default value for a directory where users can install their own R packages is <code>$HOME/R/x86_64-pc-linux-gnu-library/&lt;R_version&gt;</code> where <code>&lt;R_version&gt;</code> depends on the R version that is used. For instance, if you have the <code>R/3.5.1</code> module loaded, the default R user library path will be <code>$HOME/R/x86_64-pc-linux-gnu-library/3.5</code>.</p> <p>This directory doesn't exist by default. The first time a user installs a package, R will ask if she wants to use the default location and create the directory.</p>"},{"location":"docs/software/using/R/#installing-packages","title":"Installing packages","text":"<p>Install R packages in a standard shell session</p> <p>Make sure to install your packages in a standard Sherlock shell session, not in an RStudio session.</p> <p>To install a R package in your personal environment, the first thing to do is load the R module:</p> <pre><code>$ ml R\n</code></pre> <p>Then start a R session, and use the <code>install.packages()</code> function at the R prompt. For instance, the following example will install the <code>doParallel</code> package, using the US mirror of the CRAN repository:</p> <pre><code>$ R\n\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\n[...]\n\n&gt; install.packages('doParallel', repos='http://cran.us.r-project.org')\n</code></pre> <p>It should give the following warning:</p> <pre><code>Warning in install.packages(\"doParallel\", repos = \"http://cran.us.r-project.org\") :\n  'lib = \"/share/software/user/open/R/3.5.1/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel)\nWould you like to create a personal library\n\u2018~/R/x86_64-pc-linux-gnu-library/3.5\u2019\nto install packages into? (yes/No/cancel) y\n</code></pre> <p>Answering <code>y</code> twice will make R create a <code>~/R/x86_64-pc-linux-gnu-library/3.5</code> directory and instruct it to install future R packages there.</p> <p>The installation will then proceed:</p> <pre><code>trying URL 'http://cran.us.r-project.org/src/contrib/doParallel_1.0.14.tar.gz'\nContent type 'application/x-gzip' length 173607 bytes (169 KB)\n==================================================\ndownloaded 169 KB\n\n* installing *source* package \u2018doParallel\u2019 ...\n** package \u2018doParallel\u2019 successfully unpacked and MD5 sums checked\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded\n* DONE (doParallel)\n\nThe downloaded source packages are in\n        \u2018/tmp/Rtmp0RHrMZ/downloaded_packages\u2019\n&gt;\n</code></pre> <p>and when it's done, you should be able to load the package within R with:</p> <pre><code>&gt; library(doParallel)\nLoading required package: foreach\nLoading required package: iterators\nLoading required package: parallel\n&gt;\n</code></pre>"},{"location":"docs/software/using/R/#installing-large-packages","title":"Installing large packages","text":"<p>Installing large R packages can sometimes be very time consuming. To speed things up, R can utilize multiple CPUs in parallel when the <code>Ncpus=n</code> option is added to the <code>install.packages()</code> command (where <code>n</code> is the number of CPUs you'd like to use).</p> <p>For instance, you can get an interactive session with 4 CPU cores with <code>sh_dev</code>:</p> <pre><code>$ sh_dev -c 4\n$ ml R\n$ R\n&gt; install.packages(\"dplyr\", repos = \"http://cran.us.r-project.org\", Ncpus=4)\n</code></pre>"},{"location":"docs/software/using/R/#alternative-installation-path","title":"Alternative installation path","text":"<p>To install R packages in a different location, you'll need to create that directory, and instruct R to install the packages there:</p> <pre><code>$ mkdir ~/R_libs/\n$ R\n&gt; install.packages('doParallel', repos='http://cran.us.r-project.org', lib=\"~/R_libs\")\n</code></pre> <p>The installation will proceed normally and the <code>doParallel</code> package will be installed in <code>$HOME/R_libs/</code>.</p> <p>Specifying the full destination path for each package installation could quickly become tiresome, so to avoid this, you can create a <code>.Renviron</code> file in your <code>$HOME</code> directory, and define your <code>R_libs</code> path there:</p> <pre><code>$ cat &lt;&lt; EOF &gt; $HOME/.Renviron\nR_LIBS=~/R_libs\nEOF\n</code></pre> <p>With this, whenever R is started, the <code>$HOME/R_libs/</code> directory will be added to the list of places R will look for packages, and you won't need to specify this installation path when using <code>install.packages()</code> anymore.</p> <p>Where does R look for packages?</p> <p>To see the directories where R searches for packages and libraries, you can use the following command in R:</p> <pre><code>&gt; .libPaths()\n</code></pre> <p>Sharing R packages</p> <p>If you'd like to share R packages within your group, you can simply define <code>$R_LIBS</code> to point to a shared directory, such as <code>$GROUP_HOME/R_libs</code> and have each user in the group use the instructions below to define it in their own environment.</p>"},{"location":"docs/software/using/R/#setting-the-installation-repository","title":"Setting the installation repository","text":"<p>When installing a package, R needs to know from which repository the package should be downloaded. If it's not specified, it will prompt for it and display a list of available CRAN mirrors.</p> <p>To avoid setting the CRAN mirror each time you run install.packages you can permanently set the mirror by creating a <code>.Rprofile</code> file in your <code>$HOME</code> directory, which R will execute each time it starts.</p> <p>For instance, adding the following contents to your <code>~/.Rprofile</code> will make sure that every <code>install.packages()</code> invocation will use the closest CRAN mirror:</p> <pre><code>## local creates a new, empty environment\n## This avoids polluting the global environment with\n## the object r\nlocal({\n  r = getOption(\"repos\")\n  r[\"CRAN\"] = \"https://cloud.r-project.org/\"\n  options(repos = r)\n})\n</code></pre> <p>Once this is set, you only need to specify the name of the package to install, and R will use the mirror you defined automatically:</p> <pre><code>&gt; install.packages(\"doParallel\")\n[...]\ntrying URL 'https://cloud.r-project.org/src/contrib/doParallel_1.0.14.tar.gz'\nContent type 'application/x-gzip' length 173607 bytes (169 KB)\n==================================================\ndownloaded 169 KB\n</code></pre>"},{"location":"docs/software/using/R/#installing-packages-from-github","title":"Installing packages from GitHub","text":"<p>R packages can be directly installed from GitHub using the <code>devtools</code> package. <code>devtools</code> needs to be installed first, with:</p> <pre><code>&gt; install.packages(\"devtools\")\n</code></pre> <p>And then, you can then install a R package directly from its GitHub repository. For instance, to install <code>dplyr</code> from tidyverse/dplyr:</p> <pre><code>&gt; library(devtools)\n&gt; install_github(\"tidyverse/dplyr\")\n</code></pre>"},{"location":"docs/software/using/R/#package-dependencies","title":"Package dependencies","text":"<p>Sometimes when installing R packages, other software is needed for the installation and/or compilation. For instance, when trying to install the <code>sf</code> package, you may encounter the following error messages:</p> <pre><code>&gt; install.packages(\"sf\")\n[...]\nConfiguration failed because libudunits2.so was not found. Try installing:...\n[...]\nconfigure: error: gdal-config not found or not executable.\n</code></pre> <p>This is because <code>sf</code> needs a few dependencies, like <code>udunits</code> and <code>gdal</code> in order to compile and install successfully. Fortunately those dependencies are already available as modules on Sherlock.</p> <p>Whenever you see \"not found\" errors, you may want to try searching the modules inventory with <code>module spider</code>:</p> <pre><code>$ module spider udunits\n\n----------------------------------------------------------------------------\n  udunits: udunits/2.2.26\n----------------------------------------------------------------------------\n    Description:\n      The UDUNITS package from Unidata is a C-based package for the\n      programmatic handling of units of physical quantities.\n\n\n    You will need to load all module(s) on any one of the lines below before\n    the \"udunits/2.2.26\" module is available to load.\n\n      physics\n</code></pre> <p>So for <code>sf</code>, as well as other geo-type libraries like <code>terra</code> and <code>raster</code>, in order to load the dependencies, exit <code>R</code>, load the <code>udunits</code> and <code>gdal</code> modules, and try installing <code>sf</code> again:</p> <pre><code>$ ml physics geos/3.13.1 udunits/2.2.26 gdal/3.10.2 proj/9.5.1\n$ ml R/4.4.2\n$ R\n&gt; install.packages(\"sf\")\n</code></pre> <p>Getting dependencies right could be a matter of trial and error. You may have to load R, install packages, search modules, load modules, install packages again and so forth. Fortunately, R packages only need to be installed once, and many R package dependencies are already available as modules on Sherlock, you just need to search for them with <code>module spider</code> and load them.</p> <p>And in case you're stuck, you can of course always send us an email and we'll be happy to assist.</p>"},{"location":"docs/software/using/R/#updating-packages","title":"Updating Packages","text":"<p>To upgrade R packages, you can use the <code>update.packages()</code> function within a R session.</p> <p>For instance, to update the <code>doParallel</code> package:</p> <pre><code>&gt; update.packages('doParallel')\n</code></pre> <p>When the package name is omitted, <code>update.packages()</code> will try to update all the packages that are installed. Which is the most efficient way to ensure that all the packages in your local R library are up to date.</p> <p>Centrally installed packages can not be updated</p> <p>Note that attempting to update centrally installed packages will fail. You will have to use <code>install.packages()</code> to install your own version of the packages in your <code>$HOME</code> directory instead.</p>"},{"location":"docs/software/using/R/#removing-packages","title":"Removing packages","text":"<p>To remove a package from your local R library, you can use the <code>remove.packages()</code> function. For instance:</p> <pre><code>&gt; remove.packages('doParallel')\n</code></pre>"},{"location":"docs/software/using/R/#examples","title":"Examples","text":""},{"location":"docs/software/using/R/#installing-devtools","title":"Installing <code>devtools</code>","text":"<p><code>devtools</code> is a package that provides R functions that simplify many common tasks. While its core functionality revolves around package development, <code>devtools</code> can also be used to install packages, particularly those on GitHub.</p> <p>Installing <code>devtools</code> is somewhat memory-intensive and has several dependencies. The following example shows how to run an interactive session with 4 CPUs, load the modules for the necessary dependencies, and install <code>devtools</code> for R version 4.4.2. Note: these dependencies will also work for installing the popular library <code>tidyverse</code>.</p> <pre><code># Launch interactive dev session with 4 CPUs\n\n$ sh_dev -c 4\n\n# Load the required modules\n\n$ ml purge\n$ ml R/4.4.2\n$ ml libgit2/1.9.1 fribidi/1.0.12 libwebp/1.3.0\n\n# Launch R and install devtools\n\n$ R\n&gt; install.packages(\"devtools\", repos = \"http://cran.us.r-project.org\", Ncpus=4)\n</code></pre>"},{"location":"docs/software/using/R/#single-node","title":"Single node","text":"<p>R has a couple of powerful and easy-to-use tools to parallelize your R jobs. <code>doParallel</code> is one of them. If the <code>doParallel</code> package is not installed in your environment yet, you can install it in a few easy steps.</p> <p>Here is a quick <code>doParallel</code> example that uses one node and 16 cores on Sherlock (more nodes or CPU cores can be requested, as needed).</p> <p>Save the two scripts below in a directory on Sherlock:</p> <code>doParallel_test.R</code><code>doParallel_test.sbatch</code> <pre><code># Example doParallel script\n\nif(!require(doParallel)) install.packages(\"doParallel\")\nlibrary(doParallel)\n\n# use the environment variable SLURM_NTASKS_PER_NODE to set\n# the number of cores to use\nregisterDoParallel(cores=(Sys.getenv(\"SLURM_NTASKS_PER_NODE\")))\n\n# bootstrap iteration example\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\niterations &lt;- 10000# Number of iterations to run\n\n# parallel loop\n# note the '%dopar%' instruction\nparallel_time &lt;- system.time({\n  r &lt;- foreach(icount(iterations), .combine=cbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})[3]\n\n# show the number of parallel workers to be used\ngetDoParWorkers()\n\n# execute the function\nparallel_time\n</code></pre> <pre><code>#!/bin/bash\n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --output=doParallel_test.log\n\n# --ntasks-per-node will be used in doParallel_test.R to specify the number\n# of cores to use on the machine.\n\n# load modules\nml R/3.5.1\n\n# execute script\nRscript doParallel_test.R\n</code></pre> <p>And then submit the job with:</p> <pre><code>$ sbatch doParallel_test.sbatch\n</code></pre> <p>Once the job has completed, the output file should contain something like this:</p> <pre><code>$ cat doParallel_test.out\n[1] \"16\"\nelapsed\n  3.551\n</code></pre> <p>Bonus points: observe the scalability of the <code>doParallel</code> loop by submitting the same script using a varying number of CPU cores:</p> <pre><code>$ for i in 2 4 8 16; do\n    sbatch --out=doP_${i}.out --ntasks-per-node=$i doParallel_test.sbatch\ndone\n</code></pre> <p>When the jobs are done:</p> <pre><code>$ for i in 2 4 8 16; do\n    printf \"%2i cores: %4.1fs\\n\" $i $(tail -n1 doP_$i.out)\ndone\n 2 cores: 13.6s\n 4 cores:  7.8s\n 8 cores:  4.9s\n16 cores:  3.6s\n</code></pre>"},{"location":"docs/software/using/R/#multiple-nodes","title":"Multiple nodes","text":"<p>To distribute parallel R tasks on multiple nodes, you can use the <code>Rmpi</code> package, which provides MPI bindings for R.</p> <p>To install the <code>Rmpi</code> package, a module providing MPI library must first be loaded. For instance:</p> <pre><code>$ ml openmpi R\n$ R\n&gt; install.packages(\"Rmpi\")\n</code></pre> <p>Once the package is installed, the following scripts demonstrate a very basic <code>Rmpi</code> example.</p> <code>Rmpi-test.R</code><code>Rmpi-test.sbatch</code> <pre><code># Example Rmpi script\n\nif (!require(\"Rmpi\")) install.packages(\"Rmpi\")\nlibrary(Rmpi)\n\n# initialize an Rmpi environment\nns &lt;- mpi.universe.size() - 1\nmpi.spawn.Rslaves(nslaves=ns, needlog=TRUE)\n\n# send these commands to the slaves\nmpi.bcast.cmd( id &lt;- mpi.comm.rank() )\nmpi.bcast.cmd( ns &lt;- mpi.comm.size() )\nmpi.bcast.cmd( host &lt;- mpi.get.processor.name() )\n\n# all slaves execute this command\nmpi.remote.exec(paste(\"I am\", id, \"of\", ns, \"running on\", host))\n\n# close down the Rmpi environment\nmpi.close.Rslaves(dellog = FALSE)\nmpi.exit()\n</code></pre> <pre><code>#!/bin/bash\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=4\n#SBATCH --output=Rmpi-test.log\n\n## load modules\n# openmpi is not loaded by default with R, so it must be loaded explicitly\nml R openmpi\n\n## run script\n# we use '-np 1' since Rmpi does its own task management\nmpirun -np 1 Rscript Rmpi-test.R\n</code></pre> <p>You can save those scripts as <code>Rmpi-test.R</code> and <code>Rmpi-test.sbatch</code> and then submit your job with:</p> <pre><code>$ sbatch Rmpi-test.sbatch\n</code></pre> <p>When the job is done, its output should look like this:</p> <pre><code>$ cat Rmpi-test.log\n        3 slaves are spawned successfully. 0 failed.\nmaster (rank 0, comm 1) of size 4 is running on: sh-06-33\nslave1 (rank 1, comm 1) of size 4 is running on: sh-06-33\nslave2 (rank 2, comm 1) of size 4 is running on: sh-06-33\nslave3 (rank 3, comm 1) of size 4 is running on: sh-06-34\n$slave1\n[1] \"I am 1 of 4 running on sh-06-33\"\n\n$slave2\n[1] \"I am 2 of 4 running on sh-06-33\"\n\n$slave3\n[1] \"I am 3 of 4 running on sh-06-34\"\n\n[1] 1\n[1] \"Detaching Rmpi. Rmpi cannot be used unless relaunching R.\"\n</code></pre>"},{"location":"docs/software/using/R/#gpus","title":"GPUs","text":"<p>Here's a quick example that compares running a matrix multiplication on a CPU and on a GPU using R. It requires submitting a job to a GPU node and the <code>gpuR</code> R package.</p> <code>gpuR-test.R</code><code>gpuR-test.sbatch</code> <pre><code># Example gpuR script\n\nif (!require(\"gpuR\")) install.packages(\"gpuR\")\nlibrary(gpuR)\n\nprint(\"CPU times\")\nfor(i in seq(1:7)) {\n    ORDER = 64*(2^i)\n    A = matrix(rnorm(ORDER^2), nrow=ORDER)\n    B = matrix(rnorm(ORDER^2), nrow=ORDER)\n    print(paste(i, sprintf(\"%5.2f\", system.time({C = A %*% B})[3])))\n}\n\nprint(\"GPU times\")\nfor(i in seq(1:7)) {\n    ORDER = 64*(2^i)\n    A = matrix(rnorm(ORDER^2), nrow=ORDER)\n    B = matrix(rnorm(ORDER^2), nrow=ORDER)\n    gpuA = gpuMatrix(A, type=\"double\")\n    gpuB = gpuMatrix(B, type=\"double\")\n    print(paste(i, sprintf(\"%5.2f\", system.time({gpuC = gpuA %*% gpuB})[3])))\n}\n</code></pre> <pre><code>#!/bin/bash\n\n#SBATCH --partition gpu\n#SBATCH --mem 8GB\n#SBATCH --gres gpu:1\n#SBATCH --output=gpuR-test.log\n\n## load modules\n# cuda is not loaded by default with R, so it must be loaded explicitly\nml R cuda\n\nRscript gpuR-test.R\n</code></pre> <p>After submitting the job with <code>sbatch gpuR-test.sbatch</code>, the output file should contain something like this:</p> <pre><code>[1] \"CPU times\"\n[1] \"1  0.00\"\n[1] \"2  0.00\"\n[1] \"3  0.02\"\n[1] \"4  0.13\"\n[1] \"5  0.97\"\n[1] \"6  7.56\"\n[1] \"7 60.47\"\n\n[1] \"GPU times\"\n[1] \"1  0.10\"\n[1] \"2  0.04\"\n[1] \"3  0.02\"\n[1] \"4  0.07\"\n[1] \"5  0.39\"\n[1] \"6  2.04\"\n[1] \"7 11.59\"\n</code></pre> <p>which shows a decent speedup for running on a GPU for the largest matrix sizes.</p>"},{"location":"docs/software/using/anaconda/","title":"Anaconda","text":""},{"location":"docs/software/using/anaconda/#introduction","title":"Introduction","text":"<p>Anaconda is a Python/R distribution that aims to simplify package management and deployment for scientific computing. Although it can have merits on individual computers, it's often counter-productive on shared HPC systems like Sherlock.</p> <p>Avoid using Anaconda on Sherlock</p> <p>We recommend NOT using Anaconda on Sherlock, and instead consider other options like virtual environments or containers.</p>"},{"location":"docs/software/using/anaconda/#why-anaconda-should-be-avoided-on-sherlock","title":"Why Anaconda should be avoided on Sherlock","text":"<p>Anaconda is widely used in several scientific domain like data science, AI/ML, bio-informatics, and is often listed in some software documentation as the recommended (if not only) way to install it</p> <p>It is a useful solution for simplifying the management of Python and scientific libraries on a personal computer. However, on highly-specialized HPC systems like Sherlock, management of these libraries and dependencies should be done by Stanford Research Computing staff, to ensure compatibility and optimal performance on the cluster hardware.</p> <p>For instance:</p> <ul> <li>Anaconda very often installs software (compilers, scientific libraries etc.)   which already exist on our Sherlock as modules, and does so in   a sub-optimal fashion, by installing sub-optimal versions and configurations,</li> <li>It installs binaries which are not optimized for the processor architectures   on Sherlock,</li> <li>it makes incorrect assumptions about the location of various system   libraries,</li> <li>Anaconda installs software in <code>$HOME</code> by default, where it writes large   amounts of files. A single Anaconda installation can easily fill up your   <code>$HOME</code> directory quota, and makes things difficult to manage,</li> <li>Anaconda installations can't easily be relocated,</li> <li>Anaconda modifies your <code>$HOME/.bashrc</code> file, which can easily cause conflicts   and slow things down when you log in.</li> </ul> <p>Worse, a <code>Conda</code> recipe can force the installation of <code>R</code> (even though it's already available on Sherlock). This installation won't perform nearly as well as the version we provide as a module (which uses optimized libraries), or not at all, the jobs launched with it may crash and end up wasting both computing resources and your time.</p> <p>Installation issues</p> <p>If you absolutely need to install <code>anaconda</code>/<code>miniconda</code>, please note that because of the large number of files that the installer will try to open, this will likely fail on a login node. So make sure to run the installation on a compute node, for instance using the <code>sh_dev</code> command.</p>"},{"location":"docs/software/using/anaconda/#what-to-do-instead","title":"What to do instead","text":""},{"location":"docs/software/using/anaconda/#use-a-virtual-environment","title":"Use a virtual environment","text":"<p>Instead of using Anaconda for your project, or when the installation instructions of the software you want to install are using it, you can use a virtual environment.</p> <p>A virtual environment offers all the functionality you need to use Python on Sherlock. You can convert Anaconda instructions and use a virtual environment instead, by following these steps:</p> <ol> <li>list the dependencies (also called requirements) of the application you want    to use:<ul> <li>check if there is a <code>requirements.txt</code> file in the Git repository or in  the software sources,</li> <li>or, check the variable <code>install_requires</code> of in the <code>setup.py</code> file, which  lists the requirements.</li> </ul> </li> <li>find which dependencies are Python modules and which are libraries provided by Anaconda. For example, <code>CUDA</code> and <code>CuDNN</code> are libraries that Anaconda can install, but which should not be re-installed as they are already available as modules on Sherlock,</li> <li>remove from the list of dependencies everything which is not a Python module (e.g. <code>cudatoolkit</code> and <code>cudnn</code>),</li> <li>create a virtual environment to install your dependencies.</li> </ol> <p>And that's it: your software should run, without Anaconda. If you have any issues, please don't hesitate to contact us.</p>"},{"location":"docs/software/using/anaconda/#use-a-container","title":"Use a container","text":"<p>In some situations, the complexity of a program's dependencies requires the use of a solution where you can control the entire software environment. In these situations, we recommend using a container.</p> <p>Tip</p> <p>Existing Docker images can easily be converted into Apptainer/Singularity images.</p> <p>The only potential downside of using containers is their size and the associated storage usage. But if your research group plans on using several container images, it could be useful to collect them all in a single location (like <code>$GROUP_HOME</code>) to avoid duplication.</p>"},{"location":"docs/software/using/clustershell/","title":"ClusterShell","text":""},{"location":"docs/software/using/clustershell/#introduction","title":"Introduction","text":"<p>ClusterShell is a command-line tool and library that helps running commands in parallel on multiple servers. It allows executing arbitrary commands across multiple hosts. On Sherlock, it provides an easy way to run commands on nodes your jobs are running on, and collect back information.  The two most useful commands provided are <code>cluset</code>, which can manipulate lists of nodenames, and <code>clush</code>, which can run commands on multiple nodes at once.</p>"},{"location":"docs/software/using/clustershell/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using ClusterShell on Sherlock. For more complete documentation about ClusterShell in general, please see the ClusterShell documentation.</p> <p>The ClusterShell library can also be directly be integrated in your Python scripts, to add a wide range of functionality. See the ClusterShell Python API documentation for reference.</p>"},{"location":"docs/software/using/clustershell/#clustershell-on-sherlock","title":"ClusterShell on Sherlock","text":"<p>ClusterShell is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml system py-clustershell\n</code></pre>"},{"location":"docs/software/using/clustershell/#cluset","title":"<code>cluset</code>","text":"<p>The <code>cluset</code> command can be used to easily manipulate lists of node names, and to expand, fold, or count them:</p> <pre><code>$ cluset --expand sh03-01n[01-06]\nsh03-01n01 sh03-01n02 sh03-01n03 sh03-01n04 sh03-01n05 sh03-01n06\n\n$ cluset --count sh03-01n[01-06]\n6\n\n$ cluset --fold sh03-01n01 sh03-01n02 sh03-01n03 sh03-01n06\nsh03-01n[01-03,06]\n</code></pre>"},{"location":"docs/software/using/clustershell/#clush","title":"<code>clush</code>","text":"<p>The <code>clush</code> command uses the same node list syntax to allow running the same commands simultaneously on those nodes. <code>clush</code> uses SSH to connect to each of these nodes.</p> <p>Warning</p> <p>You can only SSH to nodes where your jobs are running, and as a consequence, <code>clush</code> will only work on those nodes.</p> <p>For instance, to check the load on multiple compute nodes at once:</p> <pre><code>$ clush -w sh03-01n[01-03] cat /proc/loadavg\nsh03-01n01: 19.48 14.43 11.76 22/731 22897\nsh03-01n02: 13.20 13.29 13.64 14/831 1163\nsh03-01n03: 11.60 11.48 11.82 18/893 23945\n</code></pre> <p>Gathering identical output</p> <p>Using the the <code>-b</code> option will regroup similar output lines to make large outputs easier to read. By default, the output of each node will be presented separately.</p> <p>For instance, without <code>-b</code>:</p> <pre><code>$ clush -w sh03-01n[01-03] echo ok\nsh03-01n02: ok\nsh03-01n03: ok\nsh03-01n01: ok\n</code></pre> <p>With <code>-b</code>:</p> <pre><code>$ clush -bw sh03-01n[01-03] echo ok\n---------------\nsh03-01n[01-03] (3)\n---------------\nok\n</code></pre>"},{"location":"docs/software/using/clustershell/#slurm-integration","title":"Slurm integration","text":"<p>On Sherlock, ClusterShell is also tightly integrated with the job scheduler, and can directly provide information about a user's jobs and the nodes they're running on. You can use the following groups to get specific node lists:</p> group name short name action example <code>@user:</code> <code>@u:</code> list nodes where user has jobs running <code>cluset -f @user:$USER</code> <code>@job:</code> <code>@j:</code> list nodes where job is running <code>cluset -f @job:123456</code> <code>@nodestate:</code> <code>@node:</code>,<code>@n:</code> list nodes in given state <code>cluset -f @nodestate:idle</code> <code>@partition:</code> <code>@part:</code>,<code>@p:</code> list nodes in given partition <code>cluset -f @partition:gpu</code> <p>For instance, to get the list of nodes where job <code>123456</code> is running:</p> <pre><code>$ cluset -f @job:123456`\n</code></pre>"},{"location":"docs/software/using/clustershell/#examples","title":"Examples","text":""},{"location":"docs/software/using/clustershell/#job-information","title":"Job information","text":"<p>For instance, if job 1988522 from user <code>kilian</code> is running on nodes <code>sh02-01n[59-60]</code>,  <code>squeue</code> would display this:</p> <pre><code>$ squeue -u kilian\n       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n     1988522    normal interact   kilian  R       1:30      2 sh02-01n[59-60]\n     1988523    normal interact   kilian  R       1:28      2 sh02-01n[61-62]\n</code></pre> <p>With ClusterShell, you could get:</p> <ul> <li> <p>the list of node names where user <code>kilian</code> has jobs running:</p> <pre><code>$ cluset -f @user:kilian\nsh02-01n[59-62]\n</code></pre> </li> <li> <p>the nodes where job 1988522 is running, in an expanded form:</p> <pre><code>$ cluset -e @job:1988522\nsh02-01n59 sh02-01n60\n</code></pre> </li> </ul>"},{"location":"docs/software/using/clustershell/#node-states","title":"Node states","text":"<p>You can also use those binding to get lists of nodes in a particular state, in a given partition. For instance, to list the nodes that are in \"mixed\" state in the <code>dev</code> partition, you can request the intersection between the <code>@state:mixed</code> and <code>@partition:dev</code> node lists:</p> <pre><code>$ cluset -f @nodestate:mixed -i @partition:dev\nsh02-01n[57-58]\n</code></pre>"},{"location":"docs/software/using/clustershell/#local-storage","title":"Local storage","text":"<p>To get a list of files in <code>$L_SCRATCH</code> on all the nodes that are part of job <code>1988522</code>:</p> <pre><code>$ $ clush -w@j:1988522 tree $L_SCRATCH\nsh02-01n59: /lscratch/kilian\nsh02-01n59: \u251c\u2500\u2500 1988522\nsh02-01n59: \u2502\u00a0\u00a0 \u2514\u2500\u2500 foo\nsh02-01n59: \u2502\u00a0\u00a0     \u2514\u2500\u2500 bar\nsh02-01n59: \u2514\u2500\u2500 1993608\nsh02-01n59:\nsh02-01n59: 3 directories, 1 file\nsh02-01n60: /lscratch/kilian\nsh02-01n60: \u2514\u2500\u2500 1988522\nsh02-01n60:\nsh02-01n60: 1 directory, 0 files\n</code></pre>"},{"location":"docs/software/using/clustershell/#process-tree","title":"Process tree","text":"<p>To display your process tree across all the nodes your jobs are running on:</p> <pre><code>$ clush -w @u:$USER pstree -au $USER\nsh02-09n71: mpiBench\nsh02-09n71:   `-3*[{mpiBench}]\nsh02-09n71: mpiBench\nsh02-09n71:   `-3*[{mpiBench}]\nsh02-09n71: mpiBench\nsh02-09n71:   `-3*[{mpiBench}]\nsh02-09n71: mpiBench\nsh02-09n71:   `-3*[{mpiBench}]\nsh02-10n01: mpiBench\nsh02-10n01:   `-3*[{mpiBench}]\nsh02-10n01: mpiBench\nsh02-10n01:   `-3*[{mpiBench}]\nsh02-10n01: mpiBench\nsh02-10n01:   `-3*[{mpiBench}]\nsh02-10n01: mpiBench\nsh02-10n01:   `-3*[{mpiBench}]\n</code></pre>"},{"location":"docs/software/using/clustershell/#cpu-usage","title":"CPU usage","text":"<p>To get the CPU and memory usage of your processes in job <code>2003264</code>:</p> <pre><code>$ clush -w @j:2003264 ps -u$USER -o%cpu,rss,cmd\nsh03-07n12: %CPU   RSS CMD\nsh03-07n12:  0.0  4780 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-07n12:  0.0  4784 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-07n12:  0.0  4784 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-07n12:  0.0  4780 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n06: %CPU   RSS CMD\nsh03-06n06:  0.0 59596 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n06:  0.0 59576 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n06:  0.0 59580 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n06:  0.0 59588 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n05: %CPU   RSS CMD\nsh03-06n05:  0.0  7360 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n05:  0.0  7328 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n05:  0.0  7344 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n05:  0.0  7340 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n11: %CPU   RSS CMD\nsh03-06n11: 17.0 59604 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n11: 17.0 59588 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n11: 17.0 59592 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\nsh03-06n11: 17.0 59580 /home/users/kilian/benchs/MPI/mpiBench/mpiBench -i 1000000\n</code></pre>"},{"location":"docs/software/using/clustershell/#gpu-usage","title":"GPU usage","text":"<p>To show what's running on all the GPUs on the nodes associated with job <code>123456</code>:</p> <pre><code>$ clush -bw @job:123456 nvidia-smi --format=csv --query-compute-apps=process_name,utilization.memory\nsh03-12n01: /share/software/user/open/python/3.6.1/bin/python3.6, 15832 MiB\nsh02-12n04: /share/software/user/open/python/3.6.1/bin/python3.6, 15943 MiB\n</code></pre>"},{"location":"docs/software/using/julia/","title":"Julia","text":""},{"location":"docs/software/using/julia/#introduction","title":"Introduction","text":"<p>Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language. Julia aims to create an unprecedented combination of ease-of-use, power, and efficiency in a single language.</p>"},{"location":"docs/software/using/julia/#more-documentation","title":"More documentation","text":"<p>The following documentation is specifically intended for using Julia on Sherlock. For more complete documentation about Julia in general, please see the Julia documentation.</p>"},{"location":"docs/software/using/julia/#julia-on-sherlock","title":"Julia on Sherlock","text":"<p>Julia is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml julia\n</code></pre> <p>For a list of available versions, you can execute <code>ml spider julia</code> at the Sherlock prompt, or refer to the Software list page.</p>"},{"location":"docs/software/using/julia/#using-julia","title":"Using Julia","text":"<p>Once your environment is configured (ie. when the <code>julia</code> module is loaded), julia can be started by simply typing julia at the shell prompt:</p> <pre><code>$ julia\n\n_\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.0.0 (2018-08-08)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ julia --help\n\njulia [switches] -- [programfile] [args...]\n -v, --version             Display version information\n -h, --help                Print this message\n\n -J, --sysimage &lt;file&gt;     Start up with the given system image file\n -H, --home &lt;dir&gt;          Set location of `julia` executable\n --startup-file={yes|no}   Load `~/.julia/config/startup.jl`\n --handle-signals={yes|no} Enable or disable Julia's default signal handlers\n --sysimage-native-code={yes|no}\n                           Use native code from system image if available\n --compiled-modules={yes|no}\n                           Enable or disable incremental precompilation of modules\n\n -e, --eval &lt;expr&gt;         Evaluate &lt;expr&gt;\n -E, --print &lt;expr&gt;        Evaluate &lt;expr&gt; and display the result\n -L, --load &lt;file&gt;         Load &lt;file&gt; immediately on all processors\n\n -p, --procs {N|auto}      Integer value N launches N additional local worker processes\n                           \"auto\" launches as many workers as the number\n                           of local CPU threads (logical cores)\n --machine-file &lt;file&gt;     Run processes on hosts listed in &lt;file&gt;\n\n -i                        Interactive mode; REPL runs and isinteractive() is true\n -q, --quiet               Quiet startup: no banner, suppress REPL warnings\n</code></pre>"},{"location":"docs/software/using/julia/#running-a-julia-script","title":"Running a Julia script","text":"<p>A Julia program is easy to run on the command line outside of its interactive mode.</p> <p>Here is an example where we create a simple Hello World program and launch it with Julia</p> <pre><code>$ echo 'println(\"hello world\")' &gt; helloworld.jl\n</code></pre> <p>That script can now simply be executed by calling <code>julia &lt;script_name&gt;</code>:</p> <pre><code>$ julia helloworld.jl\nhello world\n</code></pre>"},{"location":"docs/software/using/julia/#submitting-a-julia-job","title":"Submitting a Julia job","text":"<p>Here's an example Julia sbatch script that can be submitted via <code>sbatch</code>:</p> julia_test.sbatch <pre><code>#!/bin/bash\n\n#SBATCH --time=00:10:00\n#SBATCH --mem=4G\n#SBATCH --output=julia_test.log\n\n# load the module\nml julia\n\n# run the Julia application\njulia helloworld.jl\n</code></pre> <p>You can save this script as <code>julia_test.sbatch</code> and submit it to the scheduler with:</p> <pre><code>$ sbatch julia_test.sbatch\n</code></pre> <p>Once the job is done, you should get a <code>julia_test.log</code> file in the current directory, with the following contents:</p> <pre><code>$ cat julia_test.log\nhello world\n</code></pre>"},{"location":"docs/software/using/julia/#julia-packages","title":"Julia packages","text":"<p>Julia provides an ever-growing list of packages that can be used to install add-on functionality to your Julia code.</p> <p>Installing packages with Julia is very simple. Julia includes a package module in its base installation that handles installing, updating, and removing packages.</p> <p>First import the <code>Pkg</code> module:</p> <pre><code>julia&gt; import Pkg\njulia&gt; Pkg.status()\n    Status `~/.julia/environments/v1.0/Project.toml`\n</code></pre> <p>Julia packages only need to be installed once</p> <p>You only need to install Julia packages once on Sherlock. Since fielsystems are shared, packages installed on one node will immediately be available on all nodes on the cluster.</p>"},{"location":"docs/software/using/julia/#installing-packages","title":"Installing packages","text":"<p>You can first check the status of packages installed on Julia using the status function of the <code>Pkg</code> module:</p> <pre><code>julia&gt; Pkg.status()\nNo packages installed.\n</code></pre> <p>You can then add packages using the add function of the <code>Pkg</code> module:</p> <pre><code>julia&gt; Pkg.add(\"Distributions\")\nINFO: Cloning cache of Distributions from git://github.com/JuliaStats/Distributions.jl.git\nINFO: Cloning cache of NumericExtensions from git://github.com/lindahua/NumericExtensions.jl.git\nINFO: Cloning cache of Stats from git://github.com/JuliaStats/Stats.jl.git\nINFO: Installing Distributions v0.2.7\nINFO: Installing NumericExtensions v0.2.17\nINFO: Installing Stats v0.2.6\nINFO: REQUIRE updated.\n</code></pre> <p>Using the status function again, you can see that the package and its dependencies have been installed:</p> <pre><code>julia&gt; Pkg.status()\nRequired packages:\n - Distributions                 0.2.7\nAdditional packages:\n - NumericExtensions             0.2.17\n - Stats                         0.2.6\n</code></pre>"},{"location":"docs/software/using/julia/#updating-packages","title":"Updating Packages","text":"<p>The update function of the <code>Pkg</code> module can update all packages installed:</p> <pre><code>julia&gt; Pkg.update()\nINFO: Updating METADATA...\nINFO: Computing changes...\nINFO: Upgrading Distributions: v0.2.8 =&gt; v0.2.10\nINFO: Upgrading Stats: v0.2.7 =&gt; v0.2.8\n</code></pre>"},{"location":"docs/software/using/julia/#removing-packages","title":"Removing packages","text":"<p>The remove function of the <code>Pkg</code> module can remove any packages installed as well:</p> <pre><code>julia&gt; Pkg.rm(\"Distributions\")\nINFO: Removing Distributions v0.2.7\nINFO: Removing Stats v0.2.6\nINFO: Removing NumericExtensions v0.2.17\nINFO: REQUIRE updated.\n\njulia&gt; Pkg.status()\nRequired packages:\n - SHA                           0.3.2\n\njulia&gt; Pkg.rm(\"SHA\")\nINFO: Removing SHA v0.3.2\nINFO: REQUIRE updated.\n\njulia&gt; Pkg.status()\nNo packages installed.\n</code></pre>"},{"location":"docs/software/using/julia/#examples","title":"Examples","text":""},{"location":"docs/software/using/julia/#parallel-job","title":"Parallel job","text":"<p>Julia can natively spawn parallel workers across multiple compute nodes, without using MPI. There are two main modes of operation:</p> <ol> <li> <p>ClusterManager: in this mode, you can spawn workers from within the Julia    interpreter, and each worker will actually submit jobs to the scheduler,    executing instructions within those jobs.</p> </li> <li> <p>using the <code>--machine-file</code> option: here, you submit a SLURM job and run the    Julia interpreter in parallel mode within the job's resources.</p> </li> </ol> <p>The second mode is easier to use, and more convenient, since you have all your resources available and ready to use when the job starts. In mode 1, you'll need to wait for jobs to be dispatched and executed inside Julia.</p> <p>Here is a quick example on how to use the <code>--machine-file</code> option on Sherlock.</p> <p>Given following Julia script (<code>julia_parallel_test.jl</code>) that will print a line with the process id and the node it's executing on, in parallel:</p> julia_parallel_test.jl <pre><code>using Distributed\n@everywhere println(\"process: $(myid()) on host $(gethostname())\")\n</code></pre> <p>You can submit the following job:</p> julia_test.sbatch <pre><code>#!/bin/bash\n#SBATCH --nodes 2\n#SBATCH --ntasks-per-node 4\n#SBATCH --time 5:0\n\nml julia\njulia --machine-file &lt;(srun hostname -s)  ./julia_parallel_test.jl\n</code></pre> <p>Save as <code>julia_test.sbatch</code>, and then:</p> <pre><code>$ sbatch  julia_test.sbatch\n</code></pre> <p>It will:</p> <ol> <li>Request 2 nodes, 4 tasks per node (8 tasks total)</li> <li>load the <code>julia</code> module</li> <li>Run Julia in parallel with a machine file that is automatically generated,      listing the nodes that are assigned to your job.</li> </ol> <p>It should output something like this in your job's output file:</p> <pre><code>process: 1 on host sh-06-33.int\n      From worker 2:    process: 2 on host sh-06-33.int\n      From worker 3:    process: 3 on host sh-06-34.int\n      From worker 5:    process: 5 on host sh-06-33.int\n      From worker 4:    process: 4 on host sh-06-33.int\n      From worker 6:    process: 6 on host sh-06-33.int\n      From worker 8:    process: 8 on host sh-06-34.int\n      From worker 9:    process: 9 on host sh-06-34.int\n      From worker 7:    process: 7 on host sh-06-34.int\n</code></pre>"},{"location":"docs/software/using/mariadb/","title":"MariaDB","text":""},{"location":"docs/software/using/mariadb/#introduction","title":"Introduction","text":"<p>MariaDB is a community-developed fork of the MySQL relational database management system. It is completely compatible with MySQL and could be use as a drop-in replacement in the vast majority of cases.</p>"},{"location":"docs/software/using/mariadb/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using MariaDB on Sherlock. For more complete documentation about MariaDB in general, please see the MariaDB documentation.</p>"},{"location":"docs/software/using/mariadb/#mariadb-on-sherlock","title":"MariaDB on Sherlock","text":"<p>We don't provide any centralized database service on Sherlock, but we provide a centralized installation of MariaDB, and each user is welcome to start their own instance of the database server to fit their jobs' needs.</p> <p>The overall process to run an instance of MariaDB on Sherlock would look like this:</p> <ol> <li>configure and initialize your environment so you can start a database    instance under your user account,</li> <li>start the database server,</li> <li>run SQL queries from the same node (via a local socket), or from other nodes    and/or jobs (via the network).</li> </ol>"},{"location":"docs/software/using/mariadb/#single-node-access","title":"Single-node access","text":"<p>In that example, the database server and client will run within the same job, on the same compute node.</p>"},{"location":"docs/software/using/mariadb/#preparation","title":"Preparation","text":"<p>You first need to let MariaDB know where to store its database, where to log things, and how to allow connections from clients. The commands below only need to be executed once.</p> <p>For this, you'll need to create a <code>.my.cnf</code> file in your home directory. Assuming you'll want to store your database files in a <code>db/</code> directory in your <code>$SCRATCH</code> folder, you can run the following commands:</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ cat &lt;&lt; EOF &gt; ~/.my.cnf\n[mysqld]\ndatadir=$DB_DIR\nsocket=$DB_DIR/mariadb.sock\nuser=$USER\nsymbolic-links=0\nskip-networking\n\n[mysqld_safe]\nlog-error=$DB_DIR/mariadbd.log\npid-file=$DB_DIR/mariadbd.pid\n\n[mysql]\nsocket=$DB_DIR/mariadb.sock\nEOF\n</code></pre> <p><code>.my.cnf</code> doesn't support environment variables</p> <p>Please note that if you edit your <code>~/.my.cnf</code> file directly in a file editor, without using the HEREDOC syntax above, environment variables such as <code>$DB_DIR</code>, <code>$HOME</code> or <code>$USER</code> won't work: you will need to specify absolute paths explicitly, such as <code>/scratch/users/kilian/db/mariadbd.log</code>.</p> <p>If you use the HEREDOC syntax, you can verify that the resulting <code>.my.cnf</code> file does actually contain full paths, and not environment variables anymore.</p> <p>Once you have the <code>.my.cnf</code> file in place, you need to initialize your database with some internal data that MariaDB needs. In the same terminal, run the following commands:</p> <pre><code>$ ml system mariadb\n$ $MARIADB_DIR/scripts/mysql_install_db --basedir=$MARIADB_DIR  --datadir=$DB_DIR\n</code></pre>"},{"location":"docs/software/using/mariadb/#start-the-server","title":"Start the server","text":"<p>You can now start the MariaDB server. For this, first get an allocation on a compute node, note the hostname of the compute node your job has been allocated, load the <code>mariadb</code> module, and then run the <code>mysqld_safe</code> process:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system mariadb\n$ mysqld_safe\n180705 18:14:27 mysqld_safe Logging to '/home/users/kilian/db/mysqld.log'.\n180705 18:14:28 mysqld_safe Starting mysqld daemon with databases from /home/users/kilian/db/\n</code></pre> <p>The <code>mysqld_safe</code> will be blocking, meaning it will not give the prompt back for as long as the MariaDB server runs.</p> <p>If it does return on its own, it probably means that something went wrong, and you'll find more information about the issue in the <code>$DB_DIR/mysqld.log</code> file you defined in <code>~/.my.cnf</code>.</p>"},{"location":"docs/software/using/mariadb/#run-queries","title":"Run queries","text":"<p>You're now ready to run queries against that MariaDB instance, from the same node your job is running on.</p> <p>From another terminal on Sherlock, connect to your job's compute node (here, it's <code>sh-01-01</code>, as shown above), load the <code>mariadb</code> module, and then run the <code>mysql</code> command: it will open the MariaDB shell, ready to run your SQL queries:</p> <pre><code>$ ssh sh-01-01\n$ ml system mariadb\n$ mysql\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 8\nServer version: 10.2.11-MariaDB Source distribution\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt;\n</code></pre> <p>Once you're done with your MariaDB instance, you can just terminate your job, and all the processes will be terminated automatically.</p>"},{"location":"docs/software/using/mariadb/#multi-node-access","title":"Multi-node access","text":"<p>In case you need to run a more persistent instance of MariaDB, you can for instance submit a dedicated job to run the server, make it accessible over the network, and run queries from other jobs and/or nodes.</p>"},{"location":"docs/software/using/mariadb/#enable-network-access","title":"Enable network access","text":"<p>The preparation steps are pretty similar to the single-node case, except the MariaDB server instance will be accessed over the network rather than through a local socket.</p> <p>Network access must be secured</p> <p>When running an networked instance of MariaDB, please keep in mind that any user on Sherlock will be able to connect to the TCP ports that <code>mysqld</code> runs on, and that proper configuration must be done to prevent unauthrozied access.</p> <p>Like in the single-node case, you need to create a <code>~/.my.cnf</code> file, but without the <code>skip-networking</code> directive.</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ cat &lt;&lt; EOF &gt; ~/.my.cnf\n[mysqld]\ndatadir=$DB_DIR\nsocket=$DB_DIR/mariadb.sock\nuser=$USER\nsymbolic-links=0\n\n[mysqld_safe]\nlog-error=$DB_DIR/mariadbd.log\npid-file=$DB_DIR/mariadbd.pid\n\n[mysql]\nsocket=$DB_DIR/mariadb.sock\nEOF\n</code></pre> <p>And then initiate the database:</p> <pre><code>$ ml system mariadb\n$ $MARIADB_DIR/scripts/mysql_install_db --basedir=$MARIADB_DIR  --datadir=$DB_DIR\n</code></pre>"},{"location":"docs/software/using/mariadb/#secure-access","title":"Secure access","text":"<p>We will now set a password for the MariaDB <code>root</code> user to a random string, just for the purpose of preventing unauthorized access, since we won't need it for anything.</p> <p>We will actually create a MariaDB user with all privileges on the databases, that will be able to connect to this instance from any node. This user will need a real password, though. So please make sure to replace the <code>my-secure-password</code> string below by the actual password of your choice.</p> <p>Choose a proper password</p> <p>This password will only be used to access this specific instance of MariaDB. Note that anybody knowing that password will be allowed to connect to your MariaDB instances and modify data in the tables.</p> <ul> <li>do NOT literally use <code>my-secure-password</code></li> <li>do NOT use your SUNet ID password</li> </ul> <p>Once you've chosen your password, you can start the <code>mysqld</code> process on a compute node, like before:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system mariadb\n$ mysqld_safe\n</code></pre> <p>And then, from another terminal, run the following commands to secure access to your MariaDB database.</p> <pre><code>$ ssh sh-01-01\n$ mysql -u root &lt;&lt; EOF\nUPDATE mysql.user SET Password=PASSWORD(RAND()) WHERE User='root';\nDELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');\nDELETE FROM mysql.user WHERE User='';\nDELETE FROM mysql.db WHERE Db='test' OR Db='test_%';\nGRANT ALL PRIVILEGES ON *.* TO '$USER'@'%' IDENTIFIED BY 'my-secure-password' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\nEOF\n</code></pre> <p>Once you've done that, you're ready to terminate that interactive job, and start a dedicated MariaDB server job.</p>"},{"location":"docs/software/using/mariadb/#start-mariadb-in-a-job","title":"Start MariaDB in a job","text":"<p>You can use the following <code>mariadb.sbatch</code> job as a template:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=mariadb\n#SBATCH --time=8:0:0\n#SBATCH --dependency=singleton\n\nml system mariadb\nmysqld_safe\n</code></pre> <p>and submit it with:</p> <pre><code>$ sbatch mariadb.sbatch\n</code></pre> <p>Concurrent instances will lead to data corruption</p> <p>An important thing to keep in mind is that having multiple instances of a MariaDB server running at the same time, using the same database files, will certainly lead to catastrophic situations and the corruption of those files.</p> <p>To prevent this from happening, the <code>--dependency=singleton</code> job submission option will make sure that only one instance of that job (based on its name and user) will run at any given time.</p>"},{"location":"docs/software/using/mariadb/#connect-to-the-running-instance","title":"Connect to the running instance","text":"<p>Now, from any node on Sherlock, whether from a login node, an interactive job, or a batch job, using the <code>mysql</code> CLI or any application binding in any language, you should be able to connect to your running MariaDB instance,</p> <p>First, identify the node your job is running on with <code>squeue</code>:</p> <pre><code>$ squeue -u $USER -n mariadb\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          21383445    normal  mariadb   kilian  R       0:07      1 sh-01-02\n</code></pre> <p>and then, point your MariaDB client to that node:</p> <pre><code>$ ml system mariadb\n$ mysql -h sh-01-02 -p\nEnter password:\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 15\nServer version: 10.2.11-MariaDB Source distribution\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt;\n</code></pre> <p>That's it! You can now run SQL queries from anywhere on Sherlock to your own MariaDB instance.</p>"},{"location":"docs/software/using/mariadb/#persistent-db-instances","title":"Persistent DB instances","text":"<p>SQL data is persistent</p> <p>All the data you import in your SQL databases will be persistent across jobs. Meaning that you can run a PostgreSQL server job for the day, import data in its database, stop the job, and resubmit the same PostgreSQL server job the next day: all your data will still be there as long as the location you've chosen for your database (the <code>$DB_DIR</code> defined in the Preparation steps) is on a persistent storage location.</p> <p>If you need database access for more than the maximum runtime of a job, you can use the instructions provided to define self-resubmitting recurring jobs and submit long-running database instances.</p>"},{"location":"docs/software/using/matlab/","title":"Matlab","text":""},{"location":"docs/software/using/matlab/#introduction","title":"Introduction","text":"<p>MATLAB is a numerical computing environment and proprietary programming language developed by MathWorks.</p>"},{"location":"docs/software/using/matlab/#more-documentation","title":"More documentation","text":"<p>The following documentation is specifically intended for using Matlab on Sherlock. For more complete documentation about Matlab in general, please see the official MATLAB documentation.</p>"},{"location":"docs/software/using/matlab/#matlab-on-sherlock","title":"MATLAB on Sherlock","text":""},{"location":"docs/software/using/matlab/#licensing","title":"Licensing","text":"<p>MATLAB is a commercial software suite, which is now available to no cost for all Stanford Faculty, students, and staff.</p> <p>Note: a number of free, open-source alternatives exist and can be used in many situations: Octave, R, Julia, or Python are all available on Sherlock, and can often replace MATLAB with good results.</p>"},{"location":"docs/software/using/matlab/#using-matlab","title":"Using MATLAB","text":"<p>The MATLAB module can be loaded with:</p> <pre><code>$ ml load matlab\n</code></pre> <p>This will load the current default version. For a list of available versions run <code>ml spider matlab</code> at the Sherlock prompt, or refer to the Software list page.</p> <p>MATLAB can't run on login nodes</p> <p>Running MATLAB directly on login nodes is not supported and will produce the following message: <pre><code>-----------------------------------------------------------------------\nWARNING: running MATLAB directly on login nodes is not supported.  Please\nmake sure you request an interactive session on a compute node with \"sh_dev\"\nfor instance) before launching MATLAB interactively.\n-----------------------------------------------------------------------\n</code></pre> You will need to submit a job or request an interactive session on a compute node before you can start MATLAB.</p> <p>Once you are on a compute node and your environment is configured (ie. when the <code>matlab</code> module is loaded), MATLAB can be started by simply typing <code>matlab</code> at the shell prompt.</p> <pre><code>$ sh_dev\n$ ml load matlab\n$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\n                          &lt; M A T L A B (R) &gt;\n                Copyright 1984-2019 The MathWorks, Inc.\n                R2019a (9.6.0.1072779) 64-bit (glnxa64)\n                             March 8, 2019\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\n&gt;&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ matlab -help\n</code></pre>"},{"location":"docs/software/using/matlab/#running-a-matlab-script","title":"Running a MATLAB script","text":"<p>There are several ways to launch a MATLAB script on the command line, as documented in the MATLAB documentation:</p> Method Output <code>matlab -nodesktop &lt; script.m</code> MATLAB will run the code from <code>script.m</code> and display output on <code>stdout</code> <code>matlab -nodisplay</code> Start MATLAB in CLI mode, without its graphical desktop environment <code>matlab -nojvm</code> do not start the JVM<sup>1</sup>"},{"location":"docs/software/using/matlab/#matlab-gui","title":"MATLAB GUI","text":"<p>It's often best to use your laptop or desktop to develop, debug MATLAB and visualize the output. If you do need to use the MATLAB GUI on a large cluster like Sherlock, you will need to enable X11 forwarding in your SSH client.</p> <p>For instance:</p> <pre><code>$ ssh -X &lt;YourSUNetID&gt;@login.sherlock.stanford.edu\n</code></pre> <p>And then, once on Sherlock:</p> <pre><code>$ sh_dev\n$ ml load matlab\n$ matlab\n</code></pre> <p>For more info on X11 forwarding, you can refer to this UIT page.</p>"},{"location":"docs/software/using/matlab/#examples","title":"Examples","text":""},{"location":"docs/software/using/matlab/#simple-matlab-job","title":"Simple MATLAB job","text":"<p>Here is an example MATLAB batch script that can submitted with <code>sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=matlab_test\n#SBATCH --output=matlab_test.\"%j\".out\n#SBATCH --error=matlab_test.\"%j\".err\n#SBATCH --partition=normal\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --mail-type=ALL\n\nmodule load matlab\nmatlab -nodisplay &lt; example.m\n</code></pre> <p>This simple job, named <code>matlab_test</code> will run a MATLAB script named <code>example.m</code> in the <code>normal</code> partition, for a duration of 10 minutes, and use 1 CPU and 8GB of RAM.  It will send you an email (to whatever email you used wen you signed up for Sherlock) when it begins, ends or fails.</p> <p>Additionally, to aid in debugging, it will log any errors and output to the files <code>matlab_test.JOBID.{out,err}</code> with the jobid appended to the filename (<code>%j</code>).</p> <p>To create the script, open a text editor on Sherlock, copy the contents of the script, and save it as <code>matlab_test.sbatch</code></p> <p>Then, submit the job with the <code>sbatch</code> command:</p> <pre><code>$ sbatch matlab_test.sbatch\nSubmitted batch job 59942277\n</code></pre> <p>You can check the status of the job with the <code>squeue</code> command, and check the contents of the <code>matlab_test.JOBID.{out,err}</code> files to see the results.</p>"},{"location":"docs/software/using/matlab/#parallel-loop","title":"Parallel loop","text":"<p>You can run your MATLAB code across multiple CPUs on Sherlock using <code>parfor</code> loops, to take advantage of the multiple CPU cores that each node features. You can submit a job requesting as many CPUs as there are on a node in a single job.  The key is to grab the SLURM environment variable <code>$SLURM_CPUS_PER_TASK</code> and create the worker pool in your MATLAB code with:</p> <pre><code>parpool('local', str2num(getenv('SLURM_CPUS_PER_TASK')))\n</code></pre> <p>Here is an example of a <code>sbatch</code> submission script that requests 16 CPUs on a node, and runs a simple MATLAB script using <code>parfor</code>.</p> <p>Save the two scripts below as <code>parfor.sbatch</code> and <code>parfor_loop.m</code>:</p> parfor.sbatchparfor_loop.m <pre><code>#!/bin/bash\n#SBATCH -J pfor_matlab\n#SBATCH -o pfor\".%j\".out\n#SBATCH -e pfor\".%j\".err\n#SBATCH -t 20:00\n#SBATCH -p normal\n#SBATCH -c 16\n#SBATCH --mail-type=ALL\n\nmodule load matlab\nmatlab -batch parfor_loop\n</code></pre> <pre><code>%============================================================================\n% Parallel Monte Carlo calculation of PI\n%============================================================================\nparpool('local', str2num(getenv('SLURM_CPUS_PER_TASK')))\nR = 1;\ndarts = 1e7;\ncount = 0;\ntic\nparfor i = 1:darts\n   % Compute the X and Y coordinates of where the dart hit the...............\n   % square using Uniform distribution.......................................\n   x = R*rand(1);\n   y = R*rand(1);\n   if x^2 + y^2 &lt;= R^2\n      % Increment the count of darts that fell inside of the.................\n      % circle...............................................................\n     count = count + 1; % Count is a reduction variable.\n   end\nend\n% Compute pi.................................................................\nmyPI = 4*count/darts;\nT = toc;\nfprintf('The computed value of pi is %8.7f.n',myPI);\nfprintf('The parallel Monte-Carlo method is executed in %8.2f seconds.n', T);\ndelete(gcp);\nexit;\n</code></pre> <p>You can now submit the job with the following command:</p> <pre><code>sbatch parfor.sbatch\n</code></pre> <p>If you run <code>htop</code> or <code>pstree -u $USER</code> on the compute node that is running your job, you will see all 16 cores allocated to your MATLAB code.</p> <p>You can also try that same job with different numbers of CPUs, and see how well it scales.</p> <ol> <li> <p>MATLAB uses the Java\u00ae Virtual Machine (JVM\u2122) software to run the   desktop and to display graphics. The <code>-nojvm</code> option enables you to start   MATLAB without the JVM. Using this option minimizes memory usage and improves   initial start-up speed, but restricts functionality.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/using/ollama/","title":"Ollama","text":""},{"location":"docs/software/using/ollama/#introduction","title":"Introduction","text":"<p>Ollama provides a streamlined way to start using Large Language Models (LLMs). It allows users to easily swap and retrain popular models such as Llama, Gemma, Qwen, and Mistral on their local systems, enhancing the security of research data. For convenience, Sherlock provides a module that includes many of these models for seamless loading. Alternatively, users can pull and retrain models themselves for greater flexibility.</p>"},{"location":"docs/software/using/ollama/#more-documentation","title":"More documentation","text":"<p>The following documentation is specifically intended for using Ollama on Sherlock. For more complete documentation about Ollama in general, please see the Ollama documentation.</p>"},{"location":"docs/software/using/ollama/#ollama-on-sherlock","title":"Ollama on Sherlock","text":"<p>Ollama is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml ollama\n</code></pre> <p>For a list of available versions, you can execute <code>ml spider ollama</code> at the Sherlock prompt, or refer to the Software list page.</p>"},{"location":"docs/software/using/ollama/#using-ollama","title":"Using Ollama","text":""},{"location":"docs/software/using/ollama/#requesting-a-gpu","title":"Requesting a GPU","text":"<p>Although not strictly required, Ollama runs best on GPUs, so you will likely need to request a GPU to support inference.</p> <p>For starters and basic testing, you can get a quick allocation on a lightweight GPU instance with:</p> <pre><code>$ sh_dev -g 1\n</code></pre> <p>For more intensive work or larger models, you can request a more robust allocation. For example:</p> <pre><code>$ salloc -p gpu --cpus-per-task 4 -G 1 -C GPU_MEM:32GB\n</code></pre> <p>should be more than enough to handle all but the highest parameter models, though it may be a bit overkill for very low parameter models. You can tweak the allocation as needed.</p>"},{"location":"docs/software/using/ollama/#starting-the-ollama-server","title":"Starting the Ollama server","text":"<p>Once you have a GPU, you need to start an Ollama server:</p> <pre><code>$ ollama serve\n</code></pre> <p>This will start the server in the foreground. If you want to run it in the background, you can use:</p> <pre><code>$ ollama serve &amp;\n</code></pre> <p>or use a terminal multiplexer such as <code>tmux</code> or <code>screen</code>.</p>"},{"location":"docs/software/using/ollama/#using-models","title":"Using models","text":"<p>You'll then need to pull a model or use one of the models available on Sherlock. To do this, you need to open a new terminal, load the Ollama module again, and connect to the same GPU allocation you started the server on.</p> <p>To do this, from another terminal, get the name of the node you started the server on with <code>squeue</code>, and then connect to that node with <code>ssh</code>. Once connected, load the Ollama module again with <code>ml ollama</code>.</p> <p>You can now pull a model from the Ollama model library:</p> <pre><code>$ ollama pull &lt;model name&gt;\n</code></pre> <p>The model will download (or, if it's available locally in the Sherlock model library, it will be synchronized to your own cache location). Now you can run the model with:</p> <pre><code>$ ollama run &lt;model name&gt;\n</code></pre> <p>Once the model is running, you can query at you would any other LLM.</p> <p>Here's an example of running the <code>mistral:7b</code> model:</p> <pre><code>$ ollama pull mistral:7b\nmodel mistral:7b found in on-prem cache, syncing...\n[...]\nsuccess\n$ ollama run mistral:7b\n&gt; In one word, what is the capital of France?\nParis.\n&gt; exit\n</code></pre>"},{"location":"docs/software/using/ollama/#model-management","title":"Model management","text":"<p>There are several commands for managing the models available to you on Sherlock:</p> Command Description <code>ollama list</code> List your currently downloaded models. <code>ollama ps</code> List currently loaded models <code>ollama stop &lt;model name&gt;</code> Stop a running model <code>ollama show &lt;model name&gt;</code> Show model info <code>ollama rm &lt;model name&gt;</code> Remove a model"},{"location":"docs/software/using/ollama/#ollama-server-as-a-batch-job","title":"Ollama server as a batch job","text":""},{"location":"docs/software/using/ollama/#running-ollama-server-in-a-job","title":"Running Ollama server in a job","text":"<p>You can also run the Ollama server in a batch job, and use it from other jobs, or as a coding assistant for your development / interactive sessions.</p> <p>Here's an example Slurm script that requests a GPU and starts the Ollama server:</p> ollama_server.sh<pre><code>#!/bin/bash\n#SBATCH --job-name      ollama_server-%j\n#SBATCH --output        ollama_server-%j.out\n#SBATCH --error         ollama_server-%j.err\n#SBATCH --partition     gpu\n#SBATCH --gpus          1\n#SBATCH --constraints   [GPU_MEM:16GB|GPU_MEM:24GB|GPU_MEM:32GB|GPU_MEM:48GB|GPU_MEM:80GB]\n#SBATCH --cpus-per-task 4\n#SBATCH --time          02:00:00\n\n\n# Load the Ollama module\nml ollama\n\n# choose a random port for the Ollama server (&gt;1024)\nOLLAMA_PORT=$(( RANDOM % 60000 + 1024 ))\nwhile (echo &gt; /dev/tcp/localhost/$OLLAMA_PORT) &amp;&gt;/dev/null; do\n    OLLAMA_PORT=$(( ( RANDOM % 60000 )  + 1024 ))\ndone\n\n# Start the Ollama server\necho \"-----------------------------------------------------------------\"\necho \"Starting Ollama server on host $SLURM_NODELIST, port $OLLAMA_PORT\"\necho \"use OLLAMA_HOST=$SLURM_NODELIST:$OLLAMA_PORT to connect\"\necho \"-----------------------------------------------------------------\"\necho\n\nOLLAMA_HOST=0.0.0.0:$OLLAMA_PORT ollama serve\n</code></pre> <p>You can submit this script with <code>sbatch</code>:</p> <pre><code>$ sbatch ollama_server.sh\nSubmitted batch job 6515470\n</code></pre> <p>The job requests 1 GPU with at least 16GB of GPU memory, for 2 hours.</p> <p>Once the job has started, you can check the output file <code>ollama_server-{jobid}.out</code> to find the server endpoint.</p> <pre><code>$ cat ollama_server-6515470.out\n-----------------------------------------------------------------\nStarting Ollama server on host sh03-16n12, port 18137\nuse OLLAMA_HOST=sh03-16n12:18137 to connect\n-----------------------------------------------------------------\n[...]\n</code></pre>"},{"location":"docs/software/using/ollama/#connecting-to-the-server","title":"Connecting to the server","text":"<p>On another compute node (e.g. from another job or an interactive session), you can connect to the server using the <code>OLLAMA_HOST</code> environment variable, indicated by the output of the \"ollama_server\" server job.</p> <p>Open a new terminal, request an interactive session or connect to another job on Sherlock, and run:</p> <pre><code>$ ml ollama\n$ OLLAMA_HOST=sh03-16n12:18137 ollama run mistral:7b\n&gt; In one word, what is the capital of France?\nParis.\n&gt; exit\n</code></pre> <p>Info</p> <p>The Ollama server running in the Slurm job is capable of serving multiple clients at the same time, and it's possible to use the same server endpoint from multiple clients, as long as they can access the compute nodes (potentially through a SSH tunnel).</p>"},{"location":"docs/software/using/ollama/#use-your-own-coding-assistant-in-code-server","title":"Use your own coding assistant in <code>code-server</code>","text":"<p>You can also use the Ollama server as a coding assistant in <code>code-server</code>, the VS Code interactive application available on Sherlock.</p> <p>In the IDE, you can install the Continue extension, to connect to your local Ollama server.</p> <p>After installation, the Continue icon should be visible in the left-hand side panel, or via the Ctrl+L key-binding. Before it could be used, the extension needs to be configured to use the Ollama server.</p> <p>Here's an example configuration snippet, assuming the Ollama server is running on <code>sh03-16n12:18137</code>:</p> <pre><code>models:\n  - name: Autodetect\n    apiBase: http://sh03-16n12:18137\n    provider: ollama\n    model: AUTODETECT\n</code></pre> <p>After that, in the Continue panel, you can start chatting with the model.</p> <p>More details and examples are also available in the Ollama documentation.</p>"},{"location":"docs/software/using/ollama/#advanced-usage","title":"Advanced usage","text":"<p>You can do things like customize a model with Ollama. Refer to \"Customize a model\" in their documentation for instructions.</p>"},{"location":"docs/software/using/perl/","title":"Perl","text":""},{"location":"docs/software/using/perl/#introduction","title":"Introduction","text":"<p>Perl is a high-level, general-purpose, interpreted, dynamic programming language. Originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier, it has since undergone many changes and revisions.</p> <p>Perl provides a framework allowing users to easily extend the language by installing new modules in their local environment. The Comprehensive Perl Archive Network (CPAN<sup>1</sup>) is an archive of over 25,000 distributions of software written in Perl, as well as documentation for it. It is searchable at http://metacpan.org or http://search.cpan.org and mirrored in over 270 locations around the world.</p>"},{"location":"docs/software/using/perl/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Perl on Sherlock. For more complete documentation about Perl in general, please see the Perl documentation.</p>"},{"location":"docs/software/using/perl/#perl-modules-on-sherlock","title":"Perl modules on Sherlock","text":"<p>To install Perl modules from CPAN, we recommend using the (provided) <code>App::cpanminus</code> module and <code>local::lib</code> modules:</p> <ul> <li><code>App::cpanminus</code> is a popular alternative CPAN client that can be used to   manage Perl distributions. It has many great features, including uninstalling   modules.</li> <li><code>local::lib</code> allows users to install Perl modules in the directory of their   choice (typically their home directory) without administrative privileges.</li> </ul> <p>Both are already installed on Sherlock, and are automatically enabled and configured when you load the <code>perl</code> module. You don't need to add anything in your <code>~/.bashrc</code> file, the Sherlock <code>perl</code> module will automatically create everything that is required so you can directly run a command to install Perl modules locally.</p>"},{"location":"docs/software/using/perl/#installation","title":"Installation","text":"<p>Perl modules installation is only necessary once</p> <p>You only need to install Perl modules once on Sherlock. Since fielsystems are shared, modules installed on one node will immediately be available on all nodes on the cluster.</p> <p>As an example, to install the <code>DateTime::TimeZone</code> module, you can do the following:</p> <pre><code>$ ml perl\n$ cpanm DateTime::TimeZone\n</code></pre>"},{"location":"docs/software/using/perl/#usage","title":"Usage","text":"<p>Once installed, you can use the Perl modules directly, no specific options or syntax is required.</p> <p>For instance, to check that the <code>DateTime::TimeZone</code> module is correctly installed:</p> <pre><code>$ perl -MDateTime::TimeZone -e 'print $DateTime::TimeZone::VERSION . \"\\n\"';\n2.13\n</code></pre>"},{"location":"docs/software/using/perl/#uninstallation","title":"Uninstallation","text":"<p>To uninstall a Perl module:</p> <pre><code>$ cpanm -U DateTime::TimeZone\n</code></pre> <ol> <li> <p>CPAN can denote either the archive network itself, or the Perl program   that acts as an interface to the network and as an automated software   installer (somewhat like a package manager). Most software on CPAN is free   and open source.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/using/postgresql/","title":"PostgreSQL","text":""},{"location":"docs/software/using/postgresql/#introduction","title":"Introduction","text":"<p>PostgreSQL is a powerful, open source object-relational database system with a strong focus on reliability, feature robustness, and performance.</p>"},{"location":"docs/software/using/postgresql/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using PostgreSQL on Sherlock. For more complete documentation about PostgreSQL in general, please see the PostgreSQL documentation.</p>"},{"location":"docs/software/using/postgresql/#postgresql-on-sherlock","title":"PostgreSQL on Sherlock","text":"<p>We don't provide any centralized database service on Sherlock, but we provide a centralized installation of PostgreSQL, and each user is welcome to start their own instance of the database server to fit their jobs' needs.</p> <p>The overall process to run an instance of PostgreSQL on Sherlock would look like this:</p> <ol> <li>configure and initialize your environment so you can start a database    instance under your user account,</li> <li>start the database server,</li> <li>run SQL queries from the same node (via a local socket), or from other nodes    and/or jobs (via the network).</li> </ol>"},{"location":"docs/software/using/postgresql/#single-node-access","title":"Single-node access","text":"<p>In that example, the database server and client will run within the same job, on the same compute node.</p>"},{"location":"docs/software/using/postgresql/#preparation","title":"Preparation","text":"<p>You first need to let PostgreSQL know where to store its database. The commands below only need to be executed once.</p> <p>Assuming you'll want to store your database files in a <code>db/</code> directory in your <code>$SCRATCH</code> folder, you can run the following commands:</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n</code></pre> <p>Once you have your <code>$DB_DIR</code> in place, you need to initialize your database with some internal data that PostgreSQL needs. In the same terminal, run the following commands:</p> <pre><code>$ ml system postgresql\n$ initdb $DB_DIR\n</code></pre>"},{"location":"docs/software/using/postgresql/#start-the-server","title":"Start the server","text":"<p>You can now start the PostgreSQL server. For this, first get an allocation on a compute node, note the hostname of the compute node your job has been allocated, load the <code>postgresql</code> module, and then run the <code>postgresql</code> server:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system postgresql\n$ export DB_DIR=$SCRATCH/db\n$ postgres -D $DB_DIR\n[...]\n2018-10-09 17:42:08.094 PDT [3841] LOG:  database system is ready to accept connections\n</code></pre> <p>The <code>postgres</code> process will be blocking, meaning it will not give the prompt back for as long as the PostgreSQL server runs.</p>"},{"location":"docs/software/using/postgresql/#run-queries","title":"Run queries","text":"<p>You're now ready to run queries against that PostgreSQL instance, from the same node your job is running on.</p> <p>From another terminal on Sherlock, connect to your job's compute node (here, it's <code>sh-01-01</code>, as shown above), load the <code>postgresql</code> module, and then run the <code>createdb</code> command: it will create a database that you can use as a testbed:</p> <pre><code>$ ssh sh-01-01\n$ ml system postgresql\n$ createdb test_db\n</code></pre> <p>Once this is done, from the same shell, you can run the <code>psql</code> command, which will open the PostgreSQL shell, ready to run your SQL queries:</p> <pre><code>$ psql test_db\npsql (10.5)\nType \"help\" for help.\n\ntest_db=#\n</code></pre> <p>Once you're done with your PostgreSQL instance, you can just terminate your job, and all the processes will be terminated automatically.</p>"},{"location":"docs/software/using/postgresql/#multi-node-access","title":"Multi-node access","text":"<p>In case you need to run a more persistent instance of PostgreSQL, you can for instance submit a dedicated job to run the server, make it accessible over the network, and run queries from other jobs and/or nodes.</p>"},{"location":"docs/software/using/postgresql/#enable-network-access","title":"Enable network access","text":"<p>The preparation steps are pretty similar to the single-node case, except the PostgreSQL server instance will be accessed over the network rather than through a local socket.</p> <p>Network access must be secured</p> <p>When running an networked instance of PostgreSQL, please keep in mind that any user on Sherlock could potentially be able to connect to the TCP ports that <code>postgres</code> runs on, and that proper configuration must be done to prevent unauthorized access.</p> <p>Like in the single-node case, you need to start the <code>postgres</code> server process, but with the <code>-i</code> option to enable network connections, and define user access in your <code>$DB_DIR/pg_hba.conf</code> file (see below).</p>"},{"location":"docs/software/using/postgresql/#secure-access","title":"Secure access","text":"<p>To allow network connections to the database server, a password will need to be defined for the PostgreSQL user.  That will allow this user to connect to the PostgreSQL instance from any node.  Please make sure to replace the <code>my-secure-password</code> string below by the actual password of your choice.</p> <p>Choose a proper password</p> <p>This password will only be used to access this specific instance of PostgreSQL. Note that anybody knowing that password will be allowed to connect to your PostgreSQL instances and modify data in the tables.</p> <ul> <li>do NOT use <code>my-secure-password</code></li> <li>do NOT use your SUNet ID password</li> </ul> <p>Once you've chosen your password, you can now start the PostgreSQL server on a compute, as described in the previous section, initialize the database, and set the user password:</p> <pre><code>$ srun --pty bash\n\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ ml system postgresql\n$ initdb $DB_DIR\n$ createdb test_db\n\n$ psql -c \"ALTER USER $USER PASSWORD 'my-secure-password';\" test_db\n</code></pre> <p>Then, we need to edit the <code>$DB_DIR/ph_hba.conf</code> file to allow network access for user <code>$USER</code>:</p> <pre><code>$ cat &lt;&lt; EOF &gt; $DB_DIR/pg_hba.conf\nlocal   all             all                                     trust\nhost    all             all             127.0.0.1/32            trust\nhost    all             all             ::1/128                 trust\nhost    all             $USER           samenet                 md5\nEOF\n</code></pre> <p>Once you've done that, you're ready to terminate that interactive job, and start a dedicated PostgreSQL server job.</p> <pre><code>$ pg_ctl stop -D $DB_DIR\n$ logout\n</code></pre>"},{"location":"docs/software/using/postgresql/#start-postgresql-in-a-job","title":"Start PostgreSQL in a job","text":"<p>You can use the following <code>postgresql.sbatch</code> job as a template:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=postgresql\n#SBATCH --time=8:0:0\n#SBATCH --dependency=singleton\n\nexport DB_DIR=$SCRATCH/db\n\nml system postgresql\n\npostgres -i -D $DB_DIR\n</code></pre> <p>and submit it with:</p> <pre><code>$ sbatch postgresql.sbatch\n</code></pre> <p>Concurrent instances will lead to data corruption</p> <p>An important thing to keep in mind is that having multiple instances of a PostgreSQL server running at the same time, using the same database files, will certainly lead to catastrophic situations and the corruption of those files.</p> <p>To prevent this from happening, the <code>--dependency=singleton</code> job submission option will make sure that only one instance of that job (based on its name and user) will run at any given time.</p>"},{"location":"docs/software/using/postgresql/#connect-to-the-running-instance","title":"Connect to the running instance","text":"<p>Now, from any node on Sherlock, whether from a login node, an interactive job, or a batch job, using the <code>mysql</code> CLI or any application binding in any language, you should be able to connect to your running PostgreSQL instance,</p> <p>First, identify the node your job is running on with <code>squeue</code>:</p> <pre><code>$ squeue -u $USER -n postgresql\n             JOBID PARTITION       NAME     USER ST       TIME  NODES NODELIST(REASON)\n          21383445    normal postgresql   kilian  R       0:07      1 sh-01-02\n</code></pre> <p>and then, point your PostgreSQL client to that node:</p> <pre><code>$ ml system postgresql\n$ psql -h sh-06-34  test_db\nPassword:\npsql (10.5)\nType \"help\" for help.\n\ntest_db=#\n</code></pre> <p>That's it! You can now run SQL queries from anywhere on Sherlock to your own PostgreSQL instance.</p>"},{"location":"docs/software/using/postgresql/#persistent-db-instances","title":"Persistent DB instances","text":"<p>SQL data is persistent</p> <p>All the data you import in your SQL databases will be persistent across jobs. Meaning that you can run a PostgreSQL server job for the day, import data in its database, stop the job, and resubmit the same PostgreSQL server job the next day: all your data will still be there as long as the location you've chosen for your database (the <code>$DB_DIR</code> defined in the Preparation steps) is on a persistent storage location.</p> <p>If you need database access for more than the maximum runtime of a job, you can use the instructions provided to define self-resubmitting recurring jobs and submit long-running database instances.</p>"},{"location":"docs/software/using/python/","title":"Python","text":""},{"location":"docs/software/using/python/#introduction","title":"Introduction","text":"<p>Python is an interpreted high-level programming language for general-purpose programming. Its design philosophy emphasizes code readability. It provides constructs that enable clear programming on both small and large scales, which makes it both easy to learn and very well-suited for rapid prototyping.</p>"},{"location":"docs/software/using/python/#more-documentation","title":"More documentation","text":"<p>The following documentation is specifically intended for using Python on Sherlock. For more complete documentation about Python in general, please see the Python documentation.</p>"},{"location":"docs/software/using/python/#python-on-sherlock","title":"Python on Sherlock","text":"<p>Sherlock features multiple versions of Python.</p> <p>Some applications only work with legacy features of version 2.x, while more recent code will require specific version 3.x features.  Modules on Sherlock may only be available in a single flavor (as denoted by their suffix: <code>_py27</code> or <code>_py36</code>, because the application only supports one or the other.</p> <p>You can load either version on Sherlock by doing the following commands:</p> <pre><code>$ ml python/2.7.13\n</code></pre> <p>or</p> <pre><code>$ ml python/3.6.1\n</code></pre> <p>The Python3 interpreter is <code>python3</code></p> <p>The Python3 executable is named <code>python3</code>, not <code>python</code>. So, once you have the \"python/3.6.1\" module loaded on Sherlock, you will need to use <code>python3</code> to invoke the proper interpreter. <code>python</code> will still refer to the default, older system-level Python installation, and may result in errors when trying to run Python3 code.</p> <p>This is an upstream decision detailed in PEP-394, not something specific to Sherlock.</p>"},{"location":"docs/software/using/python/#using-python","title":"Using Python","text":"<p>Once your environment is configured (ie. when the Python module is loaded), Python can be started by simply typing <code>python</code> at the shell prompt:</p> <pre><code>$ python\nPython 2.7.13 (default, Apr 27 2017, 14:19:21)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre>"},{"location":"docs/software/using/python/#python-in-batch-jobs","title":"Python in batch jobs","text":"<p>Python output is buffered by default</p> <p>By default, Python buffers console output. It means that when running Python in a batch job through Slurm, you may see output less often than you would when running interactively.</p> <p>When output is being buffered, the <code>print</code> statements are aggregated until there is a enough data to print, and then the messages are all printed at once. And as a consequence, job output files (as specified with the <code>--output</code> and <code>--error</code> job submission options) will be refreshed less often and may give the impression that the job is not running.</p> <p>For debugging or checking that a Python script is producing the correct output, you may want to switch off buffering.</p>"},{"location":"docs/software/using/python/#switching-off-buffering","title":"Switching  off buffering","text":"<p>For a single python script you can use the <code>-u</code> option, as in <code>python -u my_script.py</code>. The <code>-u</code> option stands for \"unbuffered\".</p> <p>For instance:</p> <pre><code>#!/bin/bash\n#SBATCH -n 1\n\npython -u my_script.py\n</code></pre> <p>Tip</p> <p>You can also use the environment variable <code>PYTHONUNBUFFERED</code> to set unbuffered I/O for your whole batch script. <pre><code>#!/bin/bash\n#SBATCH -n 1\n\nexport PYTHONUNBUFFERED=True\npython my_script.py\n</code></pre></p> <p>NB: There is some performance penalty for having unbuffered print statements, so you may want to reduce the number of print statements, or run buffered for production runs.</p>"},{"location":"docs/software/using/python/#python-packages","title":"Python packages","text":"<p>The capabilities of Python can be extended with packages developed by third parties. In general, to simplify operations, it is left up to individual users and groups to install these third-party packages in their own directories. However, Sherlock provides tools to help you install the third-party packages that you need.</p> <p>Among many others, the following common Python packages are provided on Sherlock:</p> <ul> <li>NumPy</li> <li>SciPy</li> </ul> <p>Python modules on Sherlock generally follow the naming scheme below:</p> <pre><code>py-&lt;package_name&gt;/version_py&lt;python_version&gt;\n</code></pre> <p>For instance, NumPy modules are:</p> <ul> <li><code>py-numpy/1.14.3_py27</code></li> <li><code>py-numpy/1.14.3_py36</code></li> </ul> <p>You can list all available module versions for a package with <code>ml spider &lt;package_name&gt;</code>. For instance:</p> <pre><code>$ ml spider tensorflow\n-------------------------------------------------------------------------------\n  py-tensorflow:\n-------------------------------------------------------------------------------\n    Description:\n      TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs.\n\n     Versions:\n        py-tensorflow/1.6.0_py27\n        py-tensorflow/1.6.0_py36\n        py-tensorflow/1.7.0_py27\n        py-tensorflow/1.9.0_py27\n        py-tensorflow/1.9.0_py36\n</code></pre> <p>Dependencies are handled automatically</p> <p>When you decide to use NumPy on Sherlock, you just need to load the <code>py-numpy</code> module of your choice, and the correct Python interpreter will be loaded automatically. No need to load a <code>python</code> module explicitly.</p>"},{"location":"docs/software/using/python/#installing-packages","title":"Installing packages","text":"<p>If you need to use a Python package that is not already provided as a module on Sherlock, you can use the <code>pip</code> command. This command takes care of compiling and installing most of Python packages and their dependencies. All of <code>pip</code>'s commands and options are explained in detail in the Pip user guide.</p> <p>A comprehensive index of Python packages can be found at PyPI.</p> <p>To install Python packages with <code>pip</code>, you'll need to use the <code>--user</code> option. This will make sure that those packages are installed in a user-writable location (by default, your <code>$HOME</code> directory). Since your <code>$HOME</code> directory is shared across nodes on Sherlock, you'll only need to install your Python packages once, and they'll be ready to be used on every single node in the cluster.</p> <p>For example:</p> <pre><code>$ pip install --user &lt;package_name&gt;\n</code></pre> <p>For Python 3, use <code>pip3</code>:</p> <pre><code>$ pip3 install --user &lt;package_name&gt;\n</code></pre> <p>Python packages will be installed in <code> $HOME/.local/lib/python&lt;&lt;version&gt;/site-packages</code>, meaning that packages for Python 2.x and Python 3.x will be kept separate. This both means that they won't interfere with each other, but also that if you need to use a package with both Python 2.x and 3.x, you'll need to install it twice, once for each Python version.</p>"},{"location":"docs/software/using/python/#list-installed-packages","title":"List installed packages","text":"<p>You can easily see the list of the Python packages installed in your environment, and their location, with <code>pip list</code>:</p> <pre><code>$ pip list -v\nPackage    Version Location                                                            Installer\n---------- ------- ------------------------------------------------------------------- ---------\npip        18.1    /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nsetuptools 28.8.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nurllib3    1.24    /home/users/kilian/.local/lib/python2.7/site-packages               pip\nvirtualenv 15.1.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\n</code></pre>"},{"location":"docs/software/using/python/#alternative-installation-path","title":"Alternative installation path","text":"<p>Python paths</p> <p>While theoretically possible, installing Python packages in alternate locations can be tricky, so we recommend trying to stick to the <code>pip install --user</code> way as often as possible. But in case you absolutely need it, we provide some guidelines below.</p> <p>One common case of needing to install Python packages in alternate locations is to share those packages with a group of users. Here's an example that will show how to install the <code>urllib3</code> Python package in a group-shared location and let users from the group use it without having to install it themselves.</p> <p>First, you need to create a directory to store those packages. We'll put it in <code>$GROUP_HOME</code>:</p> <pre><code>$ mkdir -p $GROUP_HOME/python/\n</code></pre> <p>Then, we load the Python module we need, and we instruct <code>pip</code> to install its packages in the directory we just created:</p> <pre><code>$ ml python/2.7.13\n$ PYTHONUSERBASE=$GROUP_HOME/python pip install --user urllib3\n</code></pre> <p>We still use the <code>--user</code> option, but with <code>PYTHONUSERBASE</code> pointing to a different directory, <code>pip</code> will install packages there.</p> <p>Now, to be able to use that Python module, since it's not been installed in a default directory, you (and all the members of the group who will want to use that module) need to set their <code>PYTHONPATH</code> to include our new shared directory<sup>1</sup>:</p> <pre><code>$ export PYTHONPATH=$GROUP_HOME/python/lib/python2.7/site-packages:$PYTHONPATH\n</code></pre> <p>And now, the module should be visible:</p> <pre><code>$ pip list -v\nPackage    Version Location                                                            Installer\n---------- ------- ------------------------------------------------------------------- ---------\npip        18.1    /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nsetuptools 28.8.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nurllib3    1.24    /home/groups/ruthm/python/lib/python2.7/site-packages               pip\nvirtualenv 15.1.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\n</code></pre> <p><code>$PYTHONPATH</code> depends on the Python version</p> <p>The <code>$PYTHONPATH</code> environment variable is dependent on the Python version you're using, so for Python 3.6, it should include <code>$GROUP_HOME/python/lib/python3.6/site-packages</code></p> <p><code>$PATH</code> may also need to be updated</p> <p>Some Python package sometimes also install executable scripts. To make them easily accessible in your environment, you may also want to modify your <code>$PATH</code> to include their installation directory.</p> <p>For instance, if you installed Python packages in <code>$GROUP_HOME/python</code>: <pre><code>$ export PATH=$GROUP_HOME/python/bin:$PATH\n</code></pre></p>"},{"location":"docs/software/using/python/#installing-from-github","title":"Installing from GitHub","text":"<p><code>pip</code> also supports installing packages from a variety of sources, including GitHub repositories.</p> <p>For instance, to install HTTPie, you can do:</p> <pre><code>$ pip install --user git+git://github.com/jkbr/httpie.git\n</code></pre>"},{"location":"docs/software/using/python/#installing-from-a-requirements-file","title":"Installing from a requirements file","text":"<p><code>pip</code> allows installing a list of packages listed in a file, which can be pretty convenient to install several dependencies at once.</p> <p>In order to do this, create a text file called <code>requirements.txt</code> and place each package you would like to install on its own line:</p> <code>requirements.txt</code> <pre><code>numpy\nscikit-learn\nkeras\ntensorflow\n</code></pre> <p>You can now install your modules like so:</p> <pre><code>$ ml python\n$ pip install --user -r requirements.txt\n</code></pre>"},{"location":"docs/software/using/python/#upgrading-packages","title":"Upgrading packages","text":"<p><code>pip</code> can update already installed packages with the following command:</p> <pre><code>$ pip install --user --upgrade &lt;package_name&gt;\n</code></pre> <p>Upgrading packages also works with <code>requirements.txt</code> files:</p> <pre><code>$ pip install --user --upgrade -r requirements.txt\n</code></pre>"},{"location":"docs/software/using/python/#uninstalling-packages","title":"Uninstalling packages","text":"<p>To uninstall a Python package, you can use the <code>pip uninstall</code> command (note that it doesn't take any <code>--user</code> option):</p> <pre><code>$ pip uninstall &lt;package_name&gt;\n$ pip uninstall -r requirements.txt\n</code></pre>"},{"location":"docs/software/using/python/#virtual-environments","title":"Virtual environments","text":"<p> Work in progress </p> <p>This page is a work in progress and is not complete yet. We are actively working on adding more content and information.</p> <ol> <li> <p>This line can also be added to a user's <code>~/.profile</code> file, for a   more permanent setting.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/software/using/quantum-espresso/","title":"Quantum Espresso","text":""},{"location":"docs/software/using/quantum-espresso/#introduction","title":"Introduction","text":"<p>Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudo-potentials.Perl is a high-level, general-purpose, interpreted, dynamic programming</p> <p>Quantum ESPRESSO has evolved into a distribution of independent and inter-operable codes in the spirit of an open-source project. The Quantum ESPRESSO distribution consists of a \u201chistorical\u201d core set of components, and a set of plug-ins that perform more advanced tasks, plus a number of third-party packages designed to be inter-operable with the core components. Researchers active in the field of electronic-structure calculations are encouraged to participate in the project by contributing their own codes or by implementing their own ideas into existing codes.</p>"},{"location":"docs/software/using/quantum-espresso/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Quantum Espresso on Sherlock.  For more complete documentation about Quantum Espresso in general, please see the Quantum Espresso documentation.</p>"},{"location":"docs/software/using/quantum-espresso/#quantum-espresso-on-sherlock","title":"Quantum Espresso on Sherlock","text":"<p>To run Quantum Espresso on Sherlock, you can use one of the [provided modules][url_soft_qe], or run it from a container.</p> <p>The CPU version of Quantum Espresso can be loaded via the <code>quantum-espresso</code> module:</p> <pre><code>$ ml chemistry quantum-espresso\n</code></pre> <p>and the GPU version can be loaded via the <code>quantum-espresso_gpu</code> module:</p> <pre><code>$ ml chemistry quantum-espresso_gpu\n</code></pre>"},{"location":"docs/software/using/quantum-espresso/#examples","title":"Examples","text":"<p>Here are a few examples showing how to run the AUSURF112 benchmark.</p>"},{"location":"docs/software/using/quantum-espresso/#preparation","title":"Preparation","text":"<p>The first step is to get the benchmark files:</p> <pre><code>$ cd $SCRATCH\n$ git clone https://github.com/QEF/benchmarks qe_benchmarks\n$ cd qe_benchmarks/AUSURF112\n</code></pre>"},{"location":"docs/software/using/quantum-espresso/#cpu-version","title":"CPU version","text":"<p>To submit a Quantum Espresso job to run the AUSURF112 benchmark on CPU nodes, the following submission script can be used:</p> qe-bench_cpu.sbatch <pre><code>#!/bin/bash\n#SBATCH --nodes=2                # number of nodes for the job\n#SBATCH --ntasks-per-node=16     # number of tasks per node\n#SBATCH --time=00:30:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n\nmodule reset\nmodule load chemistry\nmodule load quantum-espresso/7.0\n\ncd $SCRATCH/qe_benchmarks\ncd AUSURF112\n\nsrun pw.x -input ausurf.in -npool 2\n</code></pre> <p>In this example, the job will request 32 CPU cores on 2 nodes, 30 minutes of run time, and will send an email notification when the job starts and when it ends.</p> <p>The job can be submitted with:</p> <pre><code>$ sbatch qe-bench_cpu.sbatch\n</code></pre>"},{"location":"docs/software/using/quantum-espresso/#gpu-version","title":"GPU version","text":""},{"location":"docs/software/using/quantum-espresso/#native","title":"Native","text":"<p>The GPU version can be loaded through the <code>quantum-espresso_gpu</code> module.</p> <p>Using the same benchmark files as for the CPU version above, you can create a job submissions script like this:</p> qe-bench_gpu.sbatch <pre><code>#!/bin/bash\n#SBATCH --partition=gpu          # partition to submit the job to\n#SBATCH --nodes=2                # number of nodes for the job\n#SBATCH --gpus-per-node=1        # number of GPUs per node\n#SBATCH --time=00:30:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n\nmodule reset\nmodule load chemistry\nmodule load quantum-espresso_gpu/7.0\n\ncd $SCRATCH/qe_benchmarks\ncd AUSURF112\n\nsrun pw.x -input ausurf.in -npool 2\n</code></pre> <p>In this example, the job will request 2 GPU on 2 nodes, 30 minutes of run time, and will send an email notification when the job starts and when it ends.</p> <p>It can be submitted with:</p> <pre><code>$ sbatch qe-bench_gpu.sbatch\n</code></pre>"},{"location":"docs/software/using/quantum-espresso/#ngc-container","title":"NGC container","text":"<p>Another option to run a GPU version of Quantum Espresso is to use a 3<sup>rd</sup>-party container.</p> <p>The NVIDIA GPU Cloud (NGC) hosts a Quantum Espresso container container that could be used on Sherlock.</p>"},{"location":"docs/software/using/quantum-espresso/#with-singularity","title":"With Singularity","text":"<p>To use the container with Singularity, first pull the Quantum Espresso container with:</p> <pre><code>$ cd $SCRATCH\n$ singularity pull docker://nvcr.io/hpc/quantum_espresso:qe-7.0\n</code></pre> <p>Then create the following script:</p> qe-bench_gpu_singularity.sbatch <pre><code>#!/bin/bash\n#SBATCH --partition=gpu          # partition to submit the job to\n#SBATCH --nodes=2                # number of nodes for the job\n#SBATCH --gpus-per-node=1        # number of GPUs per node\n#SBATCH --mem=32GB               # memory per node\n#SBATCH --time=00:30:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n\ncd $SCRATCH/qe_benchmarks\ncd AUSURF112\n\nsrun singularity run --nv \\\n    $SCRATCH/quantum_espresso_qe-7.0.sif \\\n    pw.x -input ausurf.in -npool 2\n</code></pre> <p>and submit it:</p> <pre><code>$ sbatch qe-bench_gpu_singularity.sbatch\n</code></pre>"},{"location":"docs/software/using/quantum-espresso/#with-pyxisenroot","title":"With pyxis/enroot","text":"<p>To use the container with pyxis/enroot, you can directly submit the following script:</p> qe-bench_gpu_enroot.sbatch <pre><code>#!/bin/bash\n#SBATCH --partition=gpu          # partition to submit the job to\n#SBATCH --nodes=2                # number of nodes for the job\n#SBATCH --gpus-per-node=1        # number of GPUs per node\n#SBATCH --mem=32GB               # memory per node\n#SBATCH --time=00:30:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n\ncd $SCRATCH/qe_benchmarks\ncd AUSURF112\n\nsrun --container-image nvcr.io/hpc/quantum_espresso:qe-7.0 \\\n     --container-workdir $PWD \\\n     pw.x -input ausurf.in -npool 2\n</code></pre> <p>and submit it:</p> <pre><code>$ sbatch qe-bench_gpu_singularity.sbatch\n</code></pre>"},{"location":"docs/software/using/rclone/","title":"Rclone","text":""},{"location":"docs/software/using/rclone/#introduction","title":"Introduction","text":"<p>If you need to sync files between cloud storage to Sherlock, <code>rclone</code> is a command line program that can help. You can easily use it to transfer files from a cloud storage provider to Sherlock or Oak, or vice versa. The following tutorial walks through transferring files between Google Drive and Oak storage.</p>"},{"location":"docs/software/using/rclone/#more-documentation","title":"More documentation","text":"<p>For more information on running <code>rclone</code>, please see the official documentation.</p>"},{"location":"docs/software/using/rclone/#setup","title":"Setup","text":""},{"location":"docs/software/using/rclone/#rclone-config","title":"<code>rclone config</code>","text":"<p>Before transferring data for the first time, you will need to configure <code>rclone</code> so that it can access your Google Drive. This will require use of your browser, so you will need to connect to Sherlock with local port forwarding (<code>ssh -L</code>). You only need to do this when you are configuring <code>rclone</code> for the first time.</p> <p>Use local terminal for <code>rclone config</code></p> <p>This method will not work in the Sherlock OnDemand shell. You will need to use your local machine's terminal to enable local port forwarding and to allow <code>rclone</code> to communicate with your browser. On Linux and macOS, you can use the Terminal app; on Windows, you can use the PowerShell app.</p> <p>When running <code>rclone config</code> you will be prompted to enter names and values, indicated by the <code>&gt;</code> symbol. To leave it empty, press Enter.</p> <pre><code># Connect to Sherlock with local port fowarding\n$ ssh -L localhost:53682:localhost:53682 &lt;SUNetID&gt;@login.sherlock.stanford.edu\n\n\n# Load the rclone module\n$ ml system rclone\n\n\n# Run the rclone configuration tool\n$ rclone config\n\nNo remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\n\nEnter name for new remote.\nname&gt; gdrive\n\nOption Storage.\nType of storage to configure.\nChoose a number from below, or type in your own value.\n 1 / 1Fichier\n   \\ (fichier)\n 2 / Akamai NetStorage\n   \\ (netstorage)\n       ...\n18 / Google Drive\n   \\ (drive)\n       ...\n48 / premiumize.me\n   \\ (premiumizeme)\n49 / seafile\n   \\ (seafile)\nStorage&gt; drive\n\nOption client_id.\nGoogle Application Client Id\n...\nEnter a value. Press Enter to leave empty.\nclient_id&gt;\n\nOption client_secret.\nOAuth Client Secret.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_secret&gt;\n\nOption scope.\nScope that rclone should use when requesting access from drive.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n 1 / Full access all files, excluding Application Data Folder.\n   \\ (drive)\n...\nscope&gt; 1\n\nOption service_account_file.\nService Account Credentials JSON file path.\nLeave blank normally.\n...\nEnter a value. Press Enter to leave empty.\nservice_account_file&gt;\n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt; n\n\nUse auto config?\n * Say Y if not sure\n * Say N if you are working on a remote or headless machine\n\ny) Yes (default)\nn) No\ny/n&gt; y\n\n2023/09/12 10:51:55 NOTICE: If your browser doesn't open automatically go to the\nfollowing link: http://127.0.0.1:53682/auth?state=#################\n2023/09/12 10:51:55 NOTICE: Log in and authorize rclone for access\n2023/09/12 10:51:55 NOTICE: Waiting for code...\n</code></pre> <p>At this point, you can copy and paste the provided link into your browser. You will be asked to confirm that you want to allow <code>rclone</code> to access your files. Once you have successfully done so, you can complete the configuration in the terminal.</p> <pre><code>Configure this as a Shared Drive (Team Drive)?\n\ny) Yes\nn) No (default)\ny/n&gt; n\n\nConfiguration complete.\nOptions:\n...\nKeep this \"gdrive\" remote?\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\n\nCurrent remotes:\n\nName                 Type\n====                 ====\ngdrive               drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre>"},{"location":"docs/software/using/rclone/#examples","title":"Examples","text":""},{"location":"docs/software/using/rclone/#rclone-copy","title":"<code>rclone copy</code>","text":"<p>To transfer data between cloud storage and Sherlock or Oak, you can use the <code>rclone copy</code> command.</p> <pre><code># Start an interactive dev session\n$ sh_dev\n\n# Load the rclone module\n$ ml system rclone\n\n# Copy a folder from Google Drive to Oak\n$ rclone copy gdrive:&lt;folder name&gt; /oak/stanford/groups/&lt;group_name&gt;/&lt;folder name&gt;\n\n$ Copy a single file from Oak to Google Drive\n$ rclone copy /oak/stanford/groups/&lt;group name&gt;/&lt;file name&gt; gdrive:\n</code></pre>"},{"location":"docs/software/using/rclone/#rclone-lslsd","title":"<code>rclone ls</code>/<code>lsd</code>","text":"<p>To view the files and folders in your cloud storage, you can use the <code>rclone ls</code> and <code>rclone lsd</code> commands, respectively.</p> <pre><code># Load the rclone module\n$ ml system rclone\n\n# List all top-level directories in Google Drive\n$ rclone lsd gdrive: --max-depth 1\n\n# List all files in a directory\n$ rclone ls gdrive:&lt;folder name&gt;\n\n# List all files on Google Drive (including those in folders)\n$ rclone ls gdrive:\n</code></pre>"},{"location":"docs/software/using/schrodinger/","title":"Schr\u00f6dinger","text":""},{"location":"docs/software/using/schrodinger/#introduction","title":"Introduction","text":"<p>The Schr\u00f6dinger suite is a commercial and licensed software used to simulate and model molecular behavior at the atomic level. The Schr\u00f6dinger software tools include molecular dynamics simulations, quantum mechanics calculations, virtual screening and visualization tools.</p>"},{"location":"docs/software/using/schrodinger/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Schr\u00f6dinger on Sherlock. For more complete documentation about Schr\u00f6dinger in general, please contact Schr\u00f6dinger support.</p>"},{"location":"docs/software/using/schrodinger/#schrodinger-on-sherlock","title":"Schr\u00f6dinger on Sherlock","text":""},{"location":"docs/software/using/schrodinger/#licensing","title":"Licensing","text":"<p>Stanford Libraries have purchased a site license for the Schr\u00f6dinger suite.  Please contact Stanford Libraries at sciencelibrary@stanford.edu and CC srcc-support@stanford.edu if you would like to access Schr\u00f6dinger on Sherlock: after we receive confirmation, your PI group will be granted access on Sherlock.</p>"},{"location":"docs/software/using/schrodinger/#using-schrodinger","title":"Using Schr\u00f6dinger","text":"<p>You can use Schr\u00f6dinger software after having loaded the corresponding software module with the <code>module</code> command. To load the current default version:</p> <pre><code>module load chemistry schrodinger\n</code></pre> <p>To see all the available versions, you can use the <code>module spider</code> command:</p> <pre><code>$ module spider schrodinger\n</code></pre> <p>Once loaded, the <code>$SCHRODINGER</code> environment variable is automatically set to allow all Schr\u00f6dinger commands to run. For example, to run the <code>jaguar</code> command:</p> <pre><code>$ jaguar run -WAIT H20.in\n</code></pre> <p>To call the basic Schr\u00f6dinger <code>run</code> command, just enter:</p> <pre><code>$ run\n</code></pre> <p>or <code>glide</code>:</p> <pre><code>$ glide\nusage: glide_startup.py [options] &lt;input_file&gt;\nglide_startup.py: error: the following arguments are required: input_file\n</code></pre>"},{"location":"docs/software/using/schrodinger/#maestro-gui","title":"Maestro GUI","text":"<p>OnDemand shell sessions</p> <p>Opening an X11/GUI session will not work in a Sherlock OnDemand terminal session. You will need to use the method mentioned below, i.e. a standard terminal session with an X11 client.</p> <p>To launch the Maestro GUI, once you have loaded the Schr\u00f6dinger module, simply run:</p> <pre><code>$ maestro\n</code></pre> <p>You'll need to enable X11 forwarding in your initial connection to Sherlock, and request it as well for your job allocation.</p> <p>Here are some example commands you can run:</p> <pre><code># on your local machine\n$ ssh -X login.sherlock.stanford.edu\n\n# then from a Sherlock login node\n$ sh_dev -m 16GB\n\n# and finally on the allocated compute node:\n$ ml load chemistry schrodinger\n$ maestro\n</code></pre> <p>This will launch Maestro on a compute node and display its graphical user interface on your local machine's display.</p> <p>GUI performance</p> <p>Please note that running graphical user interfaces (GUIs) over the network via X11 over SSH may not necessarily yield the best performance. Graphical analysis is often best done on a local machine, while intensive, batch scheduled computations are carried over on the cluster.</p> <p>For more information about X11 forwarding, you can refer to this page.</p>"},{"location":"docs/software/using/schrodinger/#examples","title":"Examples","text":""},{"location":"docs/software/using/schrodinger/#batch-job-submission","title":"batch job submission","text":"<p>Here's an example batch script, requesting 1 CPU, for 10 minutes on the <code>normal</code> partition, that can be saved as <code>water.sbatch</code>:</p> <pre><code>#!/usr/bin/bash\n#SBATCH -o water.%j.out\n#SBATCH -e water.%j.err\n#SBATCH -n 1\n#SBATCH -t 10:00\n#SBATCH -p normal\n\n# Load required modules\nmodule load chemistry schrodinger\n\n# Run Schr\u00f6dinger, -WAIT is often required\njaguar run -WAIT H20.in\n</code></pre> <p>Save this input file as <code>H2O.in</code>:</p> <pre><code>&amp;gen\n&amp;\n&amp;echo\n&amp;\n&amp;zmat\nO       0.0000000000000   0.0000000000000  -0.1135016000000\nH1      0.0000000000000   0.7531080000000   0.4540064000000\nH2      0.0000000000000  -0.7531080000000   0.4540064000000\n&amp;\n</code></pre> <p>And you can submit the batch script with:</p> <pre><code>$ sbatch water.sbatch\n</code></pre> <p>After execution, you should find a <code>H20.out</code> output file in the current directory, as well as a log file (<code>H20.log</code>). If you don't, you can check for errors in the job output and error files: <code>water.&lt;jobid&gt;.{out,err}</code>.</p>"},{"location":"docs/software/using/spark/","title":"Spark","text":""},{"location":"docs/software/using/spark/#introduction","title":"Introduction","text":"<p>Apache Spark\u2122 is a general engine for large-scale data processing.  This document gives a quick introduction how to get a first test program in Spark running on Sherlock.</p>"},{"location":"docs/software/using/spark/#more-documentation","title":"More documentation","text":"<p>The following documentation specifically intended for using Spark on Sherlock. For more complete documentation about Spark in general, please see the Apache Spark documentation.</p>"},{"location":"docs/software/using/spark/#spark-on-sherlock","title":"Spark on Sherlock","text":"<p>Running Apache Spark on Sherlock is a bit different from using a traditional Spark/Hadoop cluster in that it requires some level of integration with the scheduler.  In a sense, the computing resources (memory and CPU) need to be allocated twice. First, sufficient resources for the Spark application need to be allocated via Slurm ; and secondly, <code>spark-submit</code> resource allocation flags need to be properly specified.</p> <p>In order to use Spark, three steps have to be kept in mind when submitting a job to the queuing system:</p> <ol> <li>a new Spark cluster has to be started on the allocated nodes</li> <li>once the Spark cluster is up and running, Spark jobs have to be submitted to    the cluster</li> <li>after all Spark jobs have finished running, the cluster has to be shut down</li> </ol> <p>The following scripts show how to implement these three steps, and use the Pi Monte-Carlo calculation as an example.</p>"},{"location":"docs/software/using/spark/#single-node-job","title":"Single-node job","text":"<p>In this example, all the Spark processes run on the same compute node, which makes for a fairly simply sbatch script. The following example will start a 8-core job on a single node, and run a Spark task within that allocation:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=spark_singlenode\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=8\n#SBATCH --time=10\n\nmodule load spark\n\n# This syntax tells spark to use all cpu cores on the node.\nexport MASTER=\"local[*]\"\n\n# This is a Scala example\nrun-example SparkPi 1000\n\n# This is a Python example.\nspark-submit --master $MASTER $SPARK_HOME/examples/src/main/python/pi.py 1000\n</code></pre>"},{"location":"docs/software/using/spark/#multi-node-job","title":"Multi-node job","text":"<p>To start a Spark cluster and run a task on multiple nodes, more preliminary steps are necessary. Here's an example script that will span 2 nodes, start 2 Spark workers on each node, and allow each worker to use 8 cores:</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --mem-per-cpu=4G\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=2\n#SBATCH --output=sparkjob-%j.out\n\n## --------------------------------------\n## 0. Preparation\n## --------------------------------------\n\n# load the Spark module\nmodule load spark\n\n# identify the Spark cluster with the Slurm jobid\nexport SPARK_IDENT_STRING=$SLURM_JOBID\n\n# prepare directories\nexport SPARK_WORKER_DIR=${SPARK_WORKER_DIR:-$HOME/.spark/worker}\nexport SPARK_LOG_DIR=${SPARK_LOG_DIR:-$HOME/.spark/logs}\nexport SPARK_LOCAL_DIRS=${SPARK_LOCAL_DIRS:-/tmp/spark}\nmkdir -p $SPARK_LOG_DIR $SPARK_WORKER_DIR\n\n## --------------------------------------\n## 1. Start the Spark cluster master\n## --------------------------------------\n\nstart-master.sh\nsleep 1\nMASTER_URL=$(grep -Po '(?=spark://).*' \\\n             $SPARK_LOG_DIR/spark-${SPARK_IDENT_STRING}-org.*master*.out)\n\n## --------------------------------------\n## 2. Start the Spark cluster workers\n## --------------------------------------\n\n# get the resource details from the Slurm job\nexport SPARK_WORKER_CORES=${SLURM_CPUS_PER_TASK:-1}\nexport SPARK_MEM=$(( ${SLURM_MEM_PER_CPU:-4096} * ${SLURM_CPUS_PER_TASK:-1} ))M\nexport SPARK_DAEMON_MEMORY=$SPARK_MEM\nexport SPARK_WORKER_MEMORY=$SPARK_MEM\nexport SPARK_EXECUTOR_MEMORY=$SPARK_MEM\n\n# start the workers on each node allocated to the tjob\nexport SPARK_NO_DAEMONIZE=1\nsrun  --output=$SPARK_LOG_DIR/spark-%j-workers.out --label \\\n      start-slave.sh ${MASTER_URL} &amp;\n\n## --------------------------------------\n## 3. Submit a task to the Spark cluster\n## --------------------------------------\n\nspark-submit --master ${MASTER_URL} \\\n             --total-executor-cores $((SLURM_NTASKS * SLURM_CPUS_PER_TASK)) \\\n             $SPARK_HOME/examples/src/main/python/pi.py 10000\n\n## --------------------------------------\n## 4. Clean up\n## --------------------------------------\n\n# stop the workers\nscancel ${SLURM_JOBID}.0\n\n# stop the master\nstop-master.sh\n</code></pre>"},{"location":"docs/storage/","title":"Storage on Sherlock","text":"<p>Sherlock provides access to several file systems, each with distinct storage characteristics. Each user and PI group get access to a set of predefined directories in these file systems to store their data.</p> <p>Sherlock is a compute cluster, not a storage system</p> <p>Sherlock's storage resources are limited and are shared among many users. They are meant to store data and code associated with projects for which you are using Sherlock's computational resources. This space is for work actively being computed on with Sherlock, and should not be used as a target for backups from other systems.</p> <p>If you're looking for a long-term storage solution for research data, Stanford Research Computing offers the Oak storage system, which is specifically intended for this usage.</p> <p>Those file systems are shared with other users, and are subject to quota limits and for some of them, purge policies (time-residency limits).</p>"},{"location":"docs/storage/#filesystem-overview","title":"Filesystem overview","text":""},{"location":"docs/storage/#features-and-purpose","title":"Features and purpose","text":"Name Type Backups / Snapshots Performance Purpose Cost <code>$HOME</code>, <code>$GROUP_HOME</code> NFS  /  low small, important files (source code, executable files, configuration files...) free <code>$SCRATCH</code>, <code>$GROUP_SCRATCH</code> Lustre  /  high bandwidth large, temporary files (checkpoints, raw application output...) free <code>$L_SCRATCH</code> local SSD  /  low latency, high IOPS job specific output requiring high IOPS free <code>$OAK</code> Lustre option /  moderate long term storage of research data volume-based<sup>1</sup>"},{"location":"docs/storage/#access-scope","title":"Access scope","text":"Name Scope Access sharing level <code>$HOME</code> cluster user <code>$GROUP_HOME</code> cluster group <code>$SCRATCH</code> cluster user <code>$GROUP_SCRATCH</code> cluster group <code>$L_SCRATCH</code> compute node user <code>$OAK</code> cluster (optional, purchase required) group <p>Group storage locations are typically shared between all the members of the same PI group. User locations are only accessible by the user.</p>"},{"location":"docs/storage/#quotas-and-limits","title":"Quotas and limits","text":"<p>Volume and inodes</p> <p>Quotas are applied on both volume (the amount of data stored in bytes) and inodes: an inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. In practice, each filesystem entry (file, directory, link) counts as an inode.</p> Name Quota type Volume quota Inode quota Retention <code>$HOME</code> directory 15 GB n/a <code>$GROUP_HOME</code> directory 1 TB n/a <code>$SCRATCH</code> directory 100 TB 20 million time limited <code>$GROUP_SCRATCH</code> directory 100 TB 20 million time limited <code>$L_SCRATCH</code> n/a n/a n/a job lifetime <code>$OAK</code> directory amount purchased function of the volume purchased <p>Quota types:</p> <ul> <li>directory: based on files location and account for all the files   that are in a given directory.</li> <li>user: based on files ownership and account for all the files that   belong to a given user.</li> <li>group: based on files ownership and account for all the files that   belong to a given group.</li> </ul> <p>Retention types:</p> <ul> <li>: files are kept as long as the user account exists   on Sherlock.</li> <li>time limited: files are kept for a fixed length of time after they've   been last modified. Once the limit is reached, files expire and are   automatically deleted.</li> <li>job lifetime: files are only kept for the duration of the job and are   automatically purged when the job ends.</li> </ul> <p>Global fail-safe user and quota groups on <code>/scratch</code></p> <p>To prevent potential issues which would result in the file system filling up completely and making it unusable for everyone, additional user and group-level quotas are in place on the <code>/scratch</code> file system, as a fail-safe:</p> <ul> <li> <p>a user will not be able to use more than 250 TB (50M inodes) in total, in   all the <code>/scratch</code> directories they have access to.</p> </li> <li> <p>a group will not be able to use more than 1 PB (200M inodes) in total   across all the <code>/scratch</code> directories its group members have access to.</p> </li> </ul>"},{"location":"docs/storage/#checking-quotas","title":"Checking quotas","text":"<p>To check your quota usage on the different filesystems you have access to, you can use the <code>sh_quota</code> command:</p> <pre><code>$ sh_quota\n+---------------------------------------------------------------------------+\n| Disk usage for user kilian (group: ruthm)                                 |\n+---------------------------------------------------------------------------+\n|   Filesystem |  volume /   limit                  | inodes /  limit       |\n+---------------------------------------------------------------------------+\n          HOME |   9.4GB /  15.0GB [||||||     62%] |      - /      - (  -%)\n    GROUP_HOME | 562.6GB /   1.0TB [|||||      56%] |      - /      - (  -%)\n       SCRATCH |  65.0GB / 100.0TB [            0%] | 143.8K /  20.0M (  0%)\n GROUP_SCRATCH | 172.2GB / 100.0TB [            0%] |  53.4K /  20.0M (  0%)\n           OAK |  30.8TB / 240.0TB [|          12%] |   6.6M /  36.0M ( 18%)\n+---------------------------------------------------------------------------+\n</code></pre> <p>Several options are provided to allow listing quotas for a specific filesystem only, or in the context of a different group (for users who are members of several PI groups). Please see the <code>sh_quota</code> usage information for details:</p> <pre><code>$ sh_quota -h\nsh_quota: display user and group quota information for all accessible filesystems.\n\nUsage: sh_quota [OPTIONS]\n    Optional arguments:\n        -f FILESYSTEM   only display quota information for FILESYSTEM.\n                        For instance: \"-f $HOME\"\n        -g GROUP        for users with multiple group memberships, display\n                        group quotas in the context of that group\n        -n              don't display headers\n        -j              JSON output (implies -n)\n</code></pre>"},{"location":"docs/storage/#examples","title":"Examples","text":"<p>For instance, to only display your quota usage on <code>$HOME</code>:</p> <pre><code>$ sh_quota -f HOME\n</code></pre> <p>If you belong to multiple groups, you can display the group quotas for your secondary groups with:</p> <pre><code>$ sh_quota -g &lt;group_name&gt;\n</code></pre> <p>And finally, for great output control, an option to display quota usage in JSON is provided via the <code>-j</code> option:</p> <pre><code>$ sh_quota -f SCRATCH -j\n{\n  \"SCRATCH\": {\n    \"quotas\": {\n      \"type\": \"user\",\n      \"blocks\": {\n        \"usage\": \"47476660\",\n        \"limit\": \"21474836480\"\n      },\n      \"inodes\": {\n        \"usage\": \"97794\",\n        \"limit\": \"20000000\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"docs/storage/#locating-large-directories","title":"Locating large directories","text":"<p>It's not always easy to identify files and directories that take the most space when getting close to the quota limits. Some tools can help with that.</p> <ul> <li> <p><code>du</code> can be used to display the volume used by files and   directories, in a given folder:</p> <pre><code>$ cd mydir/\n$ du --human-readable --summarize  *\n101M    dir\n2.0M    file\n</code></pre> <p>Note</p> <p><code>du</code> will ignore hidden entries (everything that starts with a dot (<code>.</code>)). So when using it in your <code>$HOME</code> directory, it will skip things like <code>.cache</code> or <code>.conda</code>, which can contain significant volumes.</p> </li> <li> <p><code>ncdu</code> is an interactive disk usage analyzer, that generates   visual representation of the volume (and inode count) for directories. To run   it, you need to load the <code>ncdu</code> module, and then run it on your directory of   choice:</p> <pre><code>$ ml system ncdu\n$ ncdu $HOME\n</code></pre> <p>For very large directories, running <code>ncdu</code> in an interactive shell on a compute node is recommended, via <code>sh_dev</code>.</p> <p>You'll been there presented with an interactive file browser, showing information about the volume used by your directories, which should make easy to pinpoint where most space is used.</p> </li> </ul> <p>Info</p> <p>Note that any tool you use to view directory contents will only be able to show files that your user account has read access to. So on group-shared spaces, if you see a major difference between the totals from a tool like <code>ncdu</code> and the information reported by <code>sh_quota</code>, that can be an indicator that one of your group members has restricted permissions on a large number of items in your space.</p>"},{"location":"docs/storage/#where-should-i-store-my-files","title":"Where should I store my files?","text":"<p>Not all filesystems are equivalent</p> <p>Choosing the appropriate storage location for your files is an essential step towards making your utilization of the cluster the most efficient possible. It will make your own experience much smoother, yield better performance for your jobs and simulations, and contribute to make Sherlock a useful and well-functioning resource for everyone.</p> <p>Here is where we recommend storing different types of files and data on Sherlock:</p> <ul> <li>personal scripts, configuration files and software installations \u2192 <code>$HOME</code></li> <li>group-shared scripts, software installations and medium-sized datasets \u2192   <code>$GROUP_HOME</code></li> <li>temporary output of jobs, large checkpoint files \u2192 <code>$SCRATCH</code></li> <li>curated output of job campaigns, large group-shared datasets, archives \u2192 <code>$OAK</code></li> </ul>"},{"location":"docs/storage/#accessing-filesystems","title":"Accessing filesystems","text":""},{"location":"docs/storage/#on-sherlock","title":"On Sherlock","text":"<p>Filesystem environment variables</p> <p>To facilitate access and data management, user and group storage location on Sherlock are identified by a set of environment variables, such as <code>$HOME</code> or <code>$SCRATCH</code>.</p> <p>We strongly recommend using those variables in your scripts rather than explicit paths, to facilitate transition to new systems for instance. By using those environment variables, you'll be sure that your scripts will continue to work even if the underlying filesystem paths change.</p> <p>To see the contents of these variables, you can use the <code>echo</code> command. For instance, to see the absolute path of your $SCRATCH directory:</p> <pre><code>$ echo $SCRATCH\n/scratch/users/kilian\n</code></pre> <p>Or for instance, to move to your group-shared home directory:</p> <pre><code>$ cd $GROUP_HOME\n</code></pre>"},{"location":"docs/storage/#from-other-systems","title":"From other systems","text":"<p>External filesystems cannot be mounted on Sherlock</p> <p>For a variety of security, manageability and technical considerations, we can't mount external filesystems nor data storage systems on Sherlock. The recommended approach is to make Sherlock's data available on external systems.</p> <p>You can mount any of your Sherlock directories on any external system you have access to by using SSHFS. For more details, please refer to the Data Transfer page.</p> <ol> <li> <p>For more information about Oak, its characteristics and cost model,        please see the Oak Service Description page.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/storage/data-protection/","title":"Data protection","text":"<p>Data protection is mostly a task for the user</p> <p>Except for <code>$HOME</code> and <code>$GROUP_HOME</code>, data on Sherlock is not backed up, nor archived. It's up to each user and group to make sure they maintain multiple copies of their data if needed.</p>"},{"location":"docs/storage/data-protection/#snapshots","title":"Snapshots","text":"<p>File system snapshots represent the state of the file system at a particular point in time. They allow accessing the file system contents as it was a different times in the past, and get back data that may have been deleted or modified since the snapshot was taken.</p> <p>Important</p> <p>Snapshots are only available on <code>$HOME</code> and <code>$GROUP_HOME</code>.</p>"},{"location":"docs/storage/data-protection/#accessing-snapshots","title":"Accessing snapshots","text":"<p>Snapshots taken in <code>$HOME</code> and <code>$GROUP_HOME</code> are accessible in a <code>.snapshot</code> directory at any level of the hierarchy. Those <code>.snapshot</code> directories don't appear when listing directory contents with <code>ls</code>, but they can be listed explicitly or accessed with <code>cd</code>:</p> <pre><code>$ cd $HOME\n$ ls -ald .snapshot/users*\n[...]\ndrwx------ 118 sunetid group  6680 Jul 21 11:16 .snapshot/users.daily.20170721\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.daily.20170722\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.daily.20170723\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/users.daily.20170724\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/users.daily.latest\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-16:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-17:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-18:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-19:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-20:00\n[...]\n$ cd .snapshot/users.daily.latest\n</code></pre> <p>For instance:</p> <ul> <li>the <code>$HOME/.snapshot/users.daily.latest</code> directory is the latest daily   snapshot available, and stores the contents of the $HOME directory as they   were when the last daily snapshot was taken,</li> <li>the <code>$HOME/foo/.snapshot/users.hourly.20170722-18:00</code> can be used to retrieve   the contents of the <code>$HOME/foo</code> directory as it was at 6pm on July 22th,   2017.</li> </ul>"},{"location":"docs/storage/data-protection/#restoring-from-a-snapshot","title":"Restoring from a snapshot","text":"<p>If you deleted a file or modified it and want to restore an earlier version, you can simply copy the file from its saved version in the appropriate snapshot.</p> <p>Examples:</p> <ul> <li> <p>to restore the last known version of <code>$HOME/foo/bar</code>:</p> <pre><code>$ cp $HOME/foo/.snapshot/users.hourly.latest/bar $HOME/foo/bar\n</code></pre> <p>or</p> <pre><code>$ cp $HOME/.snapshot/foo/users.hourly.latest/bar $HOME/foo/bar\n</code></pre> <p>(both commands are equivalent)</p> </li> <li> <p>to restore your <code>~/.bashrc</code> file from 2 days ago:</p> <pre><code>$ SNAP_DATE=$(date +%Y%m%d -d \"2 days ago\")\n$ cp $HOME/.snapshot/users.daily.${SNAP_DATE}/.bashrc $HOME/.bashrc\n</code></pre> </li> </ul>"},{"location":"docs/storage/data-protection/#snapshot-policy","title":"Snapshot policy","text":"<p>The current<sup>1</sup> policy is to take snapshots on an hourly, daily and weekly basis.  Older snapshots automatically expire after their retention period. The snapshot policy applies to both <code>$HOME</code> and <code>$GROUP_HOME</code> storage spaces.</p> Snapshot frequency Retention period Number of snapshots hourly 2 days 48 daily 1 week 7 weekly 1 month 4 <p>The shortest interval between snapshots is an hour. That means that if you create a file and then delete it within the hour, it won't appear in snapshots, and you won't be able to restore it.</p> <p>If a file exists for more than an hour, and is then deleted, it will be present in the hourly snapshots for the next 48 hours, and you'll be able to retrieve it during that period. Similarly, if a file exists for more than a day, it could be restored for up to 7 days.</p> <p>Snapshots don't count towards your quota.</p> <p>Snapshots, as well as the entire filesystem, are replicated to an off-site system, to ensure that data could be retrieved even in case of a catastrophic failure of the whole system or datacenter-level disaster.</p>"},{"location":"docs/storage/data-protection/#backups","title":"Backups","text":"<p>Although Stanford Research Computing doesn't offer any backup service per se, we do provide all the tools required to transfer data in and out of Sherlock.</p> <p>Suggested options to backup your data include:</p> <ul> <li>Oak, Stanford Research Computing's long-term research data storage   service (Recommended)</li> <li>University IT Storage options and backup   services</li> <li>Cloud storage providers (see the Data transfer page for   information about the tools we provide to transfer files to/from the cloud)</li> </ul> <ol> <li> <p>The snapshot policy is subject to change and may be adjusted as        the storage system usage conditions evolve.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/storage/data-sharing/","title":"Data sharing","text":"<p>The following sections present and detail options to share data across users and groups on Sherlock.</p>"},{"location":"docs/storage/data-sharing/#sharing-data-locally-on-sherlock","title":"Sharing data locally on Sherlock","text":""},{"location":"docs/storage/data-sharing/#traditional-unix-permissions","title":"Traditional Unix permissions","text":"<p>Standard Unix file permissions are supported on Sherlock and provide read, write and execute permissions for the three distinct access classes.</p> <p>The access classes are defined as follows:</p> <ul> <li>Files and directories are owned by a user. The owner determines the file's   user class. Distinct permissions apply to the owner.</li> <li>Files and directories are assigned a group, which define the file's group   class. Distinct permissions apply to members of the file's group. The owner   may be a member of the file's group.</li> <li>Users who are not the owner, nor a member of the group, comprise a file's   others class. Distinct permissions apply to others.</li> </ul> <p>The following permissions apply to each class:</p> <ul> <li>The <code>read</code> permission grants the ability to read a file. When set for a   directory, this permission grants the ability to read the names of files in   the directory, but not to find out any further information about them such as   contents, file type, size, ownership, permissions.</li> <li>The <code>write</code> permission grants the ability to modify a file. When set for a   directory, this permission grants the ability to modify entries in the   directory. This includes creating files, deleting files, and renaming files.</li> <li>The <code>execute</code> permission grants the ability to execute a file. This   permission must be set for executable programs, including shell scripts, in   order to allow the operating system to run them. When set for a directory,   this permission grants the ability to access file contents and   meta-information if its name is known, but not list files inside the   directory, unless read is set also.</li> </ul> <p>Shared directories traversal</p> <p>If you need to give access to one of your files to another user, they will at least need execute permission on each directory within the path to that file.</p> <p>The effective permissions are determined based on the first class the user falls within in the order of user, group then others. For example, the user who is the owner of the file will have the permissions given to the user class regardless of the permissions assigned to the group class or others class.</p> <p>While traditional Unix permissions are sufficient in most cases to share files with all the users within the same group, they are not enough to share files with a specific subset of users, or with users from other groups. Access Control Lists (ACLs) can be used for that purpose.</p> <p>There are two type of ACLs supported on Sherlock depending on the underlying filesystem:</p> Type Filesystems NFSv4 ACLs <code>$HOME</code> and <code>$GROUP_HOME</code> POSIX ACLs <code>$SCRATCH</code>, <code>$GROUP_SCRATCH</code>, <code>$L_SCRATCH</code> and <code>$OAK</code>"},{"location":"docs/storage/data-sharing/#posix-acls","title":"POSIX ACLs","text":"<p>POSIX ACLs allows you to grant or deny access to files and directories for different users (or groups), independently of the file owner or group.</p> <p>Two types of POSIX ACLs can be defined:</p> <ul> <li>Access ACLs: grant permission for a specific file or directory.</li> <li>Default ACLs: allow to set a default set of ACLs that will be applied to   any file or directory without any already defined ACL. Can only be set on   directories.</li> </ul> <p>ACLs are set with the <code>setfacl</code> command, and displayed with <code>getfacl</code>. For more details and examples, please refer to this documentation.</p> <p>In the example below, we allow two users to access a restricted directory located at <code>$GROUP_SCRATCH/restricted-dir/</code>:</p> <pre><code>$ cd $GROUP_SCRATCH\n\n### Create new directory\n$ mkdir restricted-dir\n\n### Remove 'group' and 'other' access\n$ chmod g-rwx,o-rwx restricted-dir\n\n### Give user bob read and traversal permissions to the directory\n$ setfacl -m u:bob:rX restricted-dir\n\n### Use default ACLs (-d) to give user bob read access to all new\n### files and sub-directories that will be created in \"restricted-dir\"\n$ setfacl -d -m u:bob:rX restricted-dir\n\n### Give user alice read, write and traversal permissions for the directory\n$ setfacl -m u:alice:rwX restricted-dir\n\n### Use default ACLs (-d) to give user alice read and write access to all\n### new files and sub-directories\n$ setfacl -d -m u:alice:rwX restricted-dir\n\n### Show ACLs\n$ getfacl restricted-dir\n# file: restricted-dir/\n# owner: joe\n# group: grp\n# flags: -s-\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\ndefault:user::rwx\ndefault:user:alice:rwx\ndefault:user:bob:r-x\ndefault:group::---\ndefault:mask::rwx\ndefault:other::---\n</code></pre> <p>Default permissions on <code>$GROUP_SCRATCH</code></p> <p>By default, the Unix permissions on the root directory <code>$GROUP_SCRATCH</code> don't allow read nor traversal access for others (ie. any user not part of your PI group). If you need to share files with users outside of your own group, please contact us so we can set the appropriate permissions on your folder.</p> <p>For <code>$SCRATCH</code>, you're the owner of the directory and so you can change the permissions yourself.</p>"},{"location":"docs/storage/data-sharing/#nfsv4-acls","title":"NFSv4 ACLs","text":"<p><code>$HOME</code> and <code>$GROUP_HOME</code> also allow setting ACLs, albeit with different syntax and semantics than POSIX ACLs. The principle is very similar, though.</p> <p>An ACL in NFSv4 is a list of rules setting permissions on files or directories. A permission rule, or Access Control Entry (ACE), is of the form <code>type:flags:principle:permissions</code>.</p> <p>Commonly used entries for these fields are:</p> <ul> <li>type: <code>A</code> (allow) or <code>D</code> (deny)</li> <li>flags: <code>g</code> (group), <code>d</code> (directory-inherit), <code>f</code> (file-inherit), <code>n</code>   (no-propagate-inherit), or <code>i</code> (inherit-only)</li> <li>principle:  a named user (<code>user@sherlock</code>), a group, or one of three   special principles: <code>OWNER@</code>, <code>GROUP@</code>, and <code>EVERYONE@</code>.</li> <li>permissions: there are 14 permission characters, as well as the shortcuts   <code>R</code>, <code>W</code>, and <code>X</code>. Here is a list of possible permissions that can be   included in the permissions field (options are Case Sensitive) <li><code>r</code> read-data (files) / list-directory (directories)</li> <li><code>w</code> write-data (files) / create-file (directories)</li> <li><code>x</code> execute (files) / change-directory (directories)</li> <li><code>a</code> append-data (files) / create-subdirectory (directories)</li> <li><code>t</code> read-attributes: read the attributes of the file/directory.</li> <li><code>T</code> write-attributes: write the attributes of the file/directory.</li> <li><code>n</code> read-named-attributes: read the named attributes of the       file/directory.</li> <li><code>N</code> write-named-attributes: write the named attributes of the       file/directory.</li> <li><code>c</code> read-ACL: read the file/directory NFSv4 ACL.</li> <li><code>C</code> write-ACL: write the file/directory NFSv4 ACL.</li> <li><code>o</code> write-owner: change ownership of the file/directory.</li> <li><code>y</code> synchronize: allow clients to use synchronous I/O with the server.</li> <li><code>d</code> delete: delete the file/directory. Some servers will allow a delete       to occur if either this permission is set in the file/directory or if the       delete-child permission is set in its parent directory.</li> <li><code>D</code> delete-child: remove a file or subdirectory from within the given       directory (directories only)</li> <p></p> <p>A comprehensive listing of allowable field strings is given in the manual page nfs4_acl(5)</p> <p>To see what permissions are set on a particular file, use the <code>nfs4_getfacl</code> command. For example, newly created <code>file1</code> may have default permissions listed by <code>ls -l</code> as <code>-rw-r\u2014r\u2014</code>. Listing the permissions with <code>nfs4_getfacl</code> would display the following:</p> <pre><code>$ nfs4_getfacl file1\nA::OWNER@:rwatTnNcCoy\nA:g:GROUP@:rtncy\nA::EVERYONE@:rtncy\n</code></pre> <p>To set permissions on a file, use the <code>nfs4_setfacl</code> command. For convenience, NFSv4 provides the shortcuts <code>R</code>, <code>W</code> and <code>X</code> for setting read, write, and execute permissions. For example, to add write permissions for the current group on <code>file1</code>, use <code>nfs4_setfacl</code> with the <code>-a</code> switch:</p> <pre><code>$ nfs4_setfacl -a A::GROUP@:W file1\n</code></pre> <p>This command switched the <code>GROUP@</code> permission field from <code>rtncy</code> to <code>rwatTnNcCoy</code>.  However, be aware that NFSv4 file permission shortcuts have a different meanings than the traditional Unix <code>r</code>, <code>w</code>, and <code>x</code>. For example issuing <code>chmod g+w file1</code> will set <code>GROUP@</code> to <code>rwatncy</code>.</p> <p>Although the shortcut permissions can be handy, often rules need to be more customized. Use <code>nfs4_setfacl -e file1</code>  to open the ACL for <code>file1</code> in a text editor.</p> <p>Access Control Entries allow more fine grained control over file and directory permissions than does the <code>chmod</code> command. For example, if user <code>joe</code> wants to give read, write and traverse permissions to <code>jack</code> for her directory <code>private</code>, she would issue:</p> <pre><code>$ nfs4_setfacl -R -a A::jack@sherlock:RWX private/\n</code></pre> <p>The <code>-R</code> switch recursively applies the rule to the files and directories within <code>private/</code> as well.</p> <p>To allow <code>jack</code> to create files and subdirectories within <code>private/</code> with the permissions as granted above, inheritance rules need to be applied.</p> <pre><code>$ nfs4_setfacl -R -a A:fd:jack@sherlock:RWX private/\n</code></pre> <p>By default, each permission is in the Deny state and an ACE is required to explicitly allow a permission. However, be aware that a server may silently override a users ACE, usually to a less permissive setting.</p> <p>For complete documentation and examples on using NFSv4 ACLs, please see the manual page at nfs4_acl(5).</p> <p>Default permissions on <code>$GROUP_HOME</code></p> <p>By default, the Unix permissions on the root directory <code>$GROUP_HOME</code> don't allow read nor traversal access for others (ie. any user not part of your PI group). If you need to share files with users outside of your own group, please contact us so we can set the appropriate permissions on your folder.</p> <p>For <code>$HOME</code>, you're the owner of the directory and so you can change the permissions yourself.</p>"},{"location":"docs/storage/data-sharing/#sharing-data-outside-of-sherlock","title":"Sharing data outside of Sherlock","text":"<p>If you'd like to share data stored on Sherlock with external collaborators, there are two possibilities:</p> <ol> <li> <p>sponsor a SUNet ID<sup>1</sup> for these      collaborators, and contact us us to create a account for      them on Sherlock.  This will grant them access to your resources on      Sherlock (compute as well as storage) and give them access to your group      shared files, like any other user in your group.</p> </li> <li> <p>if you don't want to grant full access to your Sherlock resources to your      external collaborators, you can use the Globus data      sharing feature. This won't require your      collaborators to get Stanford accounts, and will allow easy sharing of      the datasets of your choice.</p> <p>Globus Sharing is only available through the Oak endpoint</p> <p>Globus Sharing is only available on <code>$OAK</code>, using the Oak Globus Endpoint <sup>2</sup> (<code>srcc#oak</code>).</p> <p>For complete details about sharing data with Globus, please see the Globus  documentation at https://docs.globus.org/how-to/share-files/</p> </li> </ol> <ol> <li> <p>a base-level SUNet ID (free) is sufficient to get an                 account on Sherlock. For more details about SUNet ID levels                 and associated services, please see the Stanford UIT SUNet                 IDs page.\u00a0\u21a9</p> </li> <li> <p>SUNet ID required\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/storage/data-transfer/","title":"Data transfer","text":"","tags":["connection"]},{"location":"docs/storage/data-transfer/#transfer-protocols","title":"Transfer protocols","text":"<p>A number of methods allow transferring data in/out of Sherlock. For most cases, we recommend using SSH-based file transfer commands, such as <code>scp</code>, <code>sftp</code>, or <code>rsync</code>.  They will provide the best performance for data transfers from and to campus.</p> <p>For large transfers, using DTNs is recommended</p> <p>Most casual data transfers could be done through the login nodes, by pointing your transfer tool to <code>login.sherlock.stanford.edu</code>. But because of resource limits on the login nodes, larger transfer may not work as expected.</p> <p>For transferring large amounts of data, Sherlock features a specific Data Transfer Node, with dedicated bandwidth, as well as a managed Globus endpoint, that can be used for scheduled, unattended data transfers.</p> <p>We also provide tools on Sherlock to transfer data to various Cloud providers, such as AWS, Google Drive, Dropbox, Box, etc.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#prerequisites","title":"Prerequisites","text":"<p>Most of the commands detailed below require a terminal and an SSH client<sup>1</sup> on your local machine to launch commands.</p> <p>You'll need to start a terminal and type the given example commands at the prompt, omitting the initial <code>$</code> character (it just indicates a command prompt, and then should not be typed in).</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#host-keys","title":"Host keys","text":"<p>Upon your very first connection to Sherlock, you will be greeted by a warning such as :</p> <pre><code>The authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>The same warning will be displayed if your try to connect to one of the Data Transfer Node (DTN):</p> <pre><code>The authenticity of host 'dtn.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>This warning is normal: your SSH client warns you that it is the first time it sees that new computer. To make sure you are actually connecting to the right machine, you should compare the ECDSA key fingerprint shown in the message with one of the fingerprints below:</p> Key type Key Fingerprint RSA <code>SHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA</code>legacy format: <code>f5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41</code> ECDSA <code>SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg</code>legacy format: <code>70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64</code> <p>If they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store that key and will verify it for every subsequent SSH connection, to make sure that the server you're connecting to is indeed Sherlock.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#host-keys-warning","title":"Host keys warning","text":"<p>If you've connected to Sherlock 1.0 before, there's a good chance the Sherlock 1.0 keys were stored by your local SSH client. In that case, when connecting to Sherlock 2.0 using the <code>sherlock.stanford.edu</code> alias, you will be presented with the following message:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: POSSIBLE DNS SPOOFING DETECTED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe RSA host key for sherlock.stanford.edu has changed, and the key for\nthe corresponding IP address 171.66.97.101 is unknown. This could\neither mean that DNS SPOOFING is happening or the IP address for the\nhost and its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle\nattack)!  It is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA.\nPlease contact your system administrator.\n</code></pre> <p>You can just check that the SHA256 key listed in that warning message correctly matches the one listed in the table above, and if that's the case, you can safely remove the <code>sherlock.stanford.edu</code> entry from your <code>~/.ssh/known_hosts</code> file with the following command on your local machine:</p> <pre><code>$ ssh-keygen -R sherlock.stanford.edu\n</code></pre> <p>and then connect again. You'll see the first-connection prompt mentioned above, and your SSH client will store the new keys for future connections.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#ssh-based-protocols","title":"SSH-based protocols","text":"<p>User name</p> <p>In all the examples below, you'll need to replace <code>&lt;sunetid&gt;</code> by your actual SUNet ID. If you happen to use the same login name on your local machine, you can omit it.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#scp-secure-copy","title":"SCP (Secure Copy)","text":"<p>The easiest command to use to transfer files to/from Sherlock is <code>scp</code>. It works like the <code>cp</code> command, except it can work over the network to copy files from one computer to another, using the secure SSH protocol.</p> <p>The general syntax to copy a file to a remote server is: <pre><code>$ scp &lt;source_file_path&gt; &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_path&gt;'\n</code></pre></p> <p>For instance, the following command will copy the file named <code>foo</code> from your local machine to your home directory on Sherlock: <pre><code>$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:\n</code></pre> Note the <code>:</code> character, that separates the hostname from the destination path. Here, the destination path is empty, which will instruct scp to copy the file in your home directory.</p> <p>You can copy <code>foo</code> under a different name, or to another directory, with the following commands: <pre><code>$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:bar\n$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:~/subdir/baz\n</code></pre></p> <p>To copy back files from Sherlock to your local machine, you just need to reverse the order of the arguments: <pre><code>$ scp &lt;sunetid&gt;@login.sherlock.stanford.edu:foo local_foo\n</code></pre></p> <p>And finally, <code>scp</code> also support recursive copying of directories, with the <code>-r</code> option: <pre><code>$ scp -r dir/ &lt;sunetid&gt;@login.sherlock.stanford.edu:dir/\n</code></pre> This will copy the <code>dir/</code> directory and all of its contents in your home directory on Sherlock.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#sftp-secure-file-transfer-protocol","title":"SFTP (Secure File Transfer Protocol)","text":"<p>SFTP clients are interactive file transfer programs, similar to FTP, which perform all operations over an encrypted transport.</p> <p>A variety of graphical SFTP clients are available for different OSes:</p> <ul> <li>WinSCP </li> <li>SecureFX ,</li> <li>Fetch<sup>2</sup> </li> <li>CyberDuck </li> </ul> <p>When setting up your connection to Sherlock in the above programs, use the following information:</p> <pre><code>Hostname: login.sherlock.stanford.edu\nPort:     22\nUsername: SUNet ID\nPassword: SUNet ID password\n</code></pre> <p>OpenSSH also provides a command-line SFTP client, originally named <code>sftp</code>.</p> <p>To log in to Sherlock: <pre><code>$ sftp &lt;sunetid&gt;@login.sherlock.stanford.edu\nConnected to login.sherlock.stanford.edu.\nsftp&gt;\n</code></pre> For more information about using the command-line SFTP client, you can refer to this tutorial for more details and examples.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#rsync","title":"<code>rsync</code>","text":"<p>If you have complex hierarchies of files to transfer, or if you need to synchronize a set of files and directories between your local machine and Sherlock, <code>rsync</code> will be the best tool for the job. It will efficiently transfer and synchronize files across systems, by checking the timestamp and size of files. Which means that it won't re-transfer files that have not changed since the last transfer, and will complete faster.</p> <p>For instance, to transfer the whole <code>~/data/</code> folder tree from your local machine to your home directory on Sherlock, you can use the following command: <pre><code>$ rsync -a ~/data/ &lt;sunetid&gt;@login.sherlock.stanford.edu:data/\n</code></pre> Note the slash (<code>/</code>) at the end of the directories name,  which is important to instruct <code>rsync</code> to synchronize the whole directories.</p> <p>To get more information about the transfer rate and follow its progress, you can use additional options: <pre><code>$ rsync -avP ~/data/ &lt;sunetid&gt;@login.sherlock.stanford.edu:data/\nsending incremental file list\n./\nfile1\n      1,755,049 100%    2.01MB/s    0:00:00 (xfr#2, to-chk=226/240)\nfile2\n      2,543,699 100%    2.48MB/s    0:00:00 (xfr#3, to-chk=225/240)\nfile3\n     34,930,688  19%   72.62MB/s    0:00:08\n\n[...]\n</code></pre> For more information about using the <code>rsync</code>, you can refer to this tutorial for more details and examples.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#sshfs","title":"SSHFS","text":"<p>Sometimes, moving files in and out of the cluster, and maintaining two copies of each of the files you work on, both on your local machine and on Sherlock, may be painful. Fortunately, Sherlock offers the ability to mount any of its filesystems to your local machine, using a secure and encrypted connection.</p> <p>With SSHFS, a FUSE-based filesystem implementation used to mount remote SSH-accessible filesystems, you can access your files on Sherlock as if they were locally stored on your own computer.</p> <p>This comes particularly handy when you need to access those files from an application that is not available on Sherlock, but that you already use or can install on your local machine. Like a data processing program that you have licensed for your own computer but can't use on Sherlock, a specific text editor that only runs on macOS, or any data-intensive 3D rendering software that wouldn't work comfortably enough over a forwarded X11 connection.</p> <p>SSHFS is available for Linux , macOS , and Windows .</p> <p>SSHFS on macOS</p> <p>SSHFS on macOS is known to try to automatically reconnect filesystem mounts after resuming from sleep or suspend, even without any valid credentials.  As a result, it will generate a lot of failed connection attempts and likely make your IP address blacklisted on login nodes.</p> <p>Make sure to unmount your SSHFS drives before putting your macOS system to sleep to avoid this situation.</p> <p>The following option could also be useful to avoid some permission issues: <code>-o defer_permissions</code></p> <p>For instance, on a Linux machine with SSHFS installed, you could mount your Sherlock home directory via a Sherlock DTN with the following commands:</p> <pre><code>$ mkdir ~/sherlock_home\n$ sshfs &lt;sunetid&gt;@dtn.sherlock.stanford.edu:./ ~/sherlock_home\n</code></pre> <p>Using DTNs for data transfer</p> <p>Using the Sherlock DTNs instead of login nodes will ensure optimal performance for data transfers. Login nodes only have limited resources, that could limit data transfer rates or disconnect during long data transfers.</p> <p>And to unmount it: <pre><code>$ umount ~/sherlock_home\n</code></pre></p> <p>On Windows, once SSHFS is installed, you can mount the <code>$SCRATCH</code> filesystem as a network drive through the windows file explorer.  To do this, go to \"This PC\", right-click in the \"Network Locations\" section of the window and select \"Add a Network Drive\".  Then, in the \"Add Network Location Wizard\", you would use the following network address:</p> <pre><code>\\\\sshfs\\&lt;sunetid&gt;@dtn.sherlock.stanford.edu\n</code></pre> <p>This will mount the <code>$SCRATCH</code> partition as a network drive on your PC.</p> <p>For more information about using SSHFS on your local machine, you can refer to this tutorial for more details and examples.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#globus","title":"Globus","text":"<p>Globus improves SSH-based file transfer protocols by providing the following features:</p> <ul> <li>automates large data transfers,</li> <li>handles transient errors, and can resume failed transfers,</li> <li>simplifies the implementation of high-performance transfers between computing   centers.</li> </ul> <p>Globus is a Software as a Service (SaaS) system that provides end-users with a browser interface to initiate data transfers between endpoints. Globus allows users to \"drag and drop\" files from one endpoint to another. Endpoints are terminals for data; they can be laptops or supercomputers, and anything in between. The Globus web service negotiates, monitors, and optimizes transfers through firewalls and across network address translation (NAT). Under certain circumstances, with high performance hardware transfer rates exceeding 1 GB/s are possible. For more information about Globus, please see the Globus documentation.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#authentication","title":"Authentication","text":"<p>To use Globus, you will first need to authenticate at Globus.org. You can either sign up for a Globus account, or use your SUNet ID account for authentication to Globus (which will be required to authenticate to the Sherlock endpoint).</p> <p>To use your SUNet ID, choose \"Stanford University\" from the drop down menu at the Login page and follow the instructions from there.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#transfer","title":"Transfer","text":"<p>Endpoint name</p> <p>The Globus endpoint name for Sherlock is <code>SRCC Sherlock</code>.</p> <p>Oak endpoint</p> <p>The Sherlock endpoint only provides access to Sherlock-specific file systems (<code>$HOME</code>, <code>$GROUP_HOME</code>, <code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code>). Oak features its own Globus endpoint: <code>SRCC Oak</code>.</p> <p>You can use Globus to transfer data between your local workstation (e.g., your laptop or desktop) and Sherlock. In this workflow, you configure your local workstation as a Globus endpoint by installing the Globus Connect software.</p> <ol> <li>Log in to Globus.org</li> <li>Use the Manage Endpoints interface to \"add Globus    Connect Personal\" as an endpoint (you'll need to install Globus Connect    Personal on your local machine)</li> <li>Transfer Files, using your new workstation endpoint    for one side of the transfer, and the Sherlock endpoint (<code>SRCC Sherlock</code>) on    the other side.</li> </ol> <p>You can also transfer data between two remote endpoints, by choosing another endpoint you have access to instead of your local machine.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#cli-and-api","title":"CLI and API","text":"<p>Globus also provides a command-line interface (CLI) and application programming interface (API) as alternatives to its web interface.</p> <p>For more information about the API, please see the Globus API documentation for more details.</p> <p>For more information about the CLI, please see the Globus CLI documentation and Globus CLI quick start. Note that the Globus CLI is available through the module system on Sherlock:</p> <pre><code>$ module load system py-globus-cli\n$ globus login\n# follow instructions to get set up\n</code></pre> <p>Once you've authorized the application, you can use the <code>globus</code> CLI to copy files in between endpoints and collections that you have access to. Endpoints and collections are identified by their unique UUID4 identifiers, which are viewable through the Globus web app. The CLI will step you through any additional authorizations required for you to access the endpoints or collections.</p> <p>For example, to asynchronously copy files between Sherlock and Oak (if that you have already been allocated Oak storage):</p> <pre><code>$ GLOBUS_SHERLOCK_UUID=\"6881ae2e-db26-11e5-9772-22000b9da45e\"\n$ GLOBUS_OAK_UUID=\"8b3a8b64-d4ab-4551-b37e-ca0092f769a7\"\n$ globus transfer --recursive \\\n    \"$GLOBUS_SHERLOCK_UUID:$SCRATCH/my-interesting-project\" \\\n    \"$GLOBUS_OAK_UUID:$OAK/my-interesting-project-copy\"\n</code></pre>","tags":["connection"]},{"location":"docs/storage/data-transfer/#data-transfer-nodes-dtns","title":"Data Transfer Nodes (DTNs)","text":"<p>No shell</p> <p>The DTNs don't provide any interactive shell, so connecting via SSH directly won't work. It will only accept <code>scp</code>, <code>sftp</code>, <code>rsync</code> of <code>bbcp</code> connections.</p> <p>A pool of dedicated Data Transfer Nodes is available on Sherlock, to provide exclusive resources for large-scale data transfers.</p> <p>The main benefit of using it is that transfer tasks can't be disrupted by other users interactive tasks or filesystem access and I/O-related workloads on the login nodes.</p> <p>By using the Sherlock DTNs, you'll make sure that your data flows will go through a computer whose sole purpose is to move data around.</p> <p>It supports:</p> <ul> <li>SSH-based protocols (such as the ones described   above)</li> <li><code>bbcp</code></li> <li>Globus</li> </ul> <p>To transfer files via the DTNs, simply use <code>dtn.sherlock.stanford.edu</code> as a remote server host name. For instance:</p> <pre><code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:~/foo\n</code></pre> <p>$HOME on DTNs</p> <p>One important difference to keep in mind when transferring files through the Sherlock DTNs is that the default destination path for files, unless specified, is the user <code>$SCRATCH</code> directory, not <code>$HOME</code>.</p> <p>That means that the following command: <pre><code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:\n</code></pre> will create the <code>foo</code> file in  <code>$SCRATCH/foo</code>, and not in <code>$HOME/foo</code>.</p> <p>You can transfer file to your <code>$HOME</code> directory via the DTNs by specifying the full path as the destination: <code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:$HOME/foo</code></p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#cloud-storage","title":"Cloud storage","text":"<p>If you need to backup some of your Sherlock files to cloud-based storage services, we also provide a set of utilities that can help.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#google-drive","title":"Google Drive","text":"<p>Google Drive storage for Stanford users</p> <p>For more information about using Google Drive at Stanford, please see the University IT Google Drive page.</p> <p>We provide the <code>rclone</code> tool on Sherlock to interact with Google Drive. You'll just need to load the <code>rclone</code> module to be able to use it to move your files from/to Google Drive:</p> <pre><code>$ module load system rclone\n$ rclone --help\n</code></pre> <p>This tutorial provides an example of transferring files between Google Drive and Oak storage.</p> <p>The Globus CLI (see above) can also be used to copy files from Sherlock to Stanford's Google Drive.</p>","tags":["connection"]},{"location":"docs/storage/data-transfer/#aws","title":"AWS","text":"<p>You can also access AWS storage from the Sherlock command line with the AWS Command Line Interface:</p> <pre><code>$ module load system aws-cli\n$ aws help\n</code></pre>","tags":["connection"]},{"location":"docs/storage/data-transfer/#other-services","title":"Other services","text":"<p>If you need to access other cloud storage services, you can use <code>rclone</code>: it can be used to sync files and directories to and from Google Drive, Amazon S3, Box, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft OneDrive and many more.</p> <pre><code>$ ml load system rclone\n$ rclone -h\n</code></pre> <p>For more details about how to use <code>rclone</code>, please see the official documentation.</p> <ol> <li> <p>For more details, see the SSH clients page.\u00a0\u21a9</p> </li> <li> <p>Fetch is a commercial program, and is available as part of   the Essential Stanford Software bundle.\u00a0\u21a9</p> </li> </ol>","tags":["connection"]},{"location":"docs/storage/filesystems/","title":"Filesystems","text":"<p>The following sections describe the characteristics and best uses of each of the Sherlock's filesystems.</p>"},{"location":"docs/storage/filesystems/#home","title":"<code>$HOME</code>","text":"<p>Summary</p> <p><code>$HOME</code> is your home directory. It's the best place to keep your code and important data as it provides snapshots and off-site replication. It is not meant to host data that will be actively read and written to by compute jobs.</p> Characteristics Type high speed, distributed NFS file system Quota 15 GB for the whole <code>$HOME</code> directory Snapshots yes (cf. Snapshots) for more info) Backups off-site replication Purge policy not purged Scope all login and compute nodes"},{"location":"docs/storage/filesystems/#recommended-usage","title":"Recommended usage","text":"<p><code>$HOME</code> is best suited for personal configuration files, scripts, small reference files or datasets, source code and individual software installation</p> <p>When you log in, the system automatically sets the current working directory to <code>$HOME</code>: it's the location you'll end up when connecting to Sherlock. You can store your source code and build your executables there.</p> <p>We strongly recommend using <code>$HOME</code> to reference your home directory in scripts, rather than its explicit path.</p>"},{"location":"docs/storage/filesystems/#checking-quota-usage","title":"Checking quota usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$HOME</code></p> <pre><code>$ sh_quota -f HOME\n</code></pre> <p>See the Checking Quotas section for more details.</p>"},{"location":"docs/storage/filesystems/#group_home","title":"<code>$GROUP_HOME</code>","text":"<p>Summary</p> <p><code>$GROUP_HOME</code> is your group home directory. It's the best place to keep your group's shared code, software installations and important data as it provides snapshots and off-site replication. It is not meant to host data that will be actively read and written to by compute jobs.</p> <p><code>$HOME</code> and <code>$GROUP_HOME</code> are based on the same physical file system.</p> Characteristics Type high speed, distributed NFS file system Quota 1 TB for the whole <code>$GROUP_HOME</code> directory Snapshots yes (cf. Snapshots) for more info) Backups off-site replication Purge policy not purged Scope all login and compute nodes"},{"location":"docs/storage/filesystems/#recommended-usage_1","title":"Recommended usage","text":"<p><code>$GROUP_HOME</code> is best suited for group shared source code, common software installations, shared data sets and scripts.</p> <p>We strongly recommend using <code>$GROUP_HOME</code> to reference your group home directory in scripts, rather than its explicit path.</p>"},{"location":"docs/storage/filesystems/#checking-quota-usage_1","title":"Checking quota usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$GROUP_HOME</code></p> <pre><code>$ sh_quota -f GROUP_HOME\n</code></pre> <p>See the Checking Quotas section for more details.</p>"},{"location":"docs/storage/filesystems/#scratch","title":"<code>$SCRATCH</code>","text":"<p>Summary</p> <p><code>$SCRATCH</code> is your personal scratch space. It's the best place to store temporary files, such as raw job output, intermediate files, unprocessed results, and so on.</p> <p>Purge policy</p> <p>Files are automatically purged from <code>$SCRATCH</code> after an inactivity period:</p> <ul> <li>files that are not modified after 90 days are automatically   deleted,</li> <li>contents need to change for a file to be considered modified. The <code>touch</code>   command does not modify file contents and thus does not extend a file's   lifetime on the filesystem.</li> </ul> <p><code>$SCRATCH</code> is not meant to store permanent data, and should only be used for data associated with currently running jobs. It's not a target for backups, archived data, etc. See the Expiration Policy section for details.</p> Characteristics Type Parallel, high-performance Lustre file system Quota 100 TB / 20,000,000 inodes<sup>2</sup> Snapshots NO Backups NO Purge policy data not modified in the last 90 days are automatically purged Scope all login and compute nodes"},{"location":"docs/storage/filesystems/#recommended-usage_2","title":"Recommended usage","text":"<p><code>$SCRATCH</code> is best suited for large files, such as raw job output, intermediate job files, unprocessed simulation results, and so on.  This is the recommended location to run jobs from, and to store files that will be read or written to during job execution.</p> <p>Old files are automatically purged on <code>$SCRATCH</code> so users should avoid storing long-term data there.</p> <p>Each compute node has a low latency, high-bandwidth Infiniband link to <code>$SCRATCH</code>. The aggregate bandwidth of the filesystem is about 75GB/s. So any job with high data performance requirements will take advantage from using <code>$SCRATCH</code> for I/O.</p> <p>We strongly recommend using <code>$SCRATCH</code> to reference your scratch directory in scripts, rather than its explicit path.</p>"},{"location":"docs/storage/filesystems/#checking-quota-usage_2","title":"Checking quota usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$SCRATCH</code></p> <pre><code>$ sh_quota -f SCRATCH\n</code></pre> <p>See the Checking Quotas section for more details.</p>"},{"location":"docs/storage/filesystems/#expiration-policy","title":"Expiration policy","text":"<p>Inactive files are automatically purged</p> <p>Files that are not modified in the last 90 days will be automatically deleted from the filesystem.</p> <p>To manage available space and maintain optimal performance for all jobs, all files on <code>$SCRATCH</code> are subject to automatic purges. Meaning that after a period of inactivity, files that are not used anymore will be automatically deleted from the filesystem.</p> <p>File activity is defined based on the last time a file's contents (the actual data in the file) have been modified. Meaning that files whose contents have not been modified in the previous 90 days will be automatically deleted.</p> <p>Each time a file's contents are modified, the expiration countdown is reset, and the file gets another 90-day of lifetime.</p> <p>Metadata changes don't qualify as an update</p> <p>Modifying a file's contents is the only way to reset the expiration countdown and extend the file's lifetime on the filesystem.</p> <p>Metadata modifications such as: reading the file, renaming it, moving it to a different directory, changing its permissions or its ownership, \"touching\" it to update its last modification or access times, won't have any effect on the purge countdown.</p> <p>Purges are based on an internal filesystem property that reflects the last date a file's data has been modified, and which is unfortunately not readily accessible by users.</p> <p>Please note that tools like <code>ls</code> will only display the date of the last metadata<sup>1</sup> modification for a file, which is not necessarily relevant to determine a file's eligibility for deletion. For instance, using the <code>touch</code> command on a file to update its last modification date will only update the metadata, not the data, and as such, will not reset the purge countdown timer.</p> <p>Filesystem purges are a continuous process: they don't run at particular times, but are carried out in a permanent background fashion. Files are not necessarily deleted right away when they become eligible for deletion.  For instance, if you create a file on February 1<sup>st</sup> and don't ever modify it afterwards, it will be automatically become eligible for deletion on May 1<sup>st</sup>, and can be deleted anytime after this date.</p> <p>Empty directory trees that stay devoid of any file for more than 90 days will be automatically cleaned up as well.</p>"},{"location":"docs/storage/filesystems/#group_scratch","title":"<code>$GROUP_SCRATCH</code>","text":"<p><code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code> are based on the same physical file system.</p> <p>Summary</p> <p><code>$GROUP_SCRATCH</code> is your group shared scratch space. It's the best place to store temporary files, such as raw job output, intermediate files, or unprocessed results that need to be shared among users within a group.</p> <p><code>$GROUP_SCRATCH</code> is NOT a backup target</p> <p><code>$GROUP_SCRATCH</code> is not meant to store permanent data, and should only be used for data associated with currently running jobs. It's not a target for backups, archived data, etc.</p> Characteristics Type parallel, high-performance Lustre file system Quota 100 TB / 20,000,000 inodes<sup>2</sup> Snapshots NO Backups NO Purge policy data not accessed in the last 90 days are automatically purged Scope all login and compute nodes"},{"location":"docs/storage/filesystems/#recommended-usage_3","title":"Recommended usage","text":"<p><code>$GROUP_SCRATCH</code> is best suited for large files, such as raw job output, intermediate job files, unprocessed simulation results, and so on.  This is the recommended location to run jobs from, and to store files that will be read or written to during job execution.</p> <p>Old files are automatically purged on <code>$GROUP_SCRATCH</code> so users should avoid storing long-term data there.</p> <p>We strongly recommend using <code>$GROUP_SCRATCH</code> to reference your group scratch directory in scripts, rather than its explicit path.</p>"},{"location":"docs/storage/filesystems/#checking-quota-usage_3","title":"Checking quota usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$GROUP_SCRATCH</code></p> <pre><code>$ sh_quota -f GROUP_SCRATCH\n</code></pre> <p>See the Checking Quotas section for more details.</p>"},{"location":"docs/storage/filesystems/#expiration-policy_1","title":"Expiration policy","text":"<p>As <code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code> are on the same filesystem, the same expiration policy applies to both. Please see the <code>$SCRATCH</code> section above for more details.</p>"},{"location":"docs/storage/filesystems/#l_scratch","title":"<code>$L_SCRATCH</code>","text":"<p>Summary</p> <p><code>$L_SCRATCH</code> is local to each compute node, and could be used to store temporary files for jobs with high IOPS requirements. Files stored in <code>$L_SCRATCH</code> are purged at the end of the job.</p> Characteristics Type local filesystem, specific to each node, based on SSD Quota n/a (usable space limited by the size of the physical storage devices, typically around 150 GB) Snapshots NO Backups NO Purge policy data immediately purged at the end of the job Scope locally on each node, not shared across nodes"},{"location":"docs/storage/filesystems/#recommended-usage_4","title":"Recommended usage","text":"<p><code>$L_SCRATCH</code> is best suited for small temporary files and applications which require low latency and high IOPS levels, typically intermediate job files, checkpoints, dumps of temporary states, etc.</p> <p>Files stored in <code>$L_SCRATCH</code> are local to each node and can't be accessed from other nodes, nor from login nodes.</p> <p>Please note that an additional, job-specific environment variable, <code>$L_SCRATCH_JOB</code>, will be set to a subdirectory of <code>$L_SCRATCH</code> for each job. So, if you have two jobs running on the same compute node, <code>$L_SCRATCH</code> will be the same and accessible from both jobs, while <code>$L_SCRATCH_JOB</code> will be different for each job.</p> <p>For instance, if you have jobs <code>98423</code> and <code>98672</code> running on this same nodes, the variables will be set as follows:</p> Job id <code>$L_SCRATCH</code> <code>L_SCRATCH_JOB</code> <code>98423</code> <code>/lscratch/kilian</code> <code>/lscratch/kilian/98423</code> <code>98672</code> <code>/lscratch/kilian</code> <code>/lscratch/kilian/98672</code> <p>We strongly recommend using <code>$L_SCRATCH</code> to reference your local scratch directory in scripts, rather than its full path.</p>"},{"location":"docs/storage/filesystems/#expiration-policy_2","title":"Expiration policy","text":"<p>All files stored in <code>$L_SCRATCH_JOB</code> are automatically purged at the end of the job, whether the job was successful or not. If you need to conserve files that were generated in <code>$L_SCRATCH_JOB</code> after the job ends, don't forget to add a command at the end of your batch script to copy them to one of the more persistent storage locations, such as <code>$HOME</code> or <code>$SCRATCH</code>.</p> <p>Data stored in <code>$L_SCRATCH</code> will be purged at the end of a job, only if no other job from the same user is still running on the node. Which means that data stored in <code>$L_SCRATCH</code> (but in not <code>$L_SCRATCH_JOB</code>) will persist on the node until the last job from the user terminates.</p>"},{"location":"docs/storage/filesystems/#oak","title":"<code>$OAK</code>","text":"<p>Summary</p> <p><code>$OAK</code> is Stanford Research Computing's research data storage offering. It provides an affordable, longer-term storage option for labs and researchers, and is ideally suited to host large datasets, or curated, post-processed results from job campaigns, as well as final results used for publication.</p> <p>Order <code>$OAK</code></p> <p>Oak storage can be easily ordered online using the Oak Storage Service page.</p> <p><code>$OAK</code> is opt-in and is available as an option on Sherlock. Meaning that only members of groups which have purchased storage on Oak can access this filesystem.</p> <p>For complete details and characteristics, including pricing, please refer to the Oak Storage Service page.</p> Characteristics Type parallel, capacitive Lustre filesystem Quota amount purchased (in 10 TB increments) Snapshots NO Backups optional cloud backup available please contact us for details Purge policy not purged Scope all login and compute nodes also available through gateways outside of Sherlock"},{"location":"docs/storage/filesystems/#recommended-usage_5","title":"Recommended usage","text":"<p><code>$OAK</code> is ideally suited for large shared datasets, archival data and curated, post-processed results   from job campaigns, as well as final results used for publication.</p> <p>Although jobs can directly read and write to <code>$OAK</code> during execution, it is recommended to first stage files from <code>$OAK</code> to <code>$SCRATCH</code> at the beginning of a series of jobs, and save the desired results back from <code>$SCRATCH</code> to <code>$OAK</code> at the end of the job campaign.</p> <p>We strongly recommend using <code>$OAK</code> to reference your group home directory in scripts, rather than its explicit path.</p> <p><code>$OAK</code> is not backed up</p> <p><code>$OAK</code> is not backed up or replicated, by design, and deleted files cannot be recovered. We recommend all researchers to keep an additional copy of their important files (for instance, in Google Drive).</p> <p>Cloud backup option</p> <p>For additional data security, Stanford Research Computing now offers \"cloud backup\" of Oak data as a managed service option. For an additional monthly fee, data on Oak can be backed up to the cloud (researchers are responsible for cloud storage costs). Please contact us if you'd like additional information.</p>"},{"location":"docs/storage/filesystems/#checking-quota-usage_4","title":"Checking quota usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$OAK</code></p> <pre><code>$ sh_quota -f OAK\n</code></pre> <p>See the Checking Quotas section for more details.</p> <ol> <li> <p>Metadata are data such as a file's size, name, path, owner,   permissions, etc.\u00a0\u21a9</p> </li> <li> <p>An inode (index node) is a data structure in a Unix-style file   system that describes a file-system object such as a file or a directory.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"docs/tech/","title":"Technical specifications","text":"","tags":["tech"]},{"location":"docs/tech/#in-a-nutshell","title":"In a nutshell","text":"<p>Sherlock features over 2,000 compute nodes, 73,500+ CPU cores and 1,000+ GPUs, for a total computing power of more than 13.1 Petaflops. That would rank it in the Top500 list of the most powerful supercomputers in the world.</p> <p>The cluster currently extends across 3 Infiniband fabrics (EDR, HDR, NDR). A 9.7 PB parallel, distributed filesystem, delivering over 600 GB/s of I/O bandwidth, provides scratch storage for more than 8,300 users, and 1,200 PI groups.</p>","tags":["tech"]},{"location":"docs/tech/#resources","title":"Resources","text":"<p>The Sherlock cluster has been initiated in January 2014 with a base of freely available computing resources (about 2,000 CPU cores) and the accompanying networking and storage infrastructure (about 1 PB of shared storage).</p> <p>Since then, it's been constantly expanding, spawning multiple cluster generations, with numerous contributions from many research groups on campus.</p> <p>Cluster generations</p> <p>For more information about Sherlock's ongoing evolution and expansion, please see Cluster generations.</p>","tags":["tech"]},{"location":"docs/tech/#interface","title":"Interface","text":"Type Qty Details login nodes 20  <code>sherlock.stanford.edu</code> (load-balanced) data transfer nodes 7  dedicated bandwidth for large data transfers","tags":["tech"]},{"location":"docs/tech/#computing","title":"Computing","text":"<p>Access to computing resources</p> <p>Computing resources marked with  below are freely available to every Sherlock user. Resources marked with  are only accessible to Sherlock owners and their research teams.</p> Type Access Nodes CPU cores Details compute nodes<code>normal</code> partition 218  5,844  - 63x 20  (Intel E5-2640v4), 128 GB RAM, EDR IB- 40x 24  (Intel 5118), 191 GB RAM, EDR IB- 14x 24  (AMD 8224P), 192 GB RAM, NDR IB- 1x 24  (Intel 5118), 384 GB RAM, EDR IB- 28x 32  (AMD 7543), 256 GB RAM, HDR IB- 70x 32  (AMD 7502), 256 GB RAM, HDR IB- 2x 64  (AMD 9384X), 384 GB RAM, NDR IB development nodes<code>dev</code> partition 4  104  - 2x 20  (Intel E5-2640v4), 128 GB RAM, EDR IB- 2x 32  (AMD 7543P), 256 GB RAM, HDR IB- 32x Tesla A30_MIG-1g.6gb  large memory nodes<code>bigmem</code> partition 11  824  - 4x 24  (Intel 5118), 384 GB RAM, EDR IB- 1x 32  (Intel E5-2697Av4), 512 GB RAM, EDR IB- 1x 56  (Intel E5-4650v4), 3072 GB RAM, EDR IB- 1x 64  (AMD 7502), 4096 GB RAM, HDR IB- 1x 64  (Intel 8462Y+), 4096 GB RAM, NDR IB- 2x 128  (AMD 7742), 1024 GB RAM, HDR IB- 1x 256  (AMD 9754), 1536 GB RAM, NDR IB GPU nodes<code>gpu</code> partition 32  1,048  - 1x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla P100 PCIe - 1x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla P40 - 2x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla V100_SXM2 - 1x 24  (Intel 5118), 191 GB RAM, EDR IB- 4x Tesla V100_SXM2 - 2x 24  (Intel 5118), 191 GB RAM, EDR IB- 4x Tesla V100 PCIe - 16x 32  (AMD 7502P), 256 GB RAM, HDR IB- 4x Geforce RTX_2080Ti - 2x 32  (AMD 7502P), 256 GB RAM, HDR IB- 4x Tesla V100S PCIe - 4x 32  (Intel 6426Y), 256 GB RAM, NDR IB- 4x Tesla L40S - 2x 64  (Intel 8462Y+), 1024 GB RAM, NDR IB- 4x Tesla H100_SXM5 - 1x 64  (Intel 8462Y+), 2048 GB RAM, NDR IB- 8x Tesla H100_SXM5  service nodes<code>service</code> partition 6  132  - 5x 20  (Intel E5-2640v4), 128 GB RAM, EDR IB- 1x 32  (AMD 7502), 256 GB RAM, HDR IB privately-owned nodes<code>owners</code> partition 1,774  66,136  48 different node configurations, including GPU and bigmem nodes Total 2,050  73,576  1,068","tags":["tech"]},{"location":"docs/tech/#storage","title":"Storage","text":"<p>More information</p> <p>For more information about storage options on Sherlock, please refer to the Storage section of the documentation.</p> <p>Sherlock is architected around shared storage components, meaning that users can find the same files and directories from all of the Sherlock nodes.</p> <ul> <li>Highly-available NFS filesystem for user and group home directories (with   hourly snapshots and off-site replication)</li> <li>High-performance Lustre scratch filesystem (9.7 PB parallel, distributed filesystem, delivering over 600 GB/s of I/O bandwidth)</li> <li>Direct access to Stanford Research Computing's Oak long-term   research data storage system (86.3 PB)</li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/","title":"Sherlock facts","text":"<p>as of October 2025</p>","tags":["tech"]},{"location":"docs/tech/facts/#users","title":"Users","text":"<ul> <li> <p>8,383 user accounts</p> </li> <li> <p>1,249 PI groups</p> <p>from all Stanford's seven Schools, SLAC, Stanford Institutes, etc.</p> </li> <li> <p>219 owner groups</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#interfaces","title":"Interfaces","text":"<ul> <li> <p>20 login nodes</p> </li> <li> <p>7 data transfer nodes (DTNs)</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#computing","title":"Computing","text":"<ul> <li> <p>13.12 PFLOPs (FP64)</p> <p>42.09 (FP32) PFLOPs</p> </li> <li> <p>73,576 CPU cores</p> <p>7 CPU generations (18 CPU models)</p> </li> <li> <p>1,068 GPUs</p> <p>5 GPU generations (14 GPU models)</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#hardware","title":"Hardware","text":"<ul> <li> <p>2,050 compute nodes</p> <p>25 server models (from 4 different manufacturers)</p> </li> <li> <p>57 racks</p> <p>1,545 rack units</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#energy","title":"Energy","text":"<ul> <li> <p>824.24 kW</p> <p>total power usage</p> </li> <li> <p>97 PDUs</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#storage","title":"Storage","text":"<ul> <li> <p>9.7 PB <code>$SCRATCH</code></p> <p>parallel, distributed filesystem, delivering over 600 GB/s of I/O bandwidth</p> </li> <li> <p>86.3 PB <code>$OAK</code></p> <p>long term research data storage</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#networking","title":"Networking","text":"<ul> <li> <p>140 Infiniband switches</p> <p>across 3 Infiniband fabrics (EDR, HDR, NDR)</p> </li> <li> <p>6,663 Infiniband cables</p> <p>spanning about 31.76 km</p> </li> <li> <p>75 Ethernet switches</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/facts/#scheduler","title":"Scheduler","text":"<ul> <li> <p>198 Slurm partitions</p> </li> <li> <p>62,816 CPU.hours/day</p> <p>over 7 years of computing in a single day</p> </li> <li> <p>$4,197,155 /month</p> <p>to run the same workload on t2.large on-demand cloud instances</p> </li> </ul>","tags":["tech"]},{"location":"docs/tech/status/","title":"Status","text":"<p>Scheduled maintenances</p> <p>Maintenance operations and upgrades are scheduled on Sherlock on a regular basis.  Per the University's Minimum Security policies, we deploy security patches on Sherlock as required for compliance.</p>"},{"location":"docs/tech/status/#components-and-services","title":"Components and services","text":"<p>Sherlock status is </p> <p>For more details about Sherlock components and services, see the status dashboard.</p>"},{"location":"docs/tech/status/#current-usage","title":"Current usage","text":""},{"location":"docs/user-guide/gpu/","title":"GPU nodes","text":"<p>To support the latest computing advancements in many fields of science, Sherlock features a number of compute nodes with GPUs that can be used to run a variety of GPU-accelerated applications. Those nodes are available to everyone, but are a scarce, highly-demanded resource, so getting access to them may require some wait time in queue.</p> <p>Getting your own GPU nodes</p> <p>If you need frequent access to GPU nodes, we recommend considering becoming an owner on Sherlock, so you can have immediate access to your GPU nodes when you need them.</p>"},{"location":"docs/user-guide/gpu/#gpu-nodes","title":"GPU nodes","text":"<p>A limited number of GPU nodes are available in the <code>gpu</code> partition. Anybody running on Sherlock can submit a job there. As owners contribute to expand Sherlock, more GPU nodes are added to the <code>owners</code> partition, for use by PI groups which purchased their own compute nodes.</p> <p>There are a variety of different GPU configuration available in the <code>gpu</code> partition. To see the available GPU types, please see the GPU types section.</p>"},{"location":"docs/user-guide/gpu/#submitting-a-gpu-job","title":"Submitting a GPU job","text":"<p>To submit a GPU job, you'll need to use the <code>--gpus</code> (or <code>-G</code>) option in your batch script or command line submission options.</p> <p>For instance, the following script will request one GPU for two hours in the <code>gpu</code> partition, and run the GPU-enabled version of <code>gromacs</code>:</p> <pre><code>#!/bin/bash\n#SBATCH -p gpu\n#SBATCH -c 10\n#SBATCH -G 1\n\nml load gromacs/2016.3\n\nsrun gmx_gpu ...\n</code></pre> <p>You can also directly run GPU processes on compute nodes with <code>srun</code>. For instance, the following command will display details about the GPUs allocated to your job:</p> <pre><code>$ srun -p gpu --gpus 2 nvidia-smi\nFri Jul 28 12:41:49 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 0000:03:00.0     Off |                    0 |\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P40           On   | 0000:04:00.0     Off |                    0 |\n| N/A   24C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>GPU resources MUST be requested explicitly</p> <p>Jobs will be rejected at submission time if they don't explicitly request GPU resources.</p> <p>The <code>gpu</code> partition only accepts jobs explicitly requesting GPU resources. If they don't, they will be rejected with the following message:</p> <pre><code>$ salloc -p gpu\nsrun: error: Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre>"},{"location":"docs/user-guide/gpu/#interactive-sessions","title":"Interactive sessions","text":"<p>As for any other compute node, you can submit an interactive job and request a shell on a GPU node with the following command:</p> <pre><code>$ salloc -p gpu --gpus 1\nsalloc: job 38068928 queued and waiting for resources\nsalloc: job 38068928 has been allocated resources\n$ nvidia-smi --query-gpu=index,name --format=csv,noheader\n0, Tesla V100-SXM2-16GB\n</code></pre>"},{"location":"docs/user-guide/gpu/#instant-lightweight-gpu-instances","title":"Instant lightweight GPU instances","text":"<p>Given that some tasks don't necessarily require a full-fledged, top-of-the-line GPU, lightweight GPU instances are provided to allow instant access to GPU resources for quick debugging, prototyping or testing jobs.</p> <p>Lightweight GPU instances</p> <p>Lightweight GPU instances leverage NVIDIA\u2019s Multi-Instance GPU (MIG) to provide multiple fully isolated GPU instances on the same physical GPU, each with their own high-bandwidth memory, cache, and compute cores.</p> <p>Those GPU instances are instantly available via the <code>dev</code> partition, and can be requested with the <code>sh_dev</code> command:</p> <pre><code># sh_dev -g 1\n[...]\n[kilian@sh03-17n15 ~] (job 17628407) $ nvidia-smi -L\nGPU 0: NVIDIA A30 (UUID: GPU-ac772b5a-123a-dc76-9480-5998f435fe84)\n  MIG 1g.6gb      Device  0: (UUID: MIG-87e5d835-8046-594a-b237-ccc770b868ef)\n</code></pre> <p>For interactive apps in the Sherlock OnDemand interface, requesting a GPU in the <code>dev</code> partition will initiate an interactive session with access to a lightweight GPU instance.</p> <p></p>"},{"location":"docs/user-guide/gpu/#gpu-types","title":"GPU types","text":"<p>Since Sherlock features many different types of GPUs, each with its own technical characteristics, performance profiles and specificities, you may want to ensure that your job runs on a specific type of GPU.</p> <p>To that end, Slurm allows users to specify constraints when submitting jobs, which will indicate the scheduler that only nodes having features matching the job constraints could be used to satisfy the request. Multiple constraints may be specified and combined with various operators (please refer to the official Slurm documentation for details).</p> <p>The list of available features on compute nodes can be obtained with the <code>node_feat</code><sup>1</sup> command. And more specifically, to list the GPU-related features of nodes in the <code>gpu</code> partition::</p> <pre><code>$ node_feat -p gpu | grep GPU_\nGPU_BRD:TESLA\nGPU_GEN:PSC\nGPU_MEM:16GB\nGPU_MEM:24GB\nGPU_SKU:TESLA_P100_PCIE\nGPU_SKU:TESLA_P40\n</code></pre> <p>You can use <code>node_feat</code> without any option to list all the features of all the nodes in all the partitions. But please note that <code>node_feat</code> will only list the features of nodes from partitions you have access to, so output may vary depending on your group membership.</p> <p>The different characteristics<sup>2</sup> of various GPU types are listed in the following table</p> Slurm\u00a0feature Description Possible values Example job constraint <code>GPU_BRD</code> GPU brand <code>GEFORCE</code>: GeForce / TITAN<code>TESLA</code>: Tesla <code>#SBATCH -C GPU_BRD:TESLA</code> <code>GPU_GEN</code> GPU generation <code>PSC</code>: Pascal<code>MXW</code>: Maxwell <code>#SBATCH -C GPU_GEN:PSC</code> <code>GPU_MEM</code> Amount of GPU memory <code>16GB</code>, <code>24GB</code> <code>#SBATCH -C GPU_MEM:16GB</code> <code>GPU_SKU</code> GPU model <code>TESLA_P100_PCIE</code><code>TESLA_P40</code> <code>#SBATCH -C GPU_SKU:TESLA_P40</code> <p>Depending on the partitions you have access to, more features may be available to be requested in your jobs.</p> <p>For instance, to request a Tesla GPU for you job, you can use the following submission options:</p> <pre><code>$ srun -p gpu -G 1 -C GPU_BRD:TESLA nvidia-smi -L\nGPU 0: Tesla P100-SXM2-16GB (UUID: GPU-4f91f58f-f3ea-d414-d4ce-faf587c5c4d4)\n</code></pre> <p>Unsatisfiable constraints</p> <p>If you specify a constraint that can't be satisfied in the partition you're submitting your job to, the job will be rejected by the scheduler.     For instance, requesting a RTX3090 GPU in the <code>gpu</code> partition, which doesn't feature any, will result in an error:</p> <pre><code>$ srun -p gpu -G 1 -C GPU_SKU:RTX_3090 nvidia-smi -L\nsrun: error: Unable to allocate resources: Requested node configuration is not available\n</code></pre> <p>For more information about requesting specific node features and adding job constraints, you can also refer to the \"Node features\" page.</p>"},{"location":"docs/user-guide/gpu/#gpu-compute-modes","title":"GPU compute modes","text":"<p>By default, GPUs on Sherlock are set in the Exclusive Process compute mode<sup>3</sup>, to provide the best performance and an isolated environment for jobs, out of the box.</p> <p>Some software may require GPUs to be set to a different compute mode, for instance to share a GPU across different processes within the same application.</p> <p>To handle that case, we developed a specific option, <code>--gpu_cmode</code>, that users can add to their <code>srun</code> and <code>sbatch</code> submission options, to choose the compute mode for the GPUs allocated to their job.</p> <p>Here's the list of the different compute modes supported on Sherlock's GPUs:</p> GPU\u00a0compute\u00a0mode <code>--gpu_cmode</code> option Description \"Default\" <code>shared</code> Multiple contexts are allowed per device (NVIDIA default) \"Exclusive Process\" <code>exclusive</code> Only one context is allowed per device, usable from multiple threads at a time (Sherlock default) \"Prohibited\" <code>prohibited</code> No CUDA context can be created on the device <p>By default, or if the <code>--gpu_cmode</code> option is not specified, GPUs will be set in the \"Exclusive Process\" mode, as demonstrated by this example command:</p> <pre><code>$ srun -p gpu -G 1 nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <p>With the <code>--gpu_cmode</code> option, the scheduler will set the GPU compute mode to the desired value before execution:</p> <pre><code>$ srun -p gpu -G 1 --gpu_cmode=shared nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <p>Tip</p> <p>\"Default\" is the name that the NVIDIA System Management Interface (<code>nvidia-smi</code>) uses to describe the mode where a GPU can be shared between different processes. It does not represent the default GPU compute mode on Sherlock, which is \"Exclusive Process\".</p>"},{"location":"docs/user-guide/gpu/#advanced-options","title":"Advanced options","text":"<p>A number of submission options are available when submitting GPU jobs, to request specific resource mapping or task binding options.</p> <p>Here are some examples to allocate a set of resources as a function of the number of requested GPUs:</p> <ul> <li> <p><code>--cpus-per-gpu</code>: requests a number of CPUs per allocated GPU.</p> <p>For instance, the following options will allocate 2 GPUs and 4 CPUs:</p> <pre><code>$ salloc -p gpu -G 2 --cpus-per-gpu=2\n</code></pre> </li> <li> <p><code>--gpus-per-node</code>: requests a number of GPUs per node,</p> </li> <li><code>--gpus-per-task</code>: requests a number of GPUs per spawned task,</li> <li><code>--mem-per-gpu</code>: allocates (host) memory per allocated GPU.</li> </ul> <p>Other options can help set particular GPU properties (topology, frequency...):</p> <ul> <li> <p><code>--gpu-bind</code>: specify task/GPU binding mode.</p> <p>By default every spawned task can access every GPU allocated to the job. This option can help making sure that tasks are bound to the closest GPU, for better performance.</p> </li> <li> <p><code>--gpu-freq</code>: specify GPU and memory frequency. For instance:</p> <pre><code>$ srun -p test -G 1 --gpu-freq=highm1,verbose /bin/true\nGpuFreq=memory_freq:2600,graphics_freq:758\n</code></pre> </li> </ul> <p>Those options are all available to the <code>srun</code>/<code>sbatch</code>/<code>salloc</code> commands, and more details about each of them can be found in the Slurm documentation.</p> <p>Conflicting options</p> <p>Given the multitude of options, it's very easy to submit a job with conflicting options.  In most cases the job will be rejected.</p> <p>For instance: <pre><code>$ sbatch --gpus-per-task=1 --cpus-per-gpu=2  --cpus-per-task=1 ...\n</code></pre> Here, the first two options implicitly set <code>cpu-per-task</code> to 2, while the third option explicitly sets <code>cpus-per-task</code> to 1. So the job's requirements are conflicting and can't be satisfied.</p>"},{"location":"docs/user-guide/gpu/#environment-and-diagnostic-tools","title":"Environment and diagnostic tools","text":""},{"location":"docs/user-guide/gpu/#nvtop","title":"<code>nvtop</code>","text":"<p>GPU usage information can be shown with the <code>nvtop</code> tool. <code>nvtop</code> is available as a module, which can be loaded like this:</p> <pre><code>$ ml load system nvtop\n</code></pre> <p><code>nvtop</code> provides an <code>htop</code>-like interactive view of GPU utilization.  Users can monitor, estimate and fine tune their GPU resource requests with this tool.  Percent GPU and memory utilization is shown as a user's GPU code is running.</p> <p></p> <ol> <li> <p>See <code>node_feat -h</code> for more details.\u00a0\u21a9</p> </li> <li> <p>The lists of values provided in the table are non exhaustive.\u00a0\u21a9</p> </li> <li> <p>The list of available GPU compute modes and relevant details are available in the CUDA Toolkit Documentation \u21a9</p> </li> </ol>"},{"location":"docs/user-guide/ondemand/","title":"OnDemand","text":""},{"location":"docs/user-guide/ondemand/#introduction","title":"Introduction","text":"<p>The Sherlock OnDemand interface allows you to conduct your research on Sherlock through a web browser. You can manage files (create, edit and move them), submit and monitor your jobs, see their output, check the status of the job queue, run a Jupyter notebook and much more, without logging in to Sherlock the traditional way, via a SSH terminal connection.</p> <p>Quote</p> <p>In neuroimaging there are a number of software pipelines that output HTML reports heavy on images files. Sherlock OnDemand allows users to check those as they appear on their <code>$SCRATCH</code> folder, for quick quality control, instead of having to mount remote filesystems, download data locally or move to any other storage location. Since the data itself is already quite big and costly to move, OnDemand is extremely helpful for fast assessment.</p> <p>-- Carolina Ramirez, Williams PANLab</p>"},{"location":"docs/user-guide/ondemand/#more-documentation","title":"More documentation","text":"<p>Open OnDemand was created by the Ohio Supercomputer Center. </p> <p>The following documentation is specifically intended for using OnDemand on Sherlock. For more complete documentation about OnDemand in general, please see the extensive documentation for OnDemand created by OSC, including many video tutorials.</p>"},{"location":"docs/user-guide/ondemand/#connecting","title":"Connecting","text":"<p>Connection information</p> <p>To connect to Sherlock OnDemand, simply point your browser to https://ondemand.sherlock.stanford.edu</p> <p>Sherlock OnDemand requires the same level of authentication than connecting to Sherlock over SSH. You will be prompted for your SUNet ID and password, and will go through the regular two-step authentication process.</p> <p>The Sherlock OnDemand Dashboard will then open. From there, you can use the menus across the top of the page to manage files, get a shell on Sherlock, submit jobs or open interactive applications such as Jupyter Notebooks or RStudio sessions.</p> <p></p> <p>To end your Sherlock OnDemand session, click on the \"Log Out\" link at the top right of the Dashboard window and close your browser.</p>"},{"location":"docs/user-guide/ondemand/#getting-a-shell","title":"Getting a shell","text":"<p>You can get shell access to Sherlock by choosing Clusters &gt; Sherlock Shell Access from the top menu in the OnDemand Dashboard.</p> <p>In the window that will open, you'll be logged in to one of Sherlock's login nodes, exactly as if you were using SSH to connect. Except you don't need to install any SSH client on your local machine, configure Kerberos or deal with your SSH client configuration to avoid endless two-factor prompts. How cool is that?</p> <p></p>"},{"location":"docs/user-guide/ondemand/#managing-files","title":"Managing files","text":"<p>To create, edit or move files, click on the Files menu from the Dashboard page. A drop-down menu will appear, listing your most common storage locations on Sherlock: <code>$HOME</code>, <code>$GROUP_HOME</code>, <code>$SCRATCH</code>, <code>$GROUP_SCRATCH</code>, and all Oak storage you have access to, including your main <code>$OAK</code><sup>1</sup>. Any <code>rclone</code> remotes you create on Sherlock to connect to cloud storage will appear here as well.</p> <p>Choosing one of the file spaces opens the File Explorer in a new browser tab. The files in the selected directory are listed.</p> <p>There are two sets of buttons in the File Explorer.</p> <ul> <li> <p>Under the three vertical dots menu next to each filename:    Those buttons allow you to   View, Edit, Rename, Download, or Delete a file.</p> </li> <li> <p>At the top of the window, on the right side:   </p> Button Function Open in Terminal Open a terminal window on Sherlock in a new browser tab Refresh Refresh the list of directory contents New File Create a new, empty file New Directory Create a new sub-directory Upload Copy a file from your local machine to Sherlock Download Download selected files to your local machine Copy/Move Copy or move selected files (after moving to a different directory) Delete Delete selected files Change directory Change your current working directory Copy path Copy the current working directory path to your clipboard Show Dotfiles Toggle the display of dotfiles (files starting with a <code>.</code>, which are usually hidden) Show Owner/Mode Toggle the display of owner and permission settings </li> </ul>"},{"location":"docs/user-guide/ondemand/#creating-and-editing-jobs","title":"Creating and editing jobs","text":"<p>You can create new job scripts, edit existing scripts, and submit them to the scheduler through the Sherlock OnDemand interface.</p> <p>From the top menus in the Dashboard, choose Jobs &gt; Job Composer. A Job Composer window will open. There are two tabs at the top: Jobs and Templates.</p> <p>In the Jobs tab, you'll find a list of the job you've submitted through OnDemand. The Templates tab will allow you to define your own job templates.</p>"},{"location":"docs/user-guide/ondemand/#creating-a-new-job-script","title":"Creating a new job script","text":"<p>To create a new job script. you'll need to follow the steps below.</p>"},{"location":"docs/user-guide/ondemand/#select-a-template","title":"Select a template","text":"<p>Go to the Jobs tab in the Jobs Composer interface. You'll find a default template there: \"Simple Sequential Job\".</p> <p>To create a new job script, click the blue New Job &gt; From Default Template button in the upper left. You'll see a green message at the top of the page indicating: \"Job was successfully created\".</p> <p>At the right of the Jobs page, you can see the Job Details, including the location of the script and the script name (by default, <code>main_job.sh</code>). Under that, you will see the contents of the job script in a section named Submit Script.</p> <p></p>"},{"location":"docs/user-guide/ondemand/#edit-the-job-script","title":"Edit the job script","text":"<p>You'll need to edit the job script, so it contains the commands and workflow that you want to submit to the scheduler.</p> <p>If you need more resources than the defaults, you must include options to change them in the job script. For more details, see the Running jobs section.</p> <p>You can edit the script in several ways:</p> <ul> <li>click the blue Edit Files button at the top of the Jobs tab in the   Jobs Composer window,</li> <li>in the Jobs tab in the Jobs Composer window, find the Submit   Script section at the bottom right. Click the blue Open Editor button.</li> </ul> <p>After you save the file, the editor window remains open, but if you return to the Jobs Composer window, you will see that the content of  your script has changed.</p>"},{"location":"docs/user-guide/ondemand/#edit-the-job-options","title":"Edit the job options","text":"<p>In the Jobs tab in the Jobs Composer window, click the blue Job Options button. The options for the selected job such as name, the job script to run, and the account it run under are displayed and can be edited. Click Save or Cancel to return to the job listing.</p>"},{"location":"docs/user-guide/ondemand/#submitting-jobs","title":"Submitting jobs","text":"<p>To submit a job, select in in the Jobs tab in the Jobs Composer page. Click the green Submit button to submit the selected job. A message at the top of the window shows whether the job submission was successful or not. If it is not, you can edit the job script or options and resubmit. When the job is submitted successfully, the status of the job in the Jobs Composer window will change to Queued or Running. When  the job completes, the status will change to Completed.</p> <p></p>"},{"location":"docs/user-guide/ondemand/#monitoring-jobs","title":"Monitoring jobs","text":"<p>From the Dashboard page, The Jobs &gt; Active Jobs top-level menu will bring you to a live view of Sherlock's scheduler queue. You'll be able to see all the jobs currently in queue, including running and pending jobs, as well as some details about individual jobs.</p> <p></p> <p>At the bottom of the detailed view, you'll find two button that will bring you to the directory where that job's files are located, either in the File Manager or in a Shell session.</p>"},{"location":"docs/user-guide/ondemand/#interactive-applications","title":"Interactive applications","text":"<p>One of the main features of Sherlock OnDemand is the ability to run interactive applications directly from the web interface, without leaving your web browser.</p>"},{"location":"docs/user-guide/ondemand/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>You can run Jupyter Notebooks (using Python, Julia or other languages) through Sherlock OnDemand.</p> <p>Some preliminary setup may be required</p> <p>Before running your first Jupyter Notebook with <code>IJulia</code>, you'll need to run the following steps (this only needs to be done once):</p> <pre><code>$ ml julia\n$ julia\njulia&gt; using Pkg;\njulia&gt; Pkg.add(\"IJulia\")\n</code></pre> <p>When you see the message that <code>IJulia</code> has been installed, you can end your interactive session.</p> <p>To start a Jupyter session from Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; Jupyter Notebook from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your notebook starts.</p> </li> </ol> <p></p> <ol> <li> <p>Click the blue Launch button to start your JupyterHub session. You may    have to wait in the queue for resources to become available for you.</p> </li> <li> <p>When your session starts, you can click on the blue Connect to Jupyter    button to open your Jupyter Notebook. The Dashboard window will display    information about your Jupyter session, including the name of the compute    node it is running on, when it started, and how much time remains.    </p> </li> <li> <p>In your new Jupyter Notebook tab, you'll see 3 tabs: Files, Running and    Clusters.    </p> </li> </ol> <p>By default, you are in the Files tab; that displays the contents of your    <code>$HOME</code> directory on Sherlock. You can navigate through your files there.</p> <p>Under the Running tab, you will see the list of all the notebooks or    terminal sessions that you have currently running.</p> <ol> <li> <p>You can now start a Jupyter Notebook:</p> <ol> <li>To open an existing Jupyter Notebook, which is already stored on    Sherlock, navigate to its location in the Files tab and click on its    name. A new window running the notebook will open.</li> <li>To create a new Jupyter Notebook, click on the New button at the top    right of the file listing, and choose the kernel of your choice from the    drop down.</li> </ol> </li> </ol> <p>To terminate your Jupyter Notebook session, go back to the Dashboard, and click on the My Interactive Sessions in the top menu. This will bring you to a page listing all your currently active interactive session. Identify the one you'd like to terminate and click on the red Cancel button.</p>"},{"location":"docs/user-guide/ondemand/#jupyterlab","title":"JupyterLab","text":"<p>To run JupyterLab via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; JupyterLab from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your session starts.</p> </li> <li> <p>Click the blue Launch button to start your JupyterLab session. You may    have to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to JupyterLab    button. A new window opens with the JupyterLab interface.</p> </li> <li> <p>The first time you connect to JupyterLab via Sherlock OnDemand, you'll see    2 tabs: Files and Launcher.</p> </li> </ol> <p></p> <p>The Files tab displays the contents of your <code>$HOME</code> directory on Sherlock.    You can navigate through your files there.</p> <p>In the Launcher tab, you will have the option to create a new Jupyter Notebook    new Console session by clicking the tile showing the kernel of your choice.    You can also open the Terminal or a text editor for a variety of file    types by clicking the corresponding tile.</p> <p>To create a new kernel for IJulia:</p> <ol> <li> <p>In the Launcher, click the Terminal tile in the \"Other\" section.</p> </li> <li> <p>In the Terminal, run the following commands:</p> <pre><code>$ ml julia\n$ julia\njulia&gt; using Pkg;\njulia&gt; Pkg.add(\"IJulia\")\n</code></pre> </li> <li> <p>Open a new Launcher tab by clicking the + sign next to your open Terminal    tab. Julia will now be listed in the \"Notebook\" and \"Console\" sections as an    available kernel.</p> </li> </ol> <p>To create a custom kernel for a virtual environment using Python 3.x:</p> <ol> <li> <p>In a shell session, activate your environment and run the following:</p> <pre><code>$ pip3 install ipykernel\n$ python3 -m ipykernel install --user --name env --display-name \"My Env\"\n</code></pre> <p>This will create a kernel for the environment <code>env</code>. It will appear as <code>My Env</code> in the JupyterLab Launcher.</p> <p>Creating a custom kernel for a Python 2.x environment</p> <p>When working with a Python 2.x environment, use the <code>python</code>/<code>pip</code> commands instead.</p> </li> <li> <p>The custom kernel will now be listed as option in the \"Notebook\" and    \"Console\" sections in the JupyterLab Launcher. To start a Jupyter Notebook    using your virtual environment, click on the tile for that kernel.</p> <p>Creating a custom kernel for a conda environment</p> <p>In order to use a kernel created from a conda environment, you must unload the <code>python</code> and <code>py-jupyterlab</code> modules from your JupyterLab session. This can be done using the JupyterLab Lmod extension. To use the Lmod extension, select the bottom tab in the left side menu of your JupyterLab window. You may also need to restart the kernel for your notebook or console.</p> </li> </ol>"},{"location":"docs/user-guide/ondemand/#matlab","title":"MATLAB","text":"<p>To run MATLAB via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; MATLAB from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your session starts.</p> </li> <li> <p>Click the blue Launch button to start your MATLAB session. You may    have to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to MATLAB    button. A new window opens with the MATLAB interface.</p> </li> </ol> <p></p>"},{"location":"docs/user-guide/ondemand/#rstudio","title":"RStudio","text":"<p>To run RStudio via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; RStudio Server from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your session starts.</p> </li> <li> <p>Click the blue Launch button to start your RStudio session. You may have    to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to RStudio Server    button. A new window opens with the RStudio interface.</p> </li> </ol> <p></p> <p>Installing packages in RStudio</p> <p>You may encounter errors while installing R packages within RStudio. First try installing R packages in a shell session on the Sherlock command line. See our R packages documentation for more information.</p>"},{"location":"docs/user-guide/ondemand/#tensorboard","title":"TensorBoard","text":"<p>To run TensorBoard via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; TensorBoard from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your session starts.</p> </li> <li> <p>Click the blue Launch button to start your TensorBoard session. You may have    to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to TensorBoard    button. A new window opens with the TensorBoard interface.</p> </li> </ol> <p></p>"},{"location":"docs/user-guide/ondemand/#vs-code","title":"VS Code","text":"<p>You can use VS Code on Sherlock through the code-server interactive app.</p> <p>Using your local VS Code with remote SSH</p> <p>Connecting to Sherlock from VS Code on your local machine is not supported at this time due to a known issue with the closed-source \"Remote SSH\" extension.</p> <p>To start a VS Code session via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; code-server from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your session starts.</p> </li> <li> <p>Click the blue Launch button to start your code-server session. You may have    to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to code-server    button. A new window opens with the code-server interface.</p> </li> </ol> <p></p>"},{"location":"docs/user-guide/ondemand/#common-issues","title":"Common Issues","text":""},{"location":"docs/user-guide/ondemand/#no-space-left-on-device","title":"No Space Left on Device","text":"<p>Occasionally, you may get an error message like the one below:</p> <p></p> <p>Quota exceeded</p> <p>This means that you have exceeded your storage quota in your Sherlock <code>$HOME</code> directory.</p> <p>Because this issue prevents OnDemand from running, you will probably not be able to use the \"Files\" menu the web browser to help you clean up. Instead, you will need to connect to the cluster with <code>ssh</code>.</p> <p>Once connected, you'll be able to identify the largest space consumers in your <code>$HOME</code> directory, and either delete or move them to a different storage location.</p> <p>Emptying <code>~/.cache</code></p> <p>Among potential candidates, your <code>~/.cache</code> directory is usually safe to empty, since files there are temporary files that can be re-created as needed. You can empty this directory by running:</p> <pre><code>rm -rf ~/.cache/*\n</code></pre>"},{"location":"docs/user-guide/ondemand/#bad-request","title":"Bad Request","text":"<p>Another common error looks something like this:</p> <p></p> <p>This issue is caused by some session cookies that may have grown beyond what can be handled by the server. This can usually be addressed by clearing your browser cookies.</p> <p>Cleaning site-specific cookies</p> <p>For a more targetted approach, you can delete only the cookies for the <code>ondemand.sherlock.stanford.edu</code> site. Instruction to do this vary by browser, but for Chrome, Firefox and Edge, you can usually click on the padlock icon to the left of the URL in the address bar, then click on \"Cookies\" or \"Site Settings\" to manage or delete cookies for that site. Please refer to your browser's documentation for details.</p> <p>If the issue persists, switching browsers or using \"Incognito Mode\" can work as a temporary workaround.</p>"},{"location":"docs/user-guide/ondemand/#support","title":"Support","text":"<p>If you are experiencing issues with Sherlock or your interactive session, you can contact us directly from Sherlock OnDemand.</p> <p>To submit a ticket about Sherlock or Sherlock OnDemand in general:</p> <ol> <li> <p>Select Help -&gt; Submit Support Ticket from the top menu in the    Dashboard page.</p> </li> <li> <p>In the screen that opens, complete the Support Ticket form. When applicable,    please provide:</p> <ul> <li> <p>the full path to any files involved in your question or problem,</p> </li> <li> <p>the command(s) you ran, and/or the job submission script(s) you used,</p> </li> <li> <p>the exact, entire error message (or trace) you received.</p> </li> </ul> </li> <li> <p>Click the blue Submit support ticket form. Research Computing support will    respond to you as soon as we are able.</p> </li> </ol> <p>To submit a ticket about your current or recent interactive session:</p> <ol> <li> <p>Select My Interactive Sessions from the top menu in the Dashboard page.</p> </li> <li> <p>In the screen that opens, find the card for the session you need help with.    Active sessions will have a green header, and past sessions will have a gray    header. Click that card's Submit support ticket link to open the Support    Ticket form.    </p> </li> <li> <p>Complete the Support Ticket form. When applicable, please provide:</p> <ul> <li> <p>the full path to any files involved in your question or problem,</p> </li> <li> <p>the command(s) you ran, and/or the job submission script(s) you used,</p> </li> <li> <p>the exact, entire error message (or trace) you received.</p> </li> </ul> </li> <li> <p>Click the blue Submit support ticket form. Research Computing support will    respond to you as soon as we are able.</p> </li> </ol> <ol> <li> <p>if you have access to the Oak storage system.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/user-guide/running-jobs/","title":"Running jobs","text":"","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#login-nodes","title":"Login nodes","text":"<p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p> <p>The key principle of a shared computing environment is that resources are shared among users and access to them must be scheduled. On Sherlock, it is mandatory to schedule work by requesting resources and submitting jobs to the scheduler. Because login nodes are shared by all users, they must not be used to execute computational tasks.</p> <p>Acceptable use of login nodes include:</p> <ul> <li>lightweight file transfers,</li> <li>script and configuration file editing,</li> <li>job submission and monitoring.</li> </ul> <p>Resource limits are enforced</p> <p>To minimize disruption and ensure a comfortable working environment for users, resource limits are enforced on login nodes. Processes started there will automatically be terminated if their resource usage (including CPU time, memory and run time) exceed those limits.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#slurm-commands","title":"Slurm commands","text":"<p>Slurm is the job scheduler used on Sherlock. It is responsible for managing the resources of the cluster and scheduling jobs on compute nodes.</p> <p>There are several ways to request resources and submit jobs. The main Slurm commands to submit jobs are listed in the table below:</p> Command Description Behavior <code>salloc</code> Request resources and allocates them to a job Starts a new interactive shell on a compute node <code>srun</code> Request resources and runs a command on the allocated compute node(s) Blocking command: will not return until the executed command ends <code>sbatch</code> Request resources and runs a script on the allocated compute node(s) Asynchronous command: will return as soon as the job is submitted","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#interactive-jobs","title":"Interactive jobs","text":"","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#dedicated-nodes","title":"Dedicated nodes","text":"<p>Interactive jobs allow users to log in to a compute node to run commands interactively on the command line. They could be an integral part of an interactive programming and debugging workflow. The simplest way to establish an interactive session on Sherlock is to use the <code>sh_dev</code> command:</p> <pre><code>$ sh_dev\n</code></pre> <p>This will open a login shell using one core and 4 GB of memory on one node for one hour. The <code>sh_dev</code> sessions run on dedicated compute nodes. This ensures minimal wait times when you need to access a node for testing script, debug code or any kind of interactive work.</p> <p><code>sh_dev</code> also provides X11 forwarding via the submission host (typically the login node you're connected to) and can thus be used to run GUI applications.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#compute-nodes","title":"Compute nodes","text":"<p>If you need more resources<sup>1</sup>, you can pass options to <code>sh_dev</code>, to request more CPU cores, more nodes, or even run in a different partition. <code>sh_dev -h</code> will provide more information:</p> <pre><code>$ sh_dev -h\nsh_dev: start an interactive shell on a compute node.\n\nUsage: sh_dev [OPTIONS]\n    Optional arguments:\n        -c      number of CPU cores to request (OpenMP/pthreads, default: 1)\n        -g      number of GPUs to request (default: none)\n        -n      number of tasks to request (MPI ranks, default: 1)\n        -N      number of nodes to request (default: 1)\n        -m      memory amount to request (default: 4GB)\n        -p      partition to run the job in (default: dev)\n        -t      time limit (default: 01:00:00)\n        -r      allocate resources from the named reservation (default: none)\n        -J      job name (default: sh_dev)\n        -q      quality of service to request for the job (default: normal)\n\n    Note: the default partition only allows for limited amount of resources.\n    If you need more, your job will be rejected unless you specify an\n    alternative partition with -p.\n</code></pre> <p>For instance, you can request 4 CPU cores, 8 GB of memory and 1 GPU in the <code>gpu</code> partition with:</p> <pre><code>$ sh_dev -c 4 -m 8GB -g 1 -p gpu\n</code></pre> <p>Another way to get an interactive session on a compute node is to use <code>salloc</code> to execute a shell through the scheduler. For instance, to start a shell on a compute node in the <code>normal</code> partition, with the default resource requirements (one core for 2 hours), you can run:</p> <pre><code>$ salloc\n</code></pre> <p>The main advantage of this approach is that it will allow you to specify the whole range of submission options that <code>sh_dev</code> may not support.</p> <p>You can also submit an existing job script or other executable as an interactive job:</p> <pre><code>$ salloc ./script.sh\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#connecting-to-nodes","title":"Connecting to nodes","text":"<p>Connecting to compute nodes</p> <p>Users are not allowed to connect to compute nodes unless they have a job running there.</p> <p>If you SSH to a compute node without any active job allocation, you'll be greeted by the following message:</p> <pre><code>$ ssh sh02-01n01\nAccess denied by pam_slurm_adopt: you have no active jobs on this node\nConnection closed\n$\n</code></pre> <p>Once you have a job running on a node, you can SSH directly to it and run additional processes<sup>2</sup>, or observe how you application behaves, debug issues, and so on.</p> <p>The <code>salloc</code> command supports the same parameters as <code>sbatch</code>, and can override any default configuration. Note that any <code>#SBATCH</code> directive in your job script will not be interpreted by <code>salloc</code> when it is executed in this way. You must specify all arguments directly on the command line for them to be taken into account.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#batch-jobs","title":"Batch jobs","text":"<p>It's easy to schedule batch jobs on Sherlock. A job is simply an instance of your program (for example an R, Python or Matlab script) that is submitted to the scheduler (Slurm) for asynchronous execution on a compute node. Submitting a job using the <code>sbatch</code> command creates a batch job, which will either start immediately or enter a pending state in the queue, depending on current cluster load and resource availability.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#job-queuing","title":"Job queuing","text":"<p>The time a job spends pending is primarily influenced by two factors:</p> <ul> <li>the number of jobs ahead of yours in the queue,</li> <li>the amount and type of resources your job requests.</li> </ul> <p>To minimize queue time, request only the resources necessary for your workload. Overestimating resource needs can result in longer wait times. Profiling your code interactively (for example, in an <code>sh_dev</code> session session using tools like <code>htop</code>, <code>nvtop</code>, <code>sacct</code>) can help you determine appropriate resource requirements.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#requesting-resources","title":"Requesting resources","text":"<p>When submitting a job, you can request the following:</p> <ul> <li> <p>CPUs: How many CPU cores the program you are calling the in   the <code>sbatch</code> script needs. Unless it can utilize multiple CPUs at once you   should request a single CPU. Check your code's documentation or try running   in an interactive session with <code>sh_dev</code> and run <code>htop</code> if you   are unsure.</p> </li> <li> <p>GPUs: If your code is GPU-enabled, how many GPUs does your   code need?</p> </li> <li> <p>Memory (RAM): Estimate how much memory your job will consume. Consider   whether your program loads large datasets or uses significant memory on your   local machine. For most jobs, the default memory allocation usually suffices.</p> </li> <li> <p>Time: Specify how long your job will take to run to completion.</p> </li> <li> <p>Partition: Choose the compute partition (e.g., <code>normal</code>,   <code>gpu</code>, <code>owners</code>, <code>bigmem</code>).</p> </li> </ul> <p>You also need to indicate the scheduler what your job should should do: what modules to load, and how to execute your application. Note that any logic you can code into a bash script with the bash scripting language can also be coded into an <code>sbatch</code> script.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#example-sbatch-script","title":"Example <code>sbatch</code> script","text":"<p>The example job below will run the Python script <code>mycode.py</code> for 10 minutes on the <code>normal</code> partition using 1 CPU and 8 GB of memory. To aid in debugging, we are naming this job \"test_job\" and appending the Job ID (<code>%j</code>) to the two output files that Slurm creates when a job is executed. The output files are written to the directory in which you launched your job (you can also specify a different path). One file will contain any errors and the other will contain non-error output. Look in these 2 files ending in <code>.err</code> and <code>.out</code> for useful debugging information and error output.</p> <p>Because it's a Python script that uses some NumPy code, we need to load the <code>python/3.6.1</code> and the <code>py-numpy/1.19.2_py36</code> modules. The Python script is then called just as you would on the command line, at the end of the <code>sbatch</code> script.</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --output=test_job.%j.out\n#SBATCH --error=test_job.%j.err\n#SBATCH --time=10:00\n#SBATCH -p normal\n#SBATCH -c 1\n#SBATCH --mem=8GB\n\nmodule load python/3.6.1\nmodule load py-numpy/1.19.2_py36\npython3 mycode.py\n</code></pre> <p>Here are the steps to create and submit this batch script:</p> <ol> <li> <p>Create and edit the <code>sbatch</code> script with a text editor    like <code>vim</code>/<code>nano</code> or the OnDemand file manager. Then save    the file, in this example we call it <code>test.sbatch</code>.</p> </li> <li> <p>Submit to the scheduler with the <code>sbatch</code> command:</p> <pre><code>$ sbatch test.sbatch\n</code></pre> </li> <li> <p>Monitor your job and job ID in the queue with the <code>squeue</code>    command:</p> <pre><code>$ squeue --me\n   JOBID    PARTITION   NAME      USER      ST  TIME  NODES  NODELIST(REASON)\n   4915821     normal   test_job  &lt;userID&gt;  PD  0:00      1  (Resources)\n</code></pre> <p>Notice that the jobs state is \"Pending\" (<code>PD</code>). Once the job starts to run, its state will change to \"Running\" (<code>R</code>), and the <code>NODES</code> column will indicate how many nodes are being used. The <code>NODELIST(REASON)</code> column will show the reason why the job is pending. In this case, it is waiting for resources to become available.</p> <pre><code>$ squeue --me\n   JOBID    PARTITION   NAME      USER      ST  TIME  NODES  NODELIST(REASON)\n   4915821     normal   test_job  &lt;userID&gt;   R  0:10      1  sh02-01n49\n</code></pre> <p>This last output means that job 44915821 has been running (R) on compute node <code>sh02-01n49</code> for 10 seconds (0:10).</p> </li> </ol> <p>While your job is running you can connect to the node it's running on via SSH, to monitor your job in real-time. For example, if your job is running on node <code>sh02-01n49</code>, you can connect to it with:</p> <pre><code>$ ssh sh02-01n49\n</code></pre> <p>and then use tools like <code>htop</code> to watch processes and resource usage.</p> <p>You can also manage this job based on the jobid assigned to it (44915854). For example the job can be canceled with the <code>scancel</code> command . After your job completes you can asses and fine-tune your resource requests (time, CPU/GPU, memory) with the <code>sacct</code> or <code>seff</code> commands.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#estimating-resources","title":"Estimating resources","text":"<p>To get a better idea of the amount of resources your job will need, you can use the <code>ruse</code> command, available as a module:</p> <pre><code>$ module load system ruse\n</code></pre> <p><code>ruse</code> is a command line tool developed by Jan Moren to measure a process' resource usage. It periodically measures the resource use of a process and its sub-processes, and can help you find out how much resource to allocate to your job. It will determine the actual memory, execution time and cores that individual programs or MPI applications need to request in their job submission options.</p> <p><code>ruse</code> periodically samples the process and its sub-processes and keeps track of the CPU, time and maximum memory use. It also optionally records the sampled values over time. The purpose or Ruse is not to profile processes in detail, but to follow jobs that run for many minutes, hours or days, with no performance impact and without changing the measured application in any way.</p> <p>You'll find complete documentation and details about <code>ruse</code>'s usage on the project webpage, but here are a few useful examples.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#sizing-a-job","title":"Sizing a job","text":"<p>In its simplest form, <code>ruse</code> can help discover how much resources a new script or application will need. For instance, you can start a sizing session on a compute node with an overestimated amount of resources, and start your application like this:</p> <pre><code>$ ruse ./myapp\n</code></pre> <p>This will generate a <code>&lt;myapp&gt;-&lt;pid&gt;/ruse</code> output file in the current directory, looking like this:</p> <pre><code>Time:           02:55:47\nMemory:         7.4 GB\nCores:          4\nTotal_procs:    3\nActive_procs:   2\nProc(%): 99.9  99.9\n</code></pre> <p>It shows that <code>myapp</code>:</p> <ul> <li>ran for almost 3 hours</li> <li>used a little less than 8B of memory</li> <li>had 4 cores available,</li> <li>spawned 3 processes, among which at most 2 were active at the same time,</li> <li>that both active processes each used 99.9% of a CPU core</li> </ul> <p>This information could be useful in tailoring the job resource requirements to its exact needs, making sure that the job won't be killed for exceeding one of its resource limits, and that the job won't have to wait too long in queue for resources that it won't use. The corresponding job request could look like this:</p> <pre><code>#SBATCH --time 3:00:00\n#SBATCH --mem 8GB\n#SBATCH --cpus-per-task 2\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#verifying-a-jobs-usage","title":"Verifying a job's usage","text":"<p>It's also important to verify that applications, especially parallel ones, stay in the confines of the resources they've requested. For instance, a number of parallel computing libraries will make the assumption that they can use all the resources on the host, will automatically determine the number of physical CPU cores present on the compute node, and start as many processes. This could be a significant issue if the job requested less CPUs, as more processes will be constrained on less CPU cores, which will result in node overload and degraded performance for the application.</p> <p>To avoid this, you can start your application with <code>ruse</code> and report usage for each time step specified with <code>-t</code>. You can also request the reports to be displayed directly on <code>stdout</code> rather than stored in a file.</p> <p>For instance, this will report usage every 10 seconds:</p> <pre><code>$ ruse -s -t10 --stdout ./myapp\n   time         mem   processes  process usage\n  (secs)        (MB)  tot  actv  (sorted, %CPU)\n     10        57.5    17    16   33  33  33  25  25  25  25  25  25  25  25  20  20  20  20  20\n     20        57.5    17    16   33  33  33  25  25  25  25  25  25  25  25  20  20  20  20  20\n     30        57.5    17    16   33  33  33  25  25  25  25  25  25  25  25  20  20  20  20  20\n\nTime:           00:00:30\nMemory:         57.5 MB\nCores:          4\nTotal_procs:   17\nActive_procs:  16\nProc(%): 33.3  33.3  33.2  25.0  25.0  25.0  25.0  25.0  25.0  24.9  24.9  20.0  20.0  20.0  20.0  19.9\n</code></pre> <p>Here, we can see that despite having being allocated 4 CPUs, the application started 17 threads, 16 of which were active running intensive computations, with the unfortunate consequence that each process could only use a fraction of a CPU.</p> <p>In that case, to ensure optimal performance and system operation, it's important to modify the application parameters to make sure that it doesn't start more computing processes than the number of requested CPU cores.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#available-resources","title":"Available resources","text":"<p>Whether you are submitting a batch job, or an or interactive job, it's important to know the resources that are available to you. For this reason, we provide <code>sh_part</code>, a command-line tool to help answer questions such as:</p> <ul> <li>which partitions do I have access to?</li> <li>how many jobs are running on them?</li> <li>how many CPUs can I use?</li> <li>where should I submit my jobs?</li> </ul> <p><code>sh_part</code> can be executed on any login or compute node to see what partitions are available to you, and its output looks like this:</p> <pre><code>$ sh_part\n partition           || nodes         | CPU cores             | GPUs                 || job runtime     | mem/core        | per-node\n name         public ||   idle  total |   idle  total  queued |   idle  total queued || default maximum | default maximum |    cores   mem(GB)  gpus\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n normal*      yes    ||      0    218 |    438   5844    6949 |      0      0      0 ||      2h      7d |     6GB     8GB |    20-64   128-384     0\n bigmem       yes    ||      0     11 |    537    824     255 |      0      0      0 ||      2h      1d |     6GB    64GB |   24-256  384-4096     0\n gpu          yes    ||      0     33 |    354   1068     905 |     25    136    196 ||      1h      2d |     8GB    32GB |    20-64  191-2048   4-8\n dev          yes    ||      1      4 |     64    104       0 |     62     64      0 ||      1h      2h |     6GB     8GB |    20-32   128-256  0-32\n service      yes    ||      5      6 |    129    132       0 |      0      0      0 ||      1h      2h |     1GB     8GB |    20-32   128-256     0\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The above example shows four possible partitions where jobs can be submitted: <code>normal,</code> <code>bigmem,</code> <code>gpu,</code>, <code>dev</code>, and <code>service</code>. It also provides additional information such as the maximum amount of time allowed in each partition, the number of other jobs already in queue, along with the ranges of resources available on nodes in each partition. In particular:</p> <ul> <li>in the <code>partition name</code> column, the <code>*</code> character indicates the default   partition.</li> <li>the <code>queued</code> columns show the amount ot CPU cores or GPUs requested by   pending jobs,</li> <li>the <code>per-node</code> columns show the range of resources available on each node in   the partition. For instance, the <code>gpu</code> partition has nodes with 20 to 64 CPU   cores and 191 to 2048 GB of memory, and up to 8 GPUs per node.</li> </ul>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#public-partitions","title":"Public partitions","text":"<p>Here are the main public partitions available to everyone on Sherlock:</p> Partition Purpose Resources Limits <code>normal</code> General purpose compute jobs 20-64 cores/node, 6-8 GB RAM/core default runtime of 2 hours, max. 2 days (up to 7 days with the <code>long</code> QOS<sup>4</sup>) <code>bigmem</code> High memory compute jobs for jobs requiring &gt; 256GB, up to 4 TB RAM/node Maximum runtime of 1 day <code>gpu</code> GPU compute jobs 20-64 cores/node, up to 2TB RAM/node, 4 or 8 GPUs/node 16 GPUs/user <code>dev</code> Development and testing jobs dedicated nodes and lightweight GPU instances (MIG) 2h max, 4 cores + 2 GPUs/user <code>service</code> Lightweight, recurring administrative tasks massively over-subscribed resources 2 jobs, 16 cores/user, 2 days runtime","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#service-jobs","title":"Service jobs","text":"<p>It's often useful to run lightweight, recurring administrative tasks on a cluster, such as data transfer or monitoring jobs. On Sherlock, these tasks can be run in the service partition, which is designed specifically for this purpose.</p> <p>The <code>service</code> partition is not intended for compute intensive workloads, but rather for lightweight tasks that do not require significant computing resources. This includes jobs such as data transfers, backups, archival processes, CI/CD pipelines, lightweight database servers, job managers, and other menial or cron-like operations.</p> <ul> <li> <p>Purpose: Intended for non-computational, background, or administrative   tasks that are important for cluster operations but do not need high   performance.</p> </li> <li> <p>Resource Allocation: Resources in the service partition are heavily   oversubscribed. This means that multiple jobs may share the same CPU and   memory resources, leading to minimal compute performance. The focus is on   maximizing throughput for many small, low-impact jobs, not on delivering fast   or isolated execution.</p> </li> <li> <p>Performance: Not suitable for regular compute-intensive workloads. Jobs   running here may experience significant slowdowns or contention if they   attempt to use substantial CPU or memory.</p> </li> <li> <p>Best Fit:</p> <ul> <li>Scheduled and recurring jobs (e.g., via <code>scrontab</code>)</li> <li>Data movement (<code>rsync</code>, <code>scp</code>, etc.)</li> <li>Automated backups and data archival tasks</li> <li>Monitoring agents, job managers, or lightweight daemons</li> <li>CI/CD tasks that do not require high performance nor specialized hardware</li> </ul> </li> <li> <p>Partition Usage: The service partition is an ideal replacement for   traditional cron jobs on login nodes, providing a dedicated and isolated   environment for such workloads without impacting user-facing compute   partitions.</p> </li> </ul>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#recurring-jobs","title":"Recurring jobs","text":"<p>Warning</p> <p><code>Cron</code> tasks are not supported on Sherlock.</p> <p>Users are not allowed to create <code>cron</code> jobs on Sherlock, for a variety of reasons:</p> <ul> <li>resources limits cannot be easily enforced in <code>cron</code> jobs, meaning that a   single user can end up monopolizing all the resources of a login node,</li> <li>no amount of resources can be guaranteed when executing a <code>cron</code> job, leading   to unreliable runtime and performance,</li> <li>user <code>cron</code> jobs have the potential of bringing down whole   nodes by creating fork bombs, if they're not carefully crafted and tested,</li> <li>compute and login nodes could be redeployed at any time, meaning that   <code>cron</code> jobs scheduled there could go away without the user being notified,   and cause all sorts of unexpected results,</li> <li><code>cron</code> jobs could be mistakenly scheduled on several nodes and run multiple   times, which could result in corrupted files.</li> </ul> <p>As an alternative, if you need to run recurring tasks at regular intervals, we recommend the following approach: by using the <code>--begin</code> job submission option, and creating a job that resubmits itself once it's done, you can virtually emulate the behavior and benefits of a <code>cron</code> job, without its disadvantages: your task will be scheduled on a compute node, and use all of the resources it requested, without being impacted by anything else.</p> <p>Depending on your recurring job's specificities, where you submit it and the state of the cluster at the time of execution, the starting time of that task may not be guaranteed and result in a delay in execution, as it will be scheduled by Slurm like any other jobs. Typical recurring jobs, such as file synchronization, database updates or backup tasks don't require strict starting times, though, so most users find this an acceptable trade-off.</p> <p>The table below summarizes the advantages and drawbacks of each approach:</p> Cron tasks Recurring jobs Authorized on Sherlock Dedicated resources for the task Persistent across node redeployment Unique, controlled execution Precise schedule","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#recurring-job-example","title":"Recurring job example","text":"<p>The script below presents an example of such a recurring job, that would emulate a <code>cron</code> task. It will append a timestamped line to a <code>cron.log</code> file in your <code>$HOME</code> directory and run every 7 days.</p> cron.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=cron\n#SBATCH --begin=now+7days\n#SBATCH --dependency=singleton\n#SBATCH --time=00:02:00\n#SBATCH --mail-type=FAIL\n\n\n## Insert the command to run below. Here, we're just storing the date in a\n## cron.log file\ndate -R &gt;&gt; $HOME/cron.log\n\n## Resubmit the job for the next execution\nsbatch $0\n</code></pre> <p>If the job payload (here the <code>date</code> command) fails for some reason and generates and error, the job will not be resubmitted, and the user will be notified by email.</p> <p>We encourage users to get familiar with the submission options used in this script by giving a look at the <code>sbatch</code> man page, but some details are given below:</p> Submission\u00a0option\u00a0or\u00a0command Explanation <code>--job-name=cron</code> makes it easy to identify the job, is used by the  <code>--dependency=singleton</code> option to identify identical jobs, and will allow canceling the job by name (because its jobid will change each time it's  submitted) <code>--begin=now+7days</code> will instruct the scheduler to not even consider the job   for scheduling before 7 days after it's been submitted <code>--dependency=singleton</code> will make sure that only one <code>cron</code> job runs at any given time <code>--time=00:02:00</code> runtime limit for the job (here 2 minutes). You'll need to adjust the value   depending on the task you need to run (shorter runtime requests usually   result in the job running closer to the clock mark) <code>--mail-type=FAIL</code> will send an email notification to the user if the job ever fails <code>sbatch $0</code> will resubmit the job script by calling its own name (<code>$0</code>)   after successful execution <p>You can save the script as <code>cron.sbatch</code> or any other name, and submit it with:</p> <pre><code>$ sbatch cron.sbatch\n</code></pre> <p>It will start running for the first time 7 days after you submit it, and it will continue to run until you cancel it with the following command (using the job name, as defined by the <code>--job-name</code> option):</p> <pre><code>$ scancel -n cron\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#persistent-jobs","title":"Persistent jobs","text":"<p>Recurring jobs described above are a good way to emulate <code>cron</code> jobs on Sherlock, but don't fit all needs, especially when a persistent service is required.</p> <p>For instance, workflows that require a persistent database connection would benefit from an ever-running database server instance. We don't provide persistent database services on Sherlock, but instructions and examples on how to submit database server jobs are provided for MariaDB or PostgreSQL.</p> <p>In case those database instances need to run pretty much continuously (within the limits of available resources and runtime maximums), the previous approach described in the recurring jobs section could fall a bit short.  Recurring jobs are mainly designed for jobs that have a fixed execution time and don't reach their time limit, but need to run at given intervals (like synchronization or backup jobs, for instance).</p> <p>Because a database server process will never end within the job, and will continue until the job reaches its time limit, the last re-submission command (<code>sbatch $0</code>) will actually never be executed, and the job won't be resubmitted.</p> <p>To work around this, a possible approach is to catch a specific signal sent by the scheduler at a predefined time, before the time limit is reached, and then re-queue the job. This is easily done with the Bash <code>trap</code> command, which can be instructed to re-submit a job when it receives the <code>SIGUSR1</code> signal.</p> <p>Job re-submission and execution delay</p> <p>Jobs that are automatically re-submitted using this technique won't restart right away: the will get back in queue and stay pending until their execution conditions (priority, resources, usage limits...) are satisfied.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#persistent-job-example","title":"Persistent job example","text":"<p>Here's the recurring job example from above, modified to:</p> <ol> <li>instruct the scheduler to send a <code>SIGUSR1</code> signal to the job 90    seconds<sup>3</sup> before reaching its time limit (with the <code>#SBATCH    --signal</code> option),</li> <li>re-submit itself upon receiving that <code>SIGUSR1</code> signal (with the <code>trap</code>    command)</li> </ol> persistent.sbatch<pre><code>#!/bin/bash\n#\n#SBATCH --job-name=persistent\n#SBATCH --dependency=singleton\n#SBATCH --time=00:05:00\n#SBATCH --signal=B:SIGUSR1@90\n\n# catch the SIGUSR1 signal\n_resubmit() {\n    ## Resubmit the job for the next execution\n    echo \"$(date): job $SLURM_JOBID received SIGUSR1 at $(date), re-submitting\"\n    sbatch $0\n}\ntrap _resubmit SIGUSR1\n\n## Insert the command to run below. Here, we're just outputting the date every\n## 10 seconds, forever\n\necho \"$(date): job $SLURM_JOBID starting on $SLURM_NODELIST\"\nwhile true; do\n    echo \"$(date): normal execution\"\n    sleep 60\ndone\n</code></pre> <p>Long running processes need to run in the background</p> <p>If your job's actual payload (the application or command you want to run) is running continuously for the whole duration of the job, it needs to be executed in the background, so the trap can be processed.</p> <p>To run your application in the background, just add a <code>&amp;</code> at the end of the command and then add a <code>wait</code> statement at the end of the script, to make the shell wait until the end of the job.</p> <p>For instance, if you were to run a PostgreSQL database server, the <code>while true ... done</code> loop in the previous example could be replaced by something like this:</p> <pre><code>postgres -i -D $DB_DIR &amp;\nwait\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#persistent-jobid","title":"Persistent <code>$JOBID</code>","text":"<p>One potential issue with having a persistent job re-submit itself when it reaches its runtime limit is that it will get a different <code>$JOBID</code> each time it's (re-)submitted.</p> <p>This could be particularly challenging when other jobs depend on it, like in the database server scenario, where client jobs would need to start only if the database server is running. This can be achieved with job dependencies, but those dependencies have to be expressed using jobid numbers, so having the server job's id changing at each re-submission will be difficult to handle.</p> <p>To avoid this, the re-submission command (<code>sbatch $0</code>) can be replaced by a re-queuing command:</p> <pre><code>scontrol requeue $SLURM_JOBID\n</code></pre> <p>The benefit of that change is that the job will keep the same <code>$JOBID</code> across all re-submissions. And now, dependencies can be added to other jobs using that specific <code>$JOBID</code>, without having to worry about it changing. And there will be only one <code>$JOBID</code> to track for that database server job.</p> <p>The previous example can then be modified as follows:</p> persistent.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=persistent\n#SBATCH --dependency=singleton\n#SBATCH --time=00:05:00\n#SBATCH --signal=B:SIGUSR1@90\n\n# catch the SIGUSR1 signal\n_requeue() {\n    echo \"$(date): job $SLURM_JOBID received SIGUSR1, re-queueing\"\n    scontrol requeue $SLURM_JOBID\n}\ntrap '_requeue' SIGUSR1\n\n## Insert the command to run below. Here, we're just outputting the date every\n## 60 seconds, forever\n\necho \"$(date): job $SLURM_JOBID starting on $SLURM_NODELIST\"\nwhile true; do\n    echo \"$(date): normal execution\"\n    sleep 60\ndone\n</code></pre> <p>Submitting that job will produce an output similar to this:</p> <pre><code>Mon Nov  5 10:30:59 PST 2018: Job 31182239 starting on sh-06-34\nMon Nov  5 10:30:59 PST 2018: normal execution\nMon Nov  5 10:31:59 PST 2018: normal execution\nMon Nov  5 10:32:59 PST 2018: normal execution\nMon Nov  5 10:33:59 PST 2018: normal execution\nMon Nov  5 10:34:59 PST 2018: Job 31182239 received SIGUSR1, re-queueing\nslurmstepd: error: *** JOB 31182239 ON sh-06-34 CANCELLED AT 2018-11-05T10:35:06 DUE TO JOB REQUEUE ***\nMon Nov  5 10:38:11 PST 2018: Job 31182239 starting on sh-06-34\nMon Nov  5 10:38:11 PST 2018: normal execution\nMon Nov  5 10:39:11 PST 2018: normal execution\n</code></pre> <p>The job runs for 5 minutes, then received the <code>SIGUSR1</code> signal, is re-queued, restarts for 5 minutes, and so on, until it's properly <code>scancel</code>led.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#slurm-crontab","title":"Slurm crontab","text":"<p>As an alternative, Slurm also offers the possibility to emulate regular traditional <code>cron</code> jobs using the <code>scrontab</code> command. This is a Slurm-specific command that allows users to schedule jobs to run at specific times or intervals, similar to the traditional <code>cron</code> system. The main difference is that <code>scrontab</code> jobs are managed by Slurm, and can take advantage of the scheduler resource management capabilities.</p> <p>The full documentation about <code>scrontab</code> is available in the Slurm documentation, but you'll find some more Sherlock-specific information below.</p> <p>To edit your <code>scrontab</code> script, you can use the following command:</p> <pre><code>$ scrontab -e\n</code></pre> <p>This will open your default editor on Sherlock (<code>vim</code> is the default), where you can edit your script, and once you save it, it will automatically be scheduled for execution.</p> <p>You can view your existing <code>scron</code> scripts with:</p> <pre><code>$ scrontab -l\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#example-scrontab-script","title":"Example <code>scrontab</code> script","text":"<p>Each <code>scrontab</code> script can include regular Slurm submissions, (like <code>-t-</code>/<code>--time</code>, <code>-c</code>/<code>--cpus-per-task</code>, etc).  Here's an example <code>scrontab</code> job script that will run every three hours in the <code>service</code> partition, and run a script that won't need more than 10 minutes to complete.</p> <p>Log file output</p> <p>By default, Slurm will overwrite output files at each execution. If you want to keep a runnin glog of each execution, you can add a <code>#SCRON --open-mode=append</code> line to your <code>scrontab</code> script, which will tell Slurm to append any new output to the exisitng output file.</p> <pre><code>#SCRON -p service\n#SCRON -t 00:10:00\n#SCRON -o mycron_output-%j.out\n#SCRON --open-mode=append\n\n0 */3 * * * /path/to/your/script.sh\n</code></pre> <p>Exceeding resource limits</p> <p>If your job requirement specifications exceed the defined limits in the requested partition, the job will be rejected and an error message will be displayed when saving the <code>scrontab</code> script. For instance, requesting <code>-c 32</code> in the <code>service</code> partition will result in the following massage:</p> <pre><code>There was an issue with the job submission on lines 26-29\nThe error code return was: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\nThe error message was: QOSMaxCpuPerUserLimit\nThe failed lines are commented out with #BAD:\nDo you want to retry the edit? (y/n)\n</code></pre>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#long-running-scrontab-jobs","title":"Long-running <code>scrontab</code> jobs","text":"<p>In most cases, <code>scron</code> jobs will be short-lived, and their execution duration will be smaller than the interval between executions. But sometimes, it may be useful to maintain a long-running process, to manage jobs or keep a database instance running for longer than the maximum runtime.</p> <p>For those long-running jobs, you can set the execution interval to be fairly short (so the job is restarted early when it's interrupted), and add the <code>--dependency=singleton</code> submission option, to make sure that only one instance of the job is running at any given time:</p> <pre><code>#SCRON -p service\n#SCRON -t 1-00:00:00\n#SCRON --dependency=singleton\n#SCRON --name=my_process\n\n0 * * * * /path/to/your/script.sh\n</code></pre> <p>This will instruct The scheduler to check every hour whether an instance of the job is running, and start it if it's not.</p> <p>Avoiding duplicate job instances</p> <p>To avoid having multiple instances of the same job running at the same time, and starting multiple instances each time the <code>scrontab</code> file is edited, make sure to the <code>--dependency=singleton</code> option.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#monitoring-scrontab-jobs","title":"Monitoring <code>scrontab</code> jobs","text":"<p>You can monitor your <code>scrontab</code> jobs with <code>squeue</code>, like every other Slurm job. For instance, to only list your <code>scrontab</code> jobs, you can use the following command:</p> <pre><code>$ squeue --me -O JobID,EligibleTime,CronJob | awk 'NR==1 || $NF==\"Yes\"'\nJOBID               ELIGIBLE_TIME       CRON_JOB\n105650              2025-05-23T13:20:00 Yes\n</code></pre> <p>The <code>ELIGIBLE_TIME</code> column indicates the next time the batch system will run your job.</p>","tags":["slurm"]},{"location":"docs/user-guide/running-jobs/#canceling-a-scrontab-job","title":"Canceling a <code>scrontab</code> job","text":"<p>To cancel a <code>scrontab</code> job, you can edit the <code>scrontab</code> file with <code>scrontab -e</code> and comment out all the lines associated with the job you want to cancel. This will immediately remove the <code>scrontab</code> job from the queue when the script is saved, and prevent the job from being executed in the future.</p> <p>Using <code>scancel</code> on a <code>scrontab</code> job</p> <p>The <code>scancel</code> command will give a warning when attempting to remove a job started with <code>scrontab</code>.</p> <pre><code>$ scancel 105650\nscancel: error: Kill job error on job id 105650: Cannot cancel scrontab jobs without --cron flag.\n</code></pre> <p>When canceling a <code>scrontab</code> job with the <code>--cron</code> flag, the corresponding entries in the <code>scrontab</code> file are prepended with <code>#DISABLED</code>. These comments will need to be removed before the job will be able to start again.</p> <ol> <li> <p>The dedicated partition that <code>sh_dev</code> uses by default only allows   up to 2 cores and 8 GB or memory per user at any given time. So if you need   more resources for your interactive session, you may have to specify a   different partition.\u00a0\u21a9</p> </li> <li> <p>Please note that your SSH session will be attached to your   running job, and that resources used by that interactive shell will count   towards your job's resource limits. So if you start a process using large   amounts of memory via SSH while your job is running, you may hit the job's   memory limits, which will trigger its termination.\u00a0\u21a9</p> </li> <li> <p>Due to the resolution of event handling by the scheduler, the   signal may be sent up to 60 seconds earlier than specified.\u00a0\u21a9</p> </li> <li> <p>the <code>long</code> QOS can only be used in the <code>normal</code> partition, and is   only accessible to users who are not part of an owners group (since owner   groups can already run for up to 7 days in their respective partition).\u00a0\u21a9</p> </li> </ol>","tags":["slurm"]},{"location":"docs/user-guide/troubleshoot/","title":"Troubleshooting","text":"<p>Sherlock is a resource for research, and as such, it is in perpetual evolution, as hardware, applications, libraries, and modules are added, updated, and/or modified on a regular basis.  Sometimes issues can appear where none existed before. When you find something missing or a behavior that seems odd, please let us know.</p>"},{"location":"docs/user-guide/troubleshoot/#how-to-submit-a-support-request","title":"How to submit a support request","text":"<p>Google it first!</p> <p>When encountering issues with software, if the misbehavior involves an error message, the first step should always be to look up the error message online.  There's a good chance somebody stumbled upon the same hurdles before, and may even provide some fix or workaround.</p> <p>One of the most helpful Google searches is <code>your_application sbatch</code>.  For example if you're having trouble submitting jobs or allocating resources (CPUs, time, memory) with Cell Ranger, search for <code>cell ranger sbatch</code> to see how others have successfully run your application on a cluster.</p> <p>If you're facing issues you can't figure out, we're here to help. Feel free to email us at srcc-support@stanford.edu, but please keep the following points in mind to ensure a timely and relevant response to your support requests.</p> <p>Please provide relevant information</p> <p>We need to understand the issue you're facing, and in most cases, we need to be able to reproduce it, so it could be diagnosed and addressed. Please make sure to provide enough information so we could help you in the best possible way.</p> <p>This typically involves providing the following information:</p> <ul> <li>your SUNet ID,</li> <li>some context about your problem (were you submitting a job, copying a file,   compiling an application?),</li> <li>if relevant, the full path to the files involved in your question or problem,</li> <li>the name of node where you received the error (usually displayed in your   command-line prompt),</li> <li>the command(s) you ran, and/or the job submission script(s) you used,</li> <li>the relevant job ID(s),</li> <li>the exact, entire error message (or trace) you received.</li> </ul> <p>Error messages are critical</p> <p>This is very important. Without proper error messages, there is nothing we can do to help. And \"it doesn't work\" is not a proper error message.  Also, please cut and paste the actual text of the output, commands, and error messages rather than screenshots in your tickets. That way it is much easier for us to try to replicate your errors.</p> <p>You can avoid email back and forth where we ask for all the relevant details, and thus delay the problem resolution, by providing all this information from the start. This will help us get to your problem immediately.</p>"}]}